{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.core import Activation,Flatten,Dense,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes_to_label(label):\n",
    "    if label==\"Black-grass\":\n",
    "        return 0\n",
    "    elif label==\"Charlock\":\n",
    "        return 1\n",
    "    elif label==\"Cleavers\":\n",
    "        return 2\n",
    "    elif label==\"Common Chickweed\":\n",
    "        return 3\n",
    "    elif label==\"Common wheat\":\n",
    "        return 4\n",
    "    elif label==\"Fat Hen\":\n",
    "        return 5\n",
    "    elif label==\"Loose Silky-bent\":\n",
    "        return 6\n",
    "    elif label==\"Maize\":\n",
    "        return 7\n",
    "    elif label==\"Scentless Mayweed\":\n",
    "        return 8\n",
    "    elif label==\"Shepherds Purse\":\n",
    "        return 9\n",
    "    elif label==\"Small-flowered Cranesbill\":\n",
    "        return 10\n",
    "    elif label==\"Sugar beet\":\n",
    "        return 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the train images into array of pixels with corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr=[]\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dir=os.listdir(\"D://Project2_AfterResidency7_CNN//train\")\n",
    "for directory in train_dir:\n",
    "    images=os.listdir(\"D://Project2_AfterResidency7_CNN//train//\"+directory)\n",
    "    for image in images:\n",
    "        img=load_img(\"D://Project2_AfterResidency7_CNN//train//\"+directory+\"//\"+image)\n",
    "        img_arr=img_to_array(img)\n",
    "        img_arr=cv2.resize(img_arr,(128,128))\n",
    "        image_arr.append(img_arr)\n",
    "        label=convert_classes_to_label(directory)\n",
    "        labels.append(label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr=np.array(image_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 128, 128, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 77.51807 ,  47.181885,  23.59668 ],\n",
       "        [ 81.575195,  54.592285,  33.248535],\n",
       "        [ 88.85132 ,  66.33325 ,  45.246094],\n",
       "        ...,\n",
       "        [ 88.18091 ,  78.25903 ,  55.638184],\n",
       "        [ 93.99878 ,  86.031494,  60.007324],\n",
       "        [ 89.57715 ,  82.78027 ,  52.092285]],\n",
       "\n",
       "       [[ 78.69751 ,  46.112305,  21.431885],\n",
       "        [ 85.65747 ,  57.006836,  36.235596],\n",
       "        [ 90.18872 ,  65.856445,  45.848145],\n",
       "        ...,\n",
       "        [ 87.93652 ,  76.74487 ,  59.463623],\n",
       "        [ 91.796875,  81.04126 ,  62.025635],\n",
       "        [ 88.947266,  77.55664 ,  57.29663 ]],\n",
       "\n",
       "       [[ 82.56909 ,  49.10034 ,  24.694092],\n",
       "        [ 84.1958  ,  53.528076,  31.188477],\n",
       "        [ 94.71338 ,  69.385254,  50.605713],\n",
       "        ...,\n",
       "        [ 84.0105  ,  72.0105  ,  57.44629 ],\n",
       "        [ 84.75    ,  72.488525,  57.613525],\n",
       "        [ 89.02881 ,  76.78784 ,  61.356934]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[142.64722 , 148.82568 , 155.56006 ],\n",
       "        [143.73755 , 149.87402 , 155.87402 ],\n",
       "        [142.698   , 149.04175 , 154.36987 ],\n",
       "        ...,\n",
       "        [131.81934 , 136.27075 , 141.927   ],\n",
       "        [ 91.41138 ,  91.8645  ,  97.12598 ],\n",
       "        [ 67.36426 ,  61.326416,  67.30176 ]],\n",
       "\n",
       "       [[142.31543 , 148.73022 , 155.31543 ],\n",
       "        [142.52686 , 149.68872 , 155.68872 ],\n",
       "        [140.75732 , 147.49585 , 152.63232 ],\n",
       "        ...,\n",
       "        [130.19897 , 134.73438 , 140.45728 ],\n",
       "        [ 86.85889 ,  87.47827 ,  92.84326 ],\n",
       "        [ 59.34375 ,  53.96167 ,  60.546875]],\n",
       "\n",
       "       [[141.72632 , 149.4607  , 156.      ],\n",
       "        [140.82104 , 149.3523  , 155.3523  ],\n",
       "        [139.05762 , 147.2854  , 152.95728 ],\n",
       "        ...,\n",
       "        [129.44409 , 133.44409 , 139.92188 ],\n",
       "        [ 84.356445,  85.286865,  91.286865],\n",
       "        [ 61.772705,  57.967773,  63.967773]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "epochs=10\n",
    "decay=learning_rate/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 128, 128, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.30399242, 0.185027  , 0.092536  ],\n",
       "         [0.31990272, 0.2140874 , 0.13038641],\n",
       "         [0.34843653, 0.2601304 , 0.17743567],\n",
       "         ...,\n",
       "         [0.3458075 , 0.30689818, 0.21818896],\n",
       "         [0.36862266, 0.3373784 , 0.23532283],\n",
       "         [0.35128292, 0.32462853, 0.20428348]],\n",
       "\n",
       "        [[0.30861768, 0.18083256, 0.08404661],\n",
       "         [0.33591166, 0.22355622, 0.14210038],\n",
       "         [0.35368127, 0.25826058, 0.17979665],\n",
       "         ...,\n",
       "         [0.3448491 , 0.30096027, 0.23319069],\n",
       "         [0.35998774, 0.31780887, 0.24323778],\n",
       "         [0.34881282, 0.3041437 , 0.22469267]],\n",
       "\n",
       "        [[0.32380036, 0.19255036, 0.09683958],\n",
       "         [0.3301796 , 0.20991403, 0.12230775],\n",
       "         [0.371425  , 0.27209905, 0.19845377],\n",
       "         ...,\n",
       "         [0.32945293, 0.2823941 , 0.22527957],\n",
       "         [0.33235294, 0.28426874, 0.2259354 ],\n",
       "         [0.3491326 , 0.3011288 , 0.24061543]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.55940086, 0.58363014, 0.6100395 ],\n",
       "         [0.56367666, 0.58774126, 0.61127067],\n",
       "         [0.5596    , 0.5844774 , 0.6053721 ],\n",
       "         ...,\n",
       "         [0.51693857, 0.5343951 , 0.5565765 ],\n",
       "         [0.35847598, 0.36025295, 0.3808862 ],\n",
       "         [0.26417357, 0.24049576, 0.26392847]],\n",
       "\n",
       "        [[0.55809975, 0.58325577, 0.60908014],\n",
       "         [0.55892885, 0.5870146 , 0.610544  ],\n",
       "         [0.5519895 , 0.5784151 , 0.5985581 ],\n",
       "         ...,\n",
       "         [0.51058424, 0.5283701 , 0.55081284],\n",
       "         [0.34062308, 0.34305206, 0.36409122],\n",
       "         [0.23272058, 0.21161439, 0.23743872]],\n",
       "\n",
       "        [[0.5557895 , 0.58612037, 0.6117647 ],\n",
       "         [0.5522394 , 0.58569527, 0.6092247 ],\n",
       "         [0.54532397, 0.5775898 , 0.5998325 ],\n",
       "         ...,\n",
       "         [0.5076239 , 0.5233102 , 0.5487132 ],\n",
       "         [0.3308096 , 0.3344583 , 0.3579877 ],\n",
       "         [0.2422459 , 0.2273246 , 0.25085402]]],\n",
       "\n",
       "\n",
       "       [[[0.21954848, 0.1882372 , 0.1647078 ],\n",
       "         [0.26123622, 0.22146906, 0.19836569],\n",
       "         [0.27052218, 0.2145929 , 0.1992647 ],\n",
       "         ...,\n",
       "         [0.4678309 , 0.4083898 , 0.33057597],\n",
       "         [0.51346606, 0.48172295, 0.40329158],\n",
       "         [0.5015989 , 0.4468214 , 0.3723116 ]],\n",
       "\n",
       "        [[0.22426184, 0.19276386, 0.16923445],\n",
       "         [0.246875  , 0.21496822, 0.19142157],\n",
       "         [0.25994083, 0.2087297 , 0.18121745],\n",
       "         ...,\n",
       "         [0.46552446, 0.39868835, 0.32954198],\n",
       "         [0.462613  , 0.40269512, 0.33265835],\n",
       "         [0.4309465 , 0.3718137 , 0.29338524]],\n",
       "\n",
       "        [[0.26594093, 0.21526214, 0.19118604],\n",
       "         [0.25217715, 0.21726505, 0.1850203 ],\n",
       "         [0.27195734, 0.24809761, 0.19683479],\n",
       "         ...,\n",
       "         [0.40801644, 0.34043065, 0.28583506],\n",
       "         [0.39417508, 0.29576823, 0.24088062],\n",
       "         [0.37618336, 0.28966376, 0.19959502]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.32248294, 0.18620366, 0.11880649],\n",
       "         [0.31689358, 0.19091319, 0.10809877],\n",
       "         [0.33000058, 0.1927696 , 0.10769665],\n",
       "         ...,\n",
       "         [0.3851371 , 0.31452492, 0.2593166 ],\n",
       "         [0.36521044, 0.3137255 , 0.25883788],\n",
       "         [0.3572304 , 0.3098652 , 0.24687979]],\n",
       "\n",
       "        [[0.3217467 , 0.18044768, 0.12566827],\n",
       "         [0.32159448, 0.20806143, 0.12945485],\n",
       "         [0.36215726, 0.24389744, 0.16108303],\n",
       "         ...,\n",
       "         [0.38097617, 0.32184628, 0.26302275],\n",
       "         [0.3759105 , 0.317446  , 0.25488472],\n",
       "         [0.34268823, 0.28797296, 0.22914656]],\n",
       "\n",
       "        [[0.33315334, 0.2076641 , 0.12157341],\n",
       "         [0.32983494, 0.2200339 , 0.14171933],\n",
       "         [0.3104023 , 0.19275045, 0.1106991 ],\n",
       "         ...,\n",
       "         [0.36838236, 0.30202207, 0.2351103 ],\n",
       "         [0.37647635, 0.30220875, 0.23168467],\n",
       "         [0.36874905, 0.2943618 , 0.2277564 ]]],\n",
       "\n",
       "\n",
       "       [[[0.19215088, 0.11372549, 0.08611534],\n",
       "         [0.20573443, 0.13956323, 0.11163474],\n",
       "         [0.2015005 , 0.12849121, 0.08777501],\n",
       "         ...,\n",
       "         [0.546352  , 0.5385089 , 0.52989984],\n",
       "         [0.500112  , 0.49951723, 0.49559566],\n",
       "         [0.50935036, 0.48582095, 0.48190534]],\n",
       "\n",
       "        [[0.328799  , 0.25474876, 0.17314382],\n",
       "         [0.21011771, 0.114782  , 0.08814242],\n",
       "         [0.2342132 , 0.17707688, 0.13258727],\n",
       "         ...,\n",
       "         [0.5264991 , 0.5166645 , 0.51237315],\n",
       "         [0.4890867 , 0.48511127, 0.46942496],\n",
       "         [0.48934877, 0.4745816 , 0.46327648]],\n",
       "\n",
       "        [[0.32689878, 0.24034783, 0.15027502],\n",
       "         [0.27325583, 0.20726533, 0.15113789],\n",
       "         [0.23930927, 0.17503232, 0.13428476],\n",
       "         ...,\n",
       "         [0.53463995, 0.52272564, 0.510195  ],\n",
       "         [0.47715107, 0.4723738 , 0.47246358],\n",
       "         [0.5007018 , 0.49359465, 0.4770192 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.40471742, 0.33385274, 0.2421248 ],\n",
       "         [0.37396264, 0.28768814, 0.20934652],\n",
       "         [0.30085975, 0.21249473, 0.14022504],\n",
       "         ...,\n",
       "         [0.4576665 , 0.44967377, 0.44259655],\n",
       "         [0.4003327 , 0.40786946, 0.39371505],\n",
       "         [0.3440535 , 0.3478518 , 0.33290154]],\n",
       "\n",
       "        [[0.40841997, 0.3266798 , 0.23665485],\n",
       "         [0.34360975, 0.26125678, 0.17579369],\n",
       "         [0.39650303, 0.3338214 , 0.25922182],\n",
       "         ...,\n",
       "         [0.45453215, 0.44515717, 0.4459231 ],\n",
       "         [0.43448272, 0.43050727, 0.42312372],\n",
       "         [0.42731574, 0.41857144, 0.419031  ]],\n",
       "\n",
       "        [[0.3284747 , 0.2616608 , 0.19060107],\n",
       "         [0.34395394, 0.26850706, 0.20243518],\n",
       "         [0.38183427, 0.31817004, 0.25318843],\n",
       "         ...,\n",
       "         [0.44218463, 0.4343415 , 0.4343415 ],\n",
       "         [0.45206323, 0.43697175, 0.44091126],\n",
       "         [0.40601832, 0.39063242, 0.40209076]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.29032826, 0.24542937, 0.19359681],\n",
       "         [0.29245487, 0.25532424, 0.19506076],\n",
       "         [0.30209643, 0.2563419 , 0.19725575],\n",
       "         ...,\n",
       "         [0.20027149, 0.15563522, 0.14146549],\n",
       "         [0.23705055, 0.19862796, 0.16301216],\n",
       "         [0.2541106 , 0.20894733, 0.1737854 ]],\n",
       "\n",
       "        [[0.3082926 , 0.25475746, 0.19697559],\n",
       "         [0.31254104, 0.25979382, 0.19488879],\n",
       "         [0.3195558 , 0.26638278, 0.20179947],\n",
       "         ...,\n",
       "         [0.18266392, 0.16683313, 0.1707547 ],\n",
       "         [0.17488787, 0.13726011, 0.14255024],\n",
       "         [0.16949715, 0.1146683 , 0.10905995]],\n",
       "\n",
       "        [[0.3185027 , 0.2510985 , 0.16573001],\n",
       "         [0.34570616, 0.25989786, 0.1780438 ],\n",
       "         [0.35619298, 0.29734305, 0.2156071 ],\n",
       "         ...,\n",
       "         [0.20583382, 0.18625237, 0.18326949],\n",
       "         [0.18416922, 0.1752165 , 0.17784262],\n",
       "         [0.17423646, 0.14172165, 0.13431834]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.28028715, 0.22546177, 0.16690087],\n",
       "         [0.31556374, 0.26476052, 0.18985243],\n",
       "         [0.26958826, 0.18524392, 0.11011712],\n",
       "         ...,\n",
       "         [0.3870013 , 0.26844195, 0.22009636],\n",
       "         [0.41682446, 0.28197458, 0.22236316],\n",
       "         [0.431458  , 0.31405604, 0.24420531]],\n",
       "\n",
       "        [[0.25778186, 0.18575367, 0.12477195],\n",
       "         [0.27530637, 0.21745312, 0.14783517],\n",
       "         [0.29139093, 0.21738   , 0.14477634],\n",
       "         ...,\n",
       "         [0.3406536 , 0.29361877, 0.17724316],\n",
       "         [0.34157997, 0.2872448 , 0.16413909],\n",
       "         [0.34845456, 0.28516147, 0.1662259 ]],\n",
       "\n",
       "        [[0.25778013, 0.19503503, 0.13228993],\n",
       "         [0.2939335 , 0.23687163, 0.17700471],\n",
       "         [0.2298058 , 0.15077257, 0.10267209],\n",
       "         ...,\n",
       "         [0.25227582, 0.28082755, 0.07448342],\n",
       "         [0.25026214, 0.28515798, 0.09228115],\n",
       "         [0.2448433 , 0.2836607 , 0.10849789]]],\n",
       "\n",
       "\n",
       "       [[[0.5786034 , 0.5731409 , 0.5698015 ],\n",
       "         [0.5985129 , 0.5885369 , 0.58900005],\n",
       "         [0.5954979 , 0.59157634, 0.5918666 ],\n",
       "         ...,\n",
       "         [0.19002524, 0.1983739 , 0.19851097],\n",
       "         [0.14268476, 0.12699848, 0.14147812],\n",
       "         [0.15164813, 0.13371001, 0.14772657]],\n",
       "\n",
       "        [[0.5810167 , 0.57429534, 0.5749199 ],\n",
       "         [0.6012703 , 0.5816625 , 0.57978886],\n",
       "         [0.59444267, 0.5787564 , 0.5831835 ],\n",
       "         ...,\n",
       "         [0.19086777, 0.18549326, 0.19093138],\n",
       "         [0.17036545, 0.1389929 , 0.16047435],\n",
       "         [0.15786551, 0.1364265 , 0.15786551]],\n",
       "\n",
       "        [[0.5784812 , 0.5640342 , 0.5620742 ],\n",
       "         [0.5869226 , 0.5702206 , 0.5657935 ],\n",
       "         [0.5921569 , 0.5882353 , 0.5882353 ],\n",
       "         ...,\n",
       "         [0.19082025, 0.18898591, 0.19777492],\n",
       "         [0.16718283, 0.14902793, 0.16376677],\n",
       "         [0.16245094, 0.14647438, 0.16833407]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6309275 , 0.62475413, 0.62279254],\n",
       "         [0.6405967 , 0.6405967 , 0.6400912 ],\n",
       "         [0.62374043, 0.627662  , 0.6272217 ],\n",
       "         ...,\n",
       "         [0.34249753, 0.26406616, 0.17431042],\n",
       "         [0.34952512, 0.2793022 , 0.20014021],\n",
       "         [0.3196821 , 0.25642994, 0.17174856]],\n",
       "\n",
       "        [[0.63016593, 0.6357572 , 0.6392157 ],\n",
       "         [0.6308882 , 0.6392157 , 0.63450813],\n",
       "         [0.6342065 , 0.63005984, 0.63638175],\n",
       "         ...,\n",
       "         [0.35047254, 0.26180598, 0.16646053],\n",
       "         [0.34256575, 0.26914355, 0.18208307],\n",
       "         [0.31427342, 0.2436852 , 0.16295956]],\n",
       "\n",
       "        [[0.6392157 , 0.63529414, 0.6306616 ],\n",
       "         [0.64251274, 0.64251274, 0.6431373 ],\n",
       "         [0.6387102 , 0.6313726 , 0.627451  ],\n",
       "         ...,\n",
       "         [0.3950674 , 0.3029113 , 0.20545265],\n",
       "         [0.32382047, 0.2393347 , 0.15543105],\n",
       "         [0.38529843, 0.3072013 , 0.22697137]]],\n",
       "\n",
       "\n",
       "       [[[0.37555936, 0.32797182, 0.27058825],\n",
       "         [0.3997774 , 0.34656742, 0.29155967],\n",
       "         [0.39639774, 0.35302424, 0.28762662],\n",
       "         ...,\n",
       "         [0.37542892, 0.27555147, 0.16037717],\n",
       "         [0.3815645 , 0.30402857, 0.20271116],\n",
       "         [0.33936983, 0.28003338, 0.21741872]],\n",
       "\n",
       "        [[0.3861924 , 0.33665198, 0.27509478],\n",
       "         [0.37449998, 0.32859558, 0.27333578],\n",
       "         [0.37598446, 0.3436356 , 0.289377  ],\n",
       "         ...,\n",
       "         [0.32077205, 0.23259397, 0.15495917],\n",
       "         [0.31947666, 0.26302204, 0.19711149],\n",
       "         [0.31393707, 0.25471526, 0.19485007]],\n",
       "\n",
       "        [[0.41401008, 0.36879835, 0.30357164],\n",
       "         [0.39695063, 0.35111323, 0.28156668],\n",
       "         [0.41055644, 0.37198415, 0.3211093 ],\n",
       "         ...,\n",
       "         [0.31743595, 0.2507693 , 0.18071887],\n",
       "         [0.32213637, 0.25194642, 0.19992436],\n",
       "         [0.3266197 , 0.2673237 , 0.21362545]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.31443086, 0.27665514, 0.22920687],\n",
       "         [0.31064954, 0.27471206, 0.2149122 ],\n",
       "         [0.3101749 , 0.25774094, 0.18843946],\n",
       "         ...,\n",
       "         [0.40043226, 0.31415775, 0.20827541],\n",
       "         [0.3635498 , 0.2739971 , 0.17872338],\n",
       "         [0.32939333, 0.23839183, 0.17460507]],\n",
       "\n",
       "        [[0.32508498, 0.2819477 , 0.23632884],\n",
       "         [0.33554664, 0.28068513, 0.21441674],\n",
       "         [0.35546565, 0.2762377 , 0.20629284],\n",
       "         ...,\n",
       "         [0.3927411 , 0.30588856, 0.20313123],\n",
       "         [0.41960785, 0.34430146, 0.24269852],\n",
       "         [0.2933419 , 0.20573324, 0.10611477]],\n",
       "\n",
       "        [[0.32941177, 0.26706496, 0.2028799 ],\n",
       "         [0.33413684, 0.27372712, 0.19410783],\n",
       "         [0.38119757, 0.3048584 , 0.23362678],\n",
       "         ...,\n",
       "         [0.3720677 , 0.28100082, 0.17944719],\n",
       "         [0.34726754, 0.26059476, 0.17158657],\n",
       "         [0.25876513, 0.17982082, 0.12046162]]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 128, 128, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=Sequential()\n",
    "\n",
    "#1st Convolutional Block\n",
    "model1.add(Conv2D(32,(5,5),padding=\"same\",input_shape=(128,128,3)))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "model1.add(Conv2D(64,(5,5),padding=\"same\"))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "model1.add(Conv2D(128,(5,5),padding=\"same\"))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Flattening the Output from Convolutional Layer\n",
    "model1.add(Flatten())\n",
    "\n",
    "#Add 1st Dense Layer\n",
    "model1.add(Dense(128))\n",
    "model1.add(Activation(\"relu\"))\n",
    "\n",
    "#Add 2nd Dense Layer\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation(\"relu\"))\n",
    "\n",
    "#Add the Output Layer\n",
    "model1.add(Dense(12))\n",
    "model1.add(Activation(\"softmax\"))\n",
    "\n",
    "#Adding Optimizer\n",
    "optimizer=Adam(learning_rate=learning_rate)\n",
    "\n",
    "#Compile Model\n",
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 128, 128, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                780       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 4,462,092\n",
      "Trainable params: 4,462,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3325 samples, validate on 1425 samples\n",
      "Epoch 1/10\n",
      "3325/3325 [==============================] - 270s 81ms/step - loss: 2.3287 - accuracy: 0.1940 - val_loss: 1.9600 - val_accuracy: 0.3270\n",
      "Epoch 2/10\n",
      "3325/3325 [==============================] - 259s 78ms/step - loss: 1.6269 - accuracy: 0.4235 - val_loss: 1.4599 - val_accuracy: 0.4849\n",
      "Epoch 3/10\n",
      "3325/3325 [==============================] - 248s 75ms/step - loss: 1.3171 - accuracy: 0.5411 - val_loss: 1.3415 - val_accuracy: 0.5537\n",
      "Epoch 4/10\n",
      "3325/3325 [==============================] - 244s 73ms/step - loss: 1.0006 - accuracy: 0.6629 - val_loss: 1.1040 - val_accuracy: 0.6091\n",
      "Epoch 5/10\n",
      "3325/3325 [==============================] - 233s 70ms/step - loss: 0.8513 - accuracy: 0.7098 - val_loss: 1.1186 - val_accuracy: 0.6175\n",
      "Epoch 6/10\n",
      "3325/3325 [==============================] - 231s 69ms/step - loss: 0.6762 - accuracy: 0.7645 - val_loss: 0.8825 - val_accuracy: 0.6989\n",
      "Epoch 7/10\n",
      "3325/3325 [==============================] - 236s 71ms/step - loss: 0.5520 - accuracy: 0.8120 - val_loss: 0.9302 - val_accuracy: 0.7032\n",
      "Epoch 8/10\n",
      "3325/3325 [==============================] - 246s 74ms/step - loss: 0.4630 - accuracy: 0.8391 - val_loss: 0.9039 - val_accuracy: 0.7144\n",
      "Epoch 9/10\n",
      "3325/3325 [==============================] - 233s 70ms/step - loss: 0.3354 - accuracy: 0.8833 - val_loss: 0.9990 - val_accuracy: 0.7228\n",
      "Epoch 10/10\n",
      "3325/3325 [==============================] - 239s 72ms/step - loss: 0.3114 - accuracy: 0.8872 - val_loss: 1.0984 - val_accuracy: 0.6828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x11b05772dd8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train,y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          epochs=epochs,\n",
    "          batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Training Accuracy is quite higher than Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Data Augementation and Dropouts and then  Training the same CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (49152, 49152) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-70296275c7bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Prepare the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Augmented images are not generated at datagen.fit() command rather this command just calculate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, augment, rounds, seed)\u001b[0m\n\u001b[0;32m    973\u001b[0m             flat_x = np.reshape(\n\u001b[0;32m    974\u001b[0m                 x, (x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))\n\u001b[1;32m--> 975\u001b[1;33m             \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_x\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mflat_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m             \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[0ms_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzca_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (49152, 49152) and data type float32"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=True,  # apply ZCA whitening\n",
    "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Prepare the generator\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Augmented images are not generated at datagen.fit() command rather this command just calculate \n",
    "# the statistics required to do the image transformation\n",
    "# Deep Learning Model fitting process will iterate this datagen API to generate the \n",
    "# augmented images just-in-time\n",
    "# will prevent memory overload but will be a time consuming operation during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = datagen.flow(X_train[:1], batch_size=1)\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "\n",
    "#1st Convolutional Block\n",
    "model2.add(Conv2D(32,(5,5),padding=\"same\",input_shape=(128,128,3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "model2.add(Conv2D(64,(5,5),padding=\"same\"))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "model2.add(Conv2D(128,(5,5),padding=\"same\"))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Adding Dropout Layer\n",
    "model2.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "#Flattening the Output from Convolutional Layer\n",
    "model2.add(Flatten())\n",
    "\n",
    "#Add 1st Dense Layer\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation(\"relu\"))\n",
    "\n",
    "#Adding another dropout Layer\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "#Add 2nd Dense Layer\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation(\"relu\"))\n",
    "\n",
    "#Add the Output Layer\n",
    "model2.add(Dense(12))\n",
    "model2.add(Activation(\"softmax\"))\n",
    "\n",
    "#Adding Optimizer\n",
    "optimizer=Adam(learning_rate=learning_rate)\n",
    "\n",
    "#Compile Model\n",
    "model2.compile(loss=\"categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "#Early Stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=7, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit_generator(datagen.flow(X_train, y_train,batch_size=32),\n",
    "                     samples_per_epoch=X_train.shape[0],\n",
    "                     epochs=10,\n",
    "                     validation_data=(X_test, y_test), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement VGG16 to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr_VGG=[]\n",
    "labels_VGG=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=os.listdir(\"D://Project2_AfterResidency7_CNN//train\")\n",
    "for directory in train_dir:\n",
    "    images=os.listdir(\"D://Project2_AfterResidency7_CNN//train//\"+directory)\n",
    "    for image in images:\n",
    "        img=load_img(\"D://Project2_AfterResidency7_CNN//train//\"+directory+\"//\"+image)\n",
    "        img_arr=img_to_array(img)\n",
    "        img_arr=cv2.resize(img_arr,(224,224))\n",
    "        image_arr_VGG.append(img_arr)\n",
    "        label=convert_classes_to_label(directory)\n",
    "        labels_VGG.append(label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr_VGG[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr_VGG=np.array(image_arr_VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr_VGG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VGG=image_arr_VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_VGG=labels_VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VGG/=255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VGG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_VGG=to_categorical(y_VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_VGG[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainVGG,X_testVGG,y_trainVGG,y_testVGG=train_test_split(X_VGG,y_VGG,test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG=Sequential()\n",
    "\n",
    "#1st Block\n",
    "model_VGG.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),\n",
    "                     padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "#2nd Block\n",
    "model_VGG.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "#3rd Block\n",
    "model_VGG.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "#4th Block\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "#5th Block\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model_VGG.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "#Add Fully Connected Layer\n",
    "model_VGG.add(Flatten())\n",
    "model_VGG.add(Dense(units=4096,activation=\"relu\"))\n",
    "model_VGG.add(Dense(units=4096,activation=\"relu\"))\n",
    "model_VGG.add(Dense(units=12,activation=\"softmax\"))\n",
    "optimizer=Adam(lr=0.001)\n",
    "model_VGG.compile(loss=\"categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_VGG.fit(X_trainVGG,y_trainVGG,\n",
    "#          validation_data=(X_testVGG,y_testVGG),\n",
    "#          epochs=epochs,\n",
    "#          batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System and even google colabs got crashed while trying to run VGG\n",
    "\n",
    "Also as per my observation..I definitely need to use VGG or Resnet to improve accuracy of this model\n",
    "\n",
    "But due to hardware limitations, unable to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data.drop([\"date\",\"symbol\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data[\"volume\"]=data[\"volume\"]/1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data.drop([\"volume\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=data[\"volume\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(features,target,test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "X_train=X_train.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer=Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=normalizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array([y_train]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.array([y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of initializing the weights and bias to zeros, I am initializing with the random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal(shape=[4,1]))\n",
    "b=tf.Variable(tf.random_normal(shape=[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am following my way to do the model prediction and evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(name=\"X\",shape=[None,4],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.placeholder(name=\"y\",shape=[None,1],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=tf.add(tf.matmul(X,w),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.squared_difference(y,output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is: [[ 2.4831676 ]\n",
      " [-0.2243042 ]\n",
      " [ 0.87348723]\n",
      " [-0.49836338]] \n",
      "b is: [[3.8589706]] \n",
      "loss: 22.241512\n",
      "\n",
      "\n",
      "w is: [[ 2.4550436 ]\n",
      " [-0.25235844]\n",
      " [ 0.84555936]\n",
      " [-0.5267071 ]] \n",
      "b is: [[3.8027444]] \n",
      "loss: 11.463899\n",
      "\n",
      "\n",
      "w is: [[ 2.51068   ]\n",
      " [-0.19644997]\n",
      " [ 0.9001979 ]\n",
      " [-0.47024885]] \n",
      "b is: [[3.91408]] \n",
      "loss: 210.10483\n",
      "\n",
      "\n",
      "w is: [[ 2.4596086 ]\n",
      " [-0.24749777]\n",
      " [ 0.84956604]\n",
      " [-0.5218063 ]] \n",
      "b is: [[3.8119216]] \n",
      "loss: 10.308175\n",
      "\n",
      "\n",
      "w is: [[ 2.4554458 ]\n",
      " [-0.25167537]\n",
      " [ 0.84518945]\n",
      " [-0.52588063]] \n",
      "b is: [[3.8035285]] \n",
      "loss: 30.512056\n",
      "\n",
      "\n",
      "w is: [[ 2.4595327 ]\n",
      " [-0.24757914]\n",
      " [ 0.849309  ]\n",
      " [-0.5216145 ]] \n",
      "b is: [[3.8118148]] \n",
      "loss: 88.13758\n",
      "\n",
      "\n",
      "w is: [[ 2.4594367 ]\n",
      " [-0.24719578]\n",
      " [ 0.8491038 ]\n",
      " [-0.52101094]] \n",
      "b is: [[3.812172]] \n",
      "loss: 35.345776\n",
      "\n",
      "\n",
      "w is: [[ 2.459515  ]\n",
      " [-0.24617825]\n",
      " [ 0.8493292 ]\n",
      " [-0.51990527]] \n",
      "b is: [[3.813404]] \n",
      "loss: 23.386515\n",
      "\n",
      "\n",
      "w is: [[ 2.469236  ]\n",
      " [-0.23686334]\n",
      " [ 0.8580864 ]\n",
      " [-0.5099434 ]] \n",
      "b is: [[3.8323038]] \n",
      "loss: 42.152386\n",
      "\n",
      "\n",
      "w is: [[ 2.486689  ]\n",
      " [-0.21942371]\n",
      " [ 0.8746635 ]\n",
      " [-0.49177498]] \n",
      "b is: [[3.8671432]] \n",
      "loss: 54.948\n",
      "\n",
      "\n",
      "w is: [[ 2.4358664 ]\n",
      " [-0.27022892]\n",
      " [ 0.82432735]\n",
      " [-0.54301274]] \n",
      "b is: [[3.765539]] \n",
      "loss: 11.287585\n",
      "\n",
      "\n",
      "w is: [[ 2.4217367 ]\n",
      " [-0.28511286]\n",
      " [ 0.80962497]\n",
      " [-0.55747366]] \n",
      "b is: [[3.7364554]] \n",
      "loss: 32.443825\n",
      "\n",
      "\n",
      "w is: [[ 2.4028308 ]\n",
      " [-0.3036718 ]\n",
      " [ 0.79117686]\n",
      " [-0.57655627]] \n",
      "b is: [[3.6989543]] \n",
      "loss: 26.517479\n",
      "\n",
      "\n",
      "w is: [[ 2.4098132 ]\n",
      " [-0.29622135]\n",
      " [ 0.79831696]\n",
      " [-0.56923205]] \n",
      "b is: [[3.7134047]] \n",
      "loss: 66.05838\n",
      "\n",
      "\n",
      "w is: [[ 2.399862  ]\n",
      " [-0.30642322]\n",
      " [ 0.78813803]\n",
      " [-0.5794139 ]] \n",
      "b is: [[3.693148]] \n",
      "loss: 34.45302\n",
      "\n",
      "\n",
      "w is: [[ 2.449395 ]\n",
      " [-0.2558031]\n",
      " [ 0.8368614]\n",
      " [-0.5285691]] \n",
      "b is: [[3.7930195]] \n",
      "loss: 479.01126\n",
      "\n",
      "\n",
      "w is: [[ 2.5048857 ]\n",
      " [-0.20079195]\n",
      " [ 0.89061207]\n",
      " [-0.47261128]] \n",
      "b is: [[3.9031343]] \n",
      "loss: 505.14752\n",
      "\n",
      "\n",
      "w is: [[ 2.4930806 ]\n",
      " [-0.21246816]\n",
      " [ 0.8790921 ]\n",
      " [-0.4842443 ]] \n",
      "b is: [[3.8798168]] \n",
      "loss: 27.46636\n",
      "\n",
      "\n",
      "w is: [[ 2.4612637 ]\n",
      " [-0.24438389]\n",
      " [ 0.8473496 ]\n",
      " [-0.51622766]] \n",
      "b is: [[3.816088]] \n",
      "loss: 18.732492\n",
      "\n",
      "\n",
      "w is: [[ 2.457106  ]\n",
      " [-0.24835683]\n",
      " [ 0.84317416]\n",
      " [-0.52027196]] \n",
      "b is: [[3.8079143]] \n",
      "loss: 57.320724\n",
      "\n",
      "\n",
      "w is: [[ 2.445337  ]\n",
      " [-0.25980982]\n",
      " [ 0.8316312 ]\n",
      " [-0.53194964]] \n",
      "b is: [[3.7846937]] \n",
      "loss: 20.783484\n",
      "\n",
      "\n",
      "w is: [[ 2.4091904]\n",
      " [-0.295962 ]\n",
      " [ 0.7957715]\n",
      " [-0.56835  ]] \n",
      "b is: [[3.7124126]] \n",
      "loss: 9.147521\n",
      "\n",
      "\n",
      "w is: [[ 2.4233298 ]\n",
      " [-0.28143612]\n",
      " [ 0.8099909 ]\n",
      " [-0.5537985 ]] \n",
      "b is: [[3.7411327]] \n",
      "loss: 84.8975\n",
      "\n",
      "\n",
      "w is: [[ 2.4120839]\n",
      " [-0.2920922]\n",
      " [ 0.7991633]\n",
      " [-0.5647694]] \n",
      "b is: [[3.7192826]] \n",
      "loss: 56.151665\n",
      "\n",
      "\n",
      "w is: [[ 2.408058  ]\n",
      " [-0.29547632]\n",
      " [ 0.7955215 ]\n",
      " [-0.56853926]] \n",
      "b is: [[3.71187]] \n",
      "loss: 29.297493\n",
      "\n",
      "\n",
      "w is: [[ 2.5216033 ]\n",
      " [-0.18643129]\n",
      " [ 0.90312797]\n",
      " [-0.4548304 ]] \n",
      "b is: [[3.9338784]] \n",
      "loss: 1570.5977\n",
      "\n",
      "\n",
      "w is: [[ 2.5129395 ]\n",
      " [-0.19548754]\n",
      " [ 0.8944248 ]\n",
      " [-0.46377692]] \n",
      "b is: [[3.916193]] \n",
      "loss: 50.77755\n",
      "\n",
      "\n",
      "w is: [[ 2.4612246 ]\n",
      " [-0.24705565]\n",
      " [ 0.84332156]\n",
      " [-0.51587725]] \n",
      "b is: [[3.812945]] \n",
      "loss: 8.563356\n",
      "\n",
      "\n",
      "w is: [[ 2.4901116 ]\n",
      " [-0.21893619]\n",
      " [ 0.87132907]\n",
      " [-0.48629454]] \n",
      "b is: [[3.8702528]] \n",
      "loss: 256.41855\n",
      "\n",
      "\n",
      "w is: [[ 2.4711466 ]\n",
      " [-0.23762025]\n",
      " [ 0.8527018 ]\n",
      " [-0.5051626 ]] \n",
      "b is: [[3.83268]] \n",
      "loss: 30.07309\n",
      "\n",
      "\n",
      "w is: [[ 2.433746  ]\n",
      " [-0.27514333]\n",
      " [ 0.8155209 ]\n",
      " [-0.54311305]] \n",
      "b is: [[3.7576492]] \n",
      "loss: 9.912226\n",
      "\n",
      "\n",
      "w is: [[ 2.4299214 ]\n",
      " [-0.27950343]\n",
      " [ 0.8116491 ]\n",
      " [-0.5473852 ]] \n",
      "b is: [[3.749482]] \n",
      "loss: 59.797688\n",
      "\n",
      "\n",
      "w is: [[ 2.4689903]\n",
      " [-0.2395145]\n",
      " [ 0.8506913]\n",
      " [-0.5068344]] \n",
      "b is: [[3.8288174]] \n",
      "loss: 181.92896\n",
      "\n",
      "\n",
      "w is: [[ 2.4790258 ]\n",
      " [-0.22915675]\n",
      " [ 0.8606051 ]\n",
      " [-0.4964532 ]] \n",
      "b is: [[3.8491642]] \n",
      "loss: 82.45921\n",
      "\n",
      "\n",
      "w is: [[ 2.4585323 ]\n",
      " [-0.24946804]\n",
      " [ 0.84039795]\n",
      " [-0.51698273]] \n",
      "b is: [[3.8083916]] \n",
      "loss: 28.232954\n",
      "\n",
      "\n",
      "w is: [[ 2.4334779 ]\n",
      " [-0.27441886]\n",
      " [ 0.8158002 ]\n",
      " [-0.5423403 ]] \n",
      "b is: [[3.758407]] \n",
      "loss: 37.74805\n",
      "\n",
      "\n",
      "w is: [[ 2.4807186 ]\n",
      " [-0.22718382]\n",
      " [ 0.8617762 ]\n",
      " [-0.4944632 ]] \n",
      "b is: [[3.8525891]] \n",
      "loss: 305.96075\n",
      "\n",
      "\n",
      "w is: [[ 2.4613945 ]\n",
      " [-0.24689148]\n",
      " [ 0.84240925]\n",
      " [-0.5140352 ]] \n",
      "b is: [[3.8136034]] \n",
      "loss: 26.567284\n",
      "\n",
      "\n",
      "w is: [[ 2.4731913 ]\n",
      " [-0.23467351]\n",
      " [ 0.8543207 ]\n",
      " [-0.5018519 ]] \n",
      "b is: [[3.8376596]] \n",
      "loss: 41.56978\n",
      "\n",
      "\n",
      "w is: [[ 2.475351  ]\n",
      " [-0.23324701]\n",
      " [ 0.85581034]\n",
      " [-0.49974388]] \n",
      "b is: [[3.841261]] \n",
      "loss: 19.781355\n",
      "\n",
      "\n",
      "w is: [[ 2.460388  ]\n",
      " [-0.24869938]\n",
      " [ 0.8406157 ]\n",
      " [-0.5147998 ]] \n",
      "b is: [[3.8109336]] \n",
      "loss: 35.19834\n",
      "\n",
      "\n",
      "w is: [[ 2.4432712 ]\n",
      " [-0.26580665]\n",
      " [ 0.82343   ]\n",
      " [-0.5319417 ]] \n",
      "b is: [[3.776661]] \n",
      "loss: 12.358694\n",
      "\n",
      "\n",
      "w is: [[ 2.4667447 ]\n",
      " [-0.24287292]\n",
      " [ 0.8457728 ]\n",
      " [-0.5081214 ]] \n",
      "b is: [[3.822965]] \n",
      "loss: 79.536224\n",
      "\n",
      "\n",
      "w is: [[ 2.4806125 ]\n",
      " [-0.22929737]\n",
      " [ 0.8590518 ]\n",
      " [-0.49398014]] \n",
      "b is: [[3.850405]] \n",
      "loss: 34.834507\n",
      "\n",
      "\n",
      "w is: [[ 2.4663224 ]\n",
      " [-0.24377199]\n",
      " [ 0.84494126]\n",
      " [-0.5083432 ]] \n",
      "b is: [[3.8217852]] \n",
      "loss: 53.886215\n",
      "\n",
      "\n",
      "w is: [[ 2.4278555]\n",
      " [-0.2820307]\n",
      " [ 0.8067558]\n",
      " [-0.5469411]] \n",
      "b is: [[3.7450318]] \n",
      "loss: 11.9703455\n",
      "\n",
      "\n",
      "w is: [[ 2.4399452 ]\n",
      " [-0.26940873]\n",
      " [ 0.8189038 ]\n",
      " [-0.53391564]] \n",
      "b is: [[3.7699807]] \n",
      "loss: 68.27323\n",
      "\n",
      "\n",
      "w is: [[ 2.4211903]\n",
      " [-0.288206 ]\n",
      " [ 0.8004422]\n",
      " [-0.552839 ]] \n",
      "b is: [[3.7325096]] \n",
      "loss: 27.929388\n",
      "\n",
      "\n",
      "w is: [[ 2.4180398 ]\n",
      " [-0.29102126]\n",
      " [ 0.7973945 ]\n",
      " [-0.5558425 ]] \n",
      "b is: [[3.7265036]] \n",
      "loss: 18.57839\n",
      "\n",
      "\n",
      "w is: [[ 2.4032407 ]\n",
      " [-0.30561277]\n",
      " [ 0.7824408 ]\n",
      " [-0.5703664 ]] \n",
      "b is: [[3.6970785]] \n",
      "loss: 16.210775\n",
      "\n",
      "\n",
      "w is: [[ 2.3809278 ]\n",
      " [-0.3279248 ]\n",
      " [ 0.76020455]\n",
      " [-0.5927472 ]] \n",
      "b is: [[3.6524572]] \n",
      "loss: 18.295195\n",
      "\n",
      "\n",
      "w is: [[ 2.3764122 ]\n",
      " [-0.33268037]\n",
      " [ 0.75552726]\n",
      " [-0.5973392 ]] \n",
      "b is: [[3.6431875]] \n",
      "loss: 17.795656\n",
      "\n",
      "\n",
      "w is: [[ 2.4014673 ]\n",
      " [-0.30761737]\n",
      " [ 0.78014815]\n",
      " [-0.5724543 ]] \n",
      "b is: [[3.692999]] \n",
      "loss: 184.45197\n",
      "\n",
      "\n",
      "w is: [[ 2.4397566]\n",
      " [-0.2696873]\n",
      " [ 0.8173893]\n",
      " [-0.533976 ]] \n",
      "b is: [[3.7689767]] \n",
      "loss: 242.11894\n",
      "\n",
      "\n",
      "w is: [[ 2.391878 ]\n",
      " [-0.3175575]\n",
      " [ 0.769855 ]\n",
      " [-0.5821783]] \n",
      "b is: [[3.6732326]] \n",
      "loss: 10.141401\n",
      "\n",
      "\n",
      "w is: [[ 2.368459  ]\n",
      " [-0.34097895]\n",
      " [ 0.7466755 ]\n",
      " [-0.60586077]] \n",
      "b is: [[3.626379]] \n",
      "loss: 15.093158\n",
      "\n",
      "\n",
      "w is: [[ 2.3384397 ]\n",
      " [-0.37148982]\n",
      " [ 0.71677005]\n",
      " [-0.636709  ]] \n",
      "b is: [[3.5657284]] \n",
      "loss: 32.67052\n",
      "\n",
      "\n",
      "w is: [[ 2.3182633 ]\n",
      " [-0.39156213]\n",
      " [ 0.6967808 ]\n",
      " [-0.65687823]] \n",
      "b is: [[3.525525]] \n",
      "loss: 14.690794\n",
      "\n",
      "\n",
      "w is: [[ 2.30878   ]\n",
      " [-0.40090528]\n",
      " [ 0.68744063]\n",
      " [-0.66636026]] \n",
      "b is: [[3.5067]] \n",
      "loss: 45.044636\n",
      "\n",
      "\n",
      "w is: [[ 2.3090327 ]\n",
      " [-0.40094256]\n",
      " [ 0.68704826]\n",
      " [-0.66623634]] \n",
      "b is: [[3.5066788]] \n",
      "loss: 39.006676\n",
      "\n",
      "\n",
      "w is: [[ 2.2833195 ]\n",
      " [-0.42674726]\n",
      " [ 0.6616618 ]\n",
      " [-0.69228554]] \n",
      "b is: [[3.455199]] \n",
      "loss: 12.191964\n",
      "\n",
      "\n",
      "w is: [[ 2.268612  ]\n",
      " [-0.44148034]\n",
      " [ 0.646967  ]\n",
      " [-0.70696765]] \n",
      "b is: [[3.425791]] \n",
      "loss: 13.818463\n",
      "\n",
      "\n",
      "w is: [[ 2.29944   ]\n",
      " [-0.410834  ]\n",
      " [ 0.6773183 ]\n",
      " [-0.67479104]] \n",
      "b is: [[3.487808]] \n",
      "loss: 117.51083\n",
      "\n",
      "\n",
      "w is: [[ 2.2760987 ]\n",
      " [-0.4344538 ]\n",
      " [ 0.6539327 ]\n",
      " [-0.69841975]] \n",
      "b is: [[3.4408214]] \n",
      "loss: 9.337133\n",
      "\n",
      "\n",
      "w is: [[ 2.2735093 ]\n",
      " [-0.437744  ]\n",
      " [ 0.6508672 ]\n",
      " [-0.70097756]] \n",
      "b is: [[3.435075]] \n",
      "loss: 73.12425\n",
      "\n",
      "\n",
      "w is: [[ 2.2636704 ]\n",
      " [-0.44729766]\n",
      " [ 0.6413272 ]\n",
      " [-0.710795  ]] \n",
      "b is: [[3.415698]] \n",
      "loss: 24.063314\n",
      "\n",
      "\n",
      "w is: [[ 2.2704394]\n",
      " [-0.4409286]\n",
      " [ 0.6475588]\n",
      " [-0.7041173]] \n",
      "b is: [[3.428727]] \n",
      "loss: 31.161863\n",
      "\n",
      "\n",
      "w is: [[ 2.3458443 ]\n",
      " [-0.36599123]\n",
      " [ 0.72230643]\n",
      " [-0.62759906]] \n",
      "b is: [[3.5795405]] \n",
      "loss: 143.8806\n",
      "\n",
      "\n",
      "w is: [[ 2.4057133 ]\n",
      " [-0.30450422]\n",
      " [ 0.7810372 ]\n",
      " [-0.5658719 ]] \n",
      "b is: [[3.700467]] \n",
      "loss: 617.26056\n",
      "\n",
      "\n",
      "w is: [[ 2.6179318 ]\n",
      " [-0.10047272]\n",
      " [ 0.9847098 ]\n",
      " [-0.3526448 ]] \n",
      "b is: [[4.11714]] \n",
      "loss: 2708.2515\n",
      "\n",
      "\n",
      "w is: [[ 2.6404407]\n",
      " [-0.0782782]\n",
      " [ 1.0065134]\n",
      " [-0.3301436]] \n",
      "b is: [[4.1616473]] \n",
      "loss: 133.1637\n",
      "\n",
      "\n",
      "w is: [[ 2.7334225 ]\n",
      " [ 0.02746096]\n",
      " [ 1.0960603 ]\n",
      " [-0.22244519]] \n",
      "b is: [[4.360085]] \n",
      "loss: 1475.825\n",
      "\n",
      "\n",
      "w is: [[ 2.794167  ]\n",
      " [ 0.08808345]\n",
      " [ 1.156218  ]\n",
      " [-0.16125923]] \n",
      "b is: [[4.481448]] \n",
      "loss: 445.874\n",
      "\n",
      "\n",
      "w is: [[ 2.733473 ]\n",
      " [ 0.0277238]\n",
      " [ 1.0961436]\n",
      " [-0.2222447]] \n",
      "b is: [[4.360388]] \n",
      "loss: 27.053993\n",
      "\n",
      "\n",
      "w is: [[ 2.668945  ]\n",
      " [-0.0369729 ]\n",
      " [ 1.0322211 ]\n",
      " [-0.28744987]] \n",
      "b is: [[4.231206]] \n",
      "loss: 13.168769\n",
      "\n",
      "\n",
      "w is: [[ 2.6342943 ]\n",
      " [-0.0715344 ]\n",
      " [ 0.9976651 ]\n",
      " [-0.32231158]] \n",
      "b is: [[4.1618915]] \n",
      "loss: 42.246666\n",
      "\n",
      "\n",
      "w is: [[ 2.65283   ]\n",
      " [-0.05280855]\n",
      " [ 1.0161287 ]\n",
      " [-0.3032861 ]] \n",
      "b is: [[4.1992693]] \n",
      "loss: 71.532135\n",
      "\n",
      "\n",
      "w is: [[ 2.6441975 ]\n",
      " [-0.06146207]\n",
      " [ 1.0074148 ]\n",
      " [-0.31209975]] \n",
      "b is: [[4.1818614]] \n",
      "loss: 48.3396\n",
      "\n",
      "\n",
      "w is: [[ 2.6271324 ]\n",
      " [-0.07917827]\n",
      " [ 0.9900921 ]\n",
      " [-0.32931644]] \n",
      "b is: [[4.1472054]] \n",
      "loss: 48.488884\n",
      "\n",
      "\n",
      "w is: [[ 2.5663364 ]\n",
      " [-0.13983612]\n",
      " [ 0.9298976 ]\n",
      " [-0.39058644]] \n",
      "b is: [[4.0257425]] \n",
      "loss: 25.32633\n",
      "\n",
      "\n",
      "w is: [[ 2.5346732 ]\n",
      " [-0.17152229]\n",
      " [ 0.898571  ]\n",
      " [-0.42248243]] \n",
      "b is: [[3.9624548]] \n",
      "loss: 12.182821\n",
      "\n",
      "\n",
      "w is: [[ 2.521688  ]\n",
      " [-0.18431172]\n",
      " [ 0.8859272 ]\n",
      " [-0.43538672]] \n",
      "b is: [[3.9367905]] \n",
      "loss: 23.813591\n",
      "\n",
      "\n",
      "w is: [[ 2.482532  ]\n",
      " [-0.22348614]\n",
      " [ 0.8471819 ]\n",
      " [-0.47500122]] \n",
      "b is: [[3.858442]] \n",
      "loss: 25.071863\n",
      "\n",
      "\n",
      "w is: [[ 2.4736252 ]\n",
      " [-0.23266412]\n",
      " [ 0.83757323]\n",
      " [-0.48408943]] \n",
      "b is: [[3.8400605]] \n",
      "loss: 47.294033\n",
      "\n",
      "\n",
      "w is: [[ 2.4644036 ]\n",
      " [-0.24198805]\n",
      " [ 0.82826686]\n",
      " [-0.49336967]] \n",
      "b is: [[3.8214958]] \n",
      "loss: 25.172419\n",
      "\n",
      "\n",
      "w is: [[ 2.444637 ]\n",
      " [-0.261717 ]\n",
      " [ 0.8088305]\n",
      " [-0.513063 ]] \n",
      "b is: [[3.7821815]] \n",
      "loss: 157.90366\n",
      "\n",
      "\n",
      "w is: [[ 2.4059556 ]\n",
      " [-0.3004829 ]\n",
      " [ 0.77059203]\n",
      " [-0.55210567]] \n",
      "b is: [[3.7048137]] \n",
      "loss: 10.084319\n",
      "\n",
      "\n",
      "w is: [[ 2.4000225 ]\n",
      " [-0.30645916]\n",
      " [ 0.7648451 ]\n",
      " [-0.5582423 ]] \n",
      "b is: [[3.6929154]] \n",
      "loss: 88.36348\n",
      "\n",
      "\n",
      "w is: [[ 2.4445739 ]\n",
      " [-0.2613555 ]\n",
      " [ 0.80886066]\n",
      " [-0.5133649 ]] \n",
      "b is: [[3.7821941]] \n",
      "loss: 509.03546\n",
      "\n",
      "\n",
      "w is: [[ 2.410221  ]\n",
      " [-0.2956374 ]\n",
      " [ 0.77487546]\n",
      " [-0.54794043]] \n",
      "b is: [[3.7135944]] \n",
      "loss: 7.85996\n",
      "\n",
      "\n",
      "w is: [[ 2.4657083 ]\n",
      " [-0.24120653]\n",
      " [ 0.82845956]\n",
      " [-0.49192742]] \n",
      "b is: [[3.823373]] \n",
      "loss: 229.5181\n",
      "\n",
      "\n",
      "w is: [[ 2.4684327 ]\n",
      " [-0.24014829]\n",
      " [ 0.8295744 ]\n",
      " [-0.4892957 ]] \n",
      "b is: [[3.8271637]] \n",
      "loss: 60.138393\n",
      "\n",
      "\n",
      "w is: [[ 2.4450192 ]\n",
      " [-0.26354486]\n",
      " [ 0.80613554]\n",
      " [-0.5127932 ]] \n",
      "b is: [[3.7802927]] \n",
      "loss: 35.401558\n",
      "\n",
      "\n",
      "w is: [[ 2.4294565 ]\n",
      " [-0.27916282]\n",
      " [ 0.7906049 ]\n",
      " [-0.5283087 ]] \n",
      "b is: [[3.7491806]] \n",
      "loss: 13.680073\n",
      "\n",
      "\n",
      "w is: [[ 2.4610913 ]\n",
      " [-0.24820444]\n",
      " [ 0.8213926 ]\n",
      " [-0.49669826]] \n",
      "b is: [[3.811681]] \n",
      "loss: 96.78948\n",
      "\n",
      "\n",
      "w is: [[ 2.429948  ]\n",
      " [-0.27945942]\n",
      " [ 0.79037017]\n",
      " [-0.5280033 ]] \n",
      "b is: [[3.7493181]] \n",
      "loss: 15.105819\n",
      "\n",
      "\n",
      "w is: [[ 2.429451  ]\n",
      " [-0.2793573 ]\n",
      " [ 0.78947735]\n",
      " [-0.5274322 ]] \n",
      "b is: [[3.7489889]] \n",
      "loss: 37.587936\n",
      "\n",
      "\n",
      "w is: [[ 2.4188733 ]\n",
      " [-0.29039764]\n",
      " [ 0.7787596 ]\n",
      " [-0.5382752 ]] \n",
      "b is: [[3.7273993]] \n",
      "loss: 45.26349\n",
      "\n",
      "\n",
      "w is: [[ 2.4359047 ]\n",
      " [-0.27460474]\n",
      " [ 0.7946162 ]\n",
      " [-0.5214547 ]] \n",
      "b is: [[3.760159]] \n",
      "loss: 104.924515\n",
      "\n",
      "\n",
      "w is: [[ 2.4082735]\n",
      " [-0.3027224]\n",
      " [ 0.7669201]\n",
      " [-0.5495263]] \n",
      "b is: [[3.7044003]] \n",
      "loss: 18.604183\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    random_indices=np.random.choice(len(X_train),size=batch_size)\n",
    "    x_data=X_train[random_indices]\n",
    "    y_data=y_train[random_indices]\n",
    "    session.run(train,feed_dict={X:x_data,y:y_data})\n",
    "    print(\"w is:\",session.run(w),\\\n",
    "         \"\\nb is:\",session.run(b),\\\n",
    "         \"\\nloss:\",session.run(loss,feed_dict={X:x_data,y:y_data}))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=session.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.4082735],\n",
       "       [-0.3027224],\n",
       "       [ 0.7669201],\n",
       "       [-0.5495263]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=session.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7044003]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "train_predicted=session.run(output,feed_dict={X:X_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.858347 ],\n",
       "       [4.8635526],\n",
       "       [4.8504095],\n",
       "       ...,\n",
       "       [4.8670807],\n",
       "       [4.858364 ],\n",
       "       [4.848156 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse=session.run(loss,feed_dict={X:X_train,y:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.66138"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted=session.run(output,feed_dict={X:X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse=session.run(loss,feed_dict={X:X_test,y:y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155.93457"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.squared_difference(y,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "#Following the approach taught in the class to predict the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.zeros(shape=[4,1])\n",
    "b = tf.zeros(shape=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x, w, b):    \n",
    "    return(tf.add(tf.matmul(x, w),b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_actual, y_predicted):    \n",
    "    return(tf.reduce_mean(tf.squared_difference(y_actual,y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y_actual, w, b, learning_rate=0.01):\n",
    "    \n",
    "    #Record mathematical operations on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        t.watch([w,b])\n",
    "        #print(w)\n",
    "        current_prediction = prediction(x, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    \n",
    "        #print(\"current prediction:\",current_prediction.numpy())\n",
    "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    \n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Train loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_every_5th_iteration=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss on iteration 0 161.60574\n",
      "Current Loss on iteration 1 161.60576\n",
      "Current Loss on iteration 2 161.60571\n",
      "Current Loss on iteration 3 161.60571\n",
      "Current Loss on iteration 4 161.60571\n",
      "Current Loss on iteration 5 161.60568\n",
      "Current Loss on iteration 6 161.60565\n",
      "Current Loss on iteration 7 161.60562\n",
      "Current Loss on iteration 8 161.60562\n",
      "Current Loss on iteration 9 161.60558\n",
      "Current Loss on iteration 10 161.60558\n",
      "Current Loss on iteration 11 161.60558\n",
      "Current Loss on iteration 12 161.60555\n",
      "Current Loss on iteration 13 161.60555\n",
      "Current Loss on iteration 14 161.60556\n",
      "Current Loss on iteration 15 161.60553\n",
      "Current Loss on iteration 16 161.60553\n",
      "Current Loss on iteration 17 161.60551\n",
      "Current Loss on iteration 18 161.60548\n",
      "Current Loss on iteration 19 161.60547\n",
      "Current Loss on iteration 20 161.60544\n",
      "Current Loss on iteration 21 161.60542\n",
      "Current Loss on iteration 22 161.60544\n",
      "Current Loss on iteration 23 161.60541\n",
      "Current Loss on iteration 24 161.60538\n",
      "Current Loss on iteration 25 161.60536\n",
      "Current Loss on iteration 26 161.60536\n",
      "Current Loss on iteration 27 161.60536\n",
      "Current Loss on iteration 28 161.60532\n",
      "Current Loss on iteration 29 161.60532\n",
      "Current Loss on iteration 30 161.60529\n",
      "Current Loss on iteration 31 161.60526\n",
      "Current Loss on iteration 32 161.60526\n",
      "Current Loss on iteration 33 161.60524\n",
      "Current Loss on iteration 34 161.60522\n",
      "Current Loss on iteration 35 161.60522\n",
      "Current Loss on iteration 36 161.60522\n",
      "Current Loss on iteration 37 161.6052\n",
      "Current Loss on iteration 38 161.6052\n",
      "Current Loss on iteration 39 161.60518\n",
      "Current Loss on iteration 40 161.60516\n",
      "Current Loss on iteration 41 161.60515\n",
      "Current Loss on iteration 42 161.60512\n",
      "Current Loss on iteration 43 161.60512\n",
      "Current Loss on iteration 44 161.6051\n",
      "Current Loss on iteration 45 161.60509\n",
      "Current Loss on iteration 46 161.60504\n",
      "Current Loss on iteration 47 161.60503\n",
      "Current Loss on iteration 48 161.605\n",
      "Current Loss on iteration 49 161.605\n",
      "Current Loss on iteration 50 161.60498\n",
      "Current Loss on iteration 51 161.60498\n",
      "Current Loss on iteration 52 161.60495\n",
      "Current Loss on iteration 53 161.60495\n",
      "Current Loss on iteration 54 161.6049\n",
      "Current Loss on iteration 55 161.6049\n",
      "Current Loss on iteration 56 161.6049\n",
      "Current Loss on iteration 57 161.60489\n",
      "Current Loss on iteration 58 161.60487\n",
      "Current Loss on iteration 59 161.60484\n",
      "Current Loss on iteration 60 161.60484\n",
      "Current Loss on iteration 61 161.60483\n",
      "Current Loss on iteration 62 161.60483\n",
      "Current Loss on iteration 63 161.60478\n",
      "Current Loss on iteration 64 161.60478\n",
      "Current Loss on iteration 65 161.60478\n",
      "Current Loss on iteration 66 161.60474\n",
      "Current Loss on iteration 67 161.60474\n",
      "Current Loss on iteration 68 161.60474\n",
      "Current Loss on iteration 69 161.6047\n",
      "Current Loss on iteration 70 161.6047\n",
      "Current Loss on iteration 71 161.60469\n",
      "Current Loss on iteration 72 161.60468\n",
      "Current Loss on iteration 73 161.60466\n",
      "Current Loss on iteration 74 161.60464\n",
      "Current Loss on iteration 75 161.60461\n",
      "Current Loss on iteration 76 161.60457\n",
      "Current Loss on iteration 77 161.60457\n",
      "Current Loss on iteration 78 161.60457\n",
      "Current Loss on iteration 79 161.60454\n",
      "Current Loss on iteration 80 161.60454\n",
      "Current Loss on iteration 81 161.60452\n",
      "Current Loss on iteration 82 161.60449\n",
      "Current Loss on iteration 83 161.60449\n",
      "Current Loss on iteration 84 161.60446\n",
      "Current Loss on iteration 85 161.60446\n",
      "Current Loss on iteration 86 161.60445\n",
      "Current Loss on iteration 87 161.60443\n",
      "Current Loss on iteration 88 161.60442\n",
      "Current Loss on iteration 89 161.60437\n",
      "Current Loss on iteration 90 161.60439\n",
      "Current Loss on iteration 91 161.60437\n",
      "Current Loss on iteration 92 161.60434\n",
      "Current Loss on iteration 93 161.60434\n",
      "Current Loss on iteration 94 161.60432\n",
      "Current Loss on iteration 95 161.60431\n",
      "Current Loss on iteration 96 161.6043\n",
      "Current Loss on iteration 97 161.6043\n",
      "Current Loss on iteration 98 161.60426\n",
      "Current Loss on iteration 99 161.60425\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):    \n",
    "    w, b = train(X_train, y_train, w, b, learning_rate=0.01)\n",
    "    loss_value=loss(y_train,prediction(X_train,w,b)).numpy()\n",
    "    print('Current Loss on iteration', i,loss_value)\n",
    "    if i%5==0:\n",
    "          loss_every_5th_iteration.append(loss_value)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161.61401,\n",
       " 161.61166,\n",
       " 161.61006,\n",
       " 161.60902,\n",
       " 161.60828,\n",
       " 161.60776,\n",
       " 161.60738,\n",
       " 161.6071,\n",
       " 161.60687,\n",
       " 161.60672,\n",
       " 161.60661,\n",
       " 161.60649,\n",
       " 161.60638,\n",
       " 161.60634,\n",
       " 161.6062,\n",
       " 161.60614,\n",
       " 161.60606,\n",
       " 161.60599,\n",
       " 161.60588,\n",
       " 161.6058,\n",
       " 161.60574,\n",
       " 161.60568,\n",
       " 161.60558,\n",
       " 161.60553,\n",
       " 161.60544,\n",
       " 161.60536,\n",
       " 161.60529,\n",
       " 161.60522,\n",
       " 161.60516,\n",
       " 161.60509,\n",
       " 161.60498,\n",
       " 161.6049,\n",
       " 161.60484,\n",
       " 161.60478,\n",
       " 161.6047,\n",
       " 161.60461,\n",
       " 161.60454,\n",
       " 161.60446,\n",
       " 161.60439,\n",
       " 161.60431]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_every_5th_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [],
   "source": [
    "w=w.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b=b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.076215]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(X_test[0:1],w,b).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "iris=pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               False\n",
       "SepalLengthCm    False\n",
       "SepalWidthCm     False\n",
       "PetalLengthCm    False\n",
       "PetalWidthCm     False\n",
       "Species          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJr5dYnocihm"
   },
   "outputs": [],
   "source": [
    "iris_dummy=pd.get_dummies(data=iris[\"Species\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0              1                0               0\n",
       "1              1                0               0\n",
       "2              1                0               0\n",
       "3              1                0               0\n",
       "4              1                0               0\n",
       "5              1                0               0\n",
       "6              1                0               0\n",
       "7              1                0               0\n",
       "8              1                0               0\n",
       "9              1                0               0\n",
       "10             1                0               0\n",
       "11             1                0               0\n",
       "12             1                0               0\n",
       "13             1                0               0\n",
       "14             1                0               0\n",
       "15             1                0               0\n",
       "16             1                0               0\n",
       "17             1                0               0\n",
       "18             1                0               0\n",
       "19             1                0               0\n",
       "20             1                0               0\n",
       "21             1                0               0\n",
       "22             1                0               0\n",
       "23             1                0               0\n",
       "24             1                0               0\n",
       "25             1                0               0\n",
       "26             1                0               0\n",
       "27             1                0               0\n",
       "28             1                0               0\n",
       "29             1                0               0\n",
       "..           ...              ...             ...\n",
       "120            0                0               1\n",
       "121            0                0               1\n",
       "122            0                0               1\n",
       "123            0                0               1\n",
       "124            0                0               1\n",
       "125            0                0               1\n",
       "126            0                0               1\n",
       "127            0                0               1\n",
       "128            0                0               1\n",
       "129            0                0               1\n",
       "130            0                0               1\n",
       "131            0                0               1\n",
       "132            0                0               1\n",
       "133            0                0               1\n",
       "134            0                0               1\n",
       "135            0                0               1\n",
       "136            0                0               1\n",
       "137            0                0               1\n",
       "138            0                0               1\n",
       "139            0                0               1\n",
       "140            0                0               1\n",
       "141            0                0               1\n",
       "142            0                0               1\n",
       "143            0                0               1\n",
       "144            0                0               1\n",
       "145            0                0               1\n",
       "146            0                0               1\n",
       "147            0                0               1\n",
       "148            0                0               1\n",
       "149            0                0               1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df=pd.concat([iris,iris_dummy.iloc[:,0:3]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>121</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>122</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>123</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>125</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>126</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>127</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>130</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>132</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>135</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>136</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>137</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>140</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>142</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>143</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "5      6            5.4           3.9            1.7           0.4   \n",
       "6      7            4.6           3.4            1.4           0.3   \n",
       "7      8            5.0           3.4            1.5           0.2   \n",
       "8      9            4.4           2.9            1.4           0.2   \n",
       "9     10            4.9           3.1            1.5           0.1   \n",
       "10    11            5.4           3.7            1.5           0.2   \n",
       "11    12            4.8           3.4            1.6           0.2   \n",
       "12    13            4.8           3.0            1.4           0.1   \n",
       "13    14            4.3           3.0            1.1           0.1   \n",
       "14    15            5.8           4.0            1.2           0.2   \n",
       "15    16            5.7           4.4            1.5           0.4   \n",
       "16    17            5.4           3.9            1.3           0.4   \n",
       "17    18            5.1           3.5            1.4           0.3   \n",
       "18    19            5.7           3.8            1.7           0.3   \n",
       "19    20            5.1           3.8            1.5           0.3   \n",
       "20    21            5.4           3.4            1.7           0.2   \n",
       "21    22            5.1           3.7            1.5           0.4   \n",
       "22    23            4.6           3.6            1.0           0.2   \n",
       "23    24            5.1           3.3            1.7           0.5   \n",
       "24    25            4.8           3.4            1.9           0.2   \n",
       "25    26            5.0           3.0            1.6           0.2   \n",
       "26    27            5.0           3.4            1.6           0.4   \n",
       "27    28            5.2           3.5            1.5           0.2   \n",
       "28    29            5.2           3.4            1.4           0.2   \n",
       "29    30            4.7           3.2            1.6           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "120  121            6.9           3.2            5.7           2.3   \n",
       "121  122            5.6           2.8            4.9           2.0   \n",
       "122  123            7.7           2.8            6.7           2.0   \n",
       "123  124            6.3           2.7            4.9           1.8   \n",
       "124  125            6.7           3.3            5.7           2.1   \n",
       "125  126            7.2           3.2            6.0           1.8   \n",
       "126  127            6.2           2.8            4.8           1.8   \n",
       "127  128            6.1           3.0            4.9           1.8   \n",
       "128  129            6.4           2.8            5.6           2.1   \n",
       "129  130            7.2           3.0            5.8           1.6   \n",
       "130  131            7.4           2.8            6.1           1.9   \n",
       "131  132            7.9           3.8            6.4           2.0   \n",
       "132  133            6.4           2.8            5.6           2.2   \n",
       "133  134            6.3           2.8            5.1           1.5   \n",
       "134  135            6.1           2.6            5.6           1.4   \n",
       "135  136            7.7           3.0            6.1           2.3   \n",
       "136  137            6.3           3.4            5.6           2.4   \n",
       "137  138            6.4           3.1            5.5           1.8   \n",
       "138  139            6.0           3.0            4.8           1.8   \n",
       "139  140            6.9           3.1            5.4           2.1   \n",
       "140  141            6.7           3.1            5.6           2.4   \n",
       "141  142            6.9           3.1            5.1           2.3   \n",
       "142  143            5.8           2.7            5.1           1.9   \n",
       "143  144            6.8           3.2            5.9           2.3   \n",
       "144  145            6.7           3.3            5.7           2.5   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "            Species  Iris-setosa  Iris-versicolor  Iris-virginica  \n",
       "0       Iris-setosa            1                0               0  \n",
       "1       Iris-setosa            1                0               0  \n",
       "2       Iris-setosa            1                0               0  \n",
       "3       Iris-setosa            1                0               0  \n",
       "4       Iris-setosa            1                0               0  \n",
       "5       Iris-setosa            1                0               0  \n",
       "6       Iris-setosa            1                0               0  \n",
       "7       Iris-setosa            1                0               0  \n",
       "8       Iris-setosa            1                0               0  \n",
       "9       Iris-setosa            1                0               0  \n",
       "10      Iris-setosa            1                0               0  \n",
       "11      Iris-setosa            1                0               0  \n",
       "12      Iris-setosa            1                0               0  \n",
       "13      Iris-setosa            1                0               0  \n",
       "14      Iris-setosa            1                0               0  \n",
       "15      Iris-setosa            1                0               0  \n",
       "16      Iris-setosa            1                0               0  \n",
       "17      Iris-setosa            1                0               0  \n",
       "18      Iris-setosa            1                0               0  \n",
       "19      Iris-setosa            1                0               0  \n",
       "20      Iris-setosa            1                0               0  \n",
       "21      Iris-setosa            1                0               0  \n",
       "22      Iris-setosa            1                0               0  \n",
       "23      Iris-setosa            1                0               0  \n",
       "24      Iris-setosa            1                0               0  \n",
       "25      Iris-setosa            1                0               0  \n",
       "26      Iris-setosa            1                0               0  \n",
       "27      Iris-setosa            1                0               0  \n",
       "28      Iris-setosa            1                0               0  \n",
       "29      Iris-setosa            1                0               0  \n",
       "..              ...          ...              ...             ...  \n",
       "120  Iris-virginica            0                0               1  \n",
       "121  Iris-virginica            0                0               1  \n",
       "122  Iris-virginica            0                0               1  \n",
       "123  Iris-virginica            0                0               1  \n",
       "124  Iris-virginica            0                0               1  \n",
       "125  Iris-virginica            0                0               1  \n",
       "126  Iris-virginica            0                0               1  \n",
       "127  Iris-virginica            0                0               1  \n",
       "128  Iris-virginica            0                0               1  \n",
       "129  Iris-virginica            0                0               1  \n",
       "130  Iris-virginica            0                0               1  \n",
       "131  Iris-virginica            0                0               1  \n",
       "132  Iris-virginica            0                0               1  \n",
       "133  Iris-virginica            0                0               1  \n",
       "134  Iris-virginica            0                0               1  \n",
       "135  Iris-virginica            0                0               1  \n",
       "136  Iris-virginica            0                0               1  \n",
       "137  Iris-virginica            0                0               1  \n",
       "138  Iris-virginica            0                0               1  \n",
       "139  Iris-virginica            0                0               1  \n",
       "140  Iris-virginica            0                0               1  \n",
       "141  Iris-virginica            0                0               1  \n",
       "142  Iris-virginica            0                0               1  \n",
       "143  Iris-virginica            0                0               1  \n",
       "144  Iris-virginica            0                0               1  \n",
       "145  Iris-virginica            0                0               1  \n",
       "146  Iris-virginica            0                0               1  \n",
       "147  Iris-virginica            0                0               1  \n",
       "148  Iris-virginica            0                0               1  \n",
       "149  Iris-virginica            0                0               1  \n",
       "\n",
       "[150 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.replace(to_replace=\"Iris-setosa\",value=0,inplace=True)\n",
    "iris_df.replace(to_replace=\"Iris-versicolor\",value=1,inplace=True)\n",
    "iris_df.replace(to_replace=\"Iris-virginica\",value=2,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species  \\\n",
       "0   1            5.1           3.5            1.4           0.2        0   \n",
       "1   2            4.9           3.0            1.4           0.2        0   \n",
       "2   3            4.7           3.2            1.3           0.2        0   \n",
       "3   4            4.6           3.1            1.5           0.2        0   \n",
       "4   5            5.0           3.6            1.4           0.2        0   \n",
       "\n",
       "   Iris-setosa  Iris-versicolor  Iris-virginica  \n",
       "0            1                0               0  \n",
       "1            1                0               0  \n",
       "2            1                0               0  \n",
       "3            1                0               0  \n",
       "4            1                0               0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQoLMucihj"
   },
   "outputs": [],
   "source": [
    "features=iris_df[[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]\n",
    "target=iris_df[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(features,target,test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the Dense Layer for Prediction\n",
    "model.add(tf.keras.layers.Dense(3, input_shape=(4,),activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Normalization\n",
    "model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qLEdHPscihx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 21\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1103 16:04:52.789997  5928 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 1s 10ms/sample - loss: 7.7105 - acc: 0.3333 - val_loss: 4.5466 - val_acc: 0.3778\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.9958 - acc: 0.5048 - val_loss: 4.5764 - val_acc: 0.3778\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.3928 - acc: 0.5524 - val_loss: 4.5855 - val_acc: 0.3778\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.6728 - acc: 0.5810 - val_loss: 4.5907 - val_acc: 0.3778\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 10.4794 - acc: 0.6095 - val_loss: 4.5883 - val_acc: 0.3778\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 10.9281 - acc: 0.6095 - val_loss: 4.5868 - val_acc: 0.3778\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.7602 - acc: 0.6286 - val_loss: 4.5826 - val_acc: 0.3778\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.7600 - acc: 0.6190 - val_loss: 4.5784 - val_acc: 0.3778\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.7572 - acc: 0.6190 - val_loss: 4.5736 - val_acc: 0.3778\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9131 - acc: 0.6190 - val_loss: 4.5699 - val_acc: 0.3778\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7575 - acc: 0.6190 - val_loss: 4.5646 - val_acc: 0.3778\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 10.9074 - acc: 0.6286 - val_loss: 4.5593 - val_acc: 0.3778\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9025 - acc: 0.6286 - val_loss: 4.5534 - val_acc: 0.3778\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9145 - acc: 0.6095 - val_loss: 4.5506 - val_acc: 0.3778\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7555 - acc: 0.6190 - val_loss: 4.5434 - val_acc: 0.3778\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 10.7499 - acc: 0.6286 - val_loss: 4.5374 - val_acc: 0.3778\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9091 - acc: 0.6286 - val_loss: 4.5290 - val_acc: 0.3778\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 10.7522 - acc: 0.6190 - val_loss: 4.5196 - val_acc: 0.3778\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.9005 - acc: 0.6286 - val_loss: 4.5128 - val_acc: 0.3778\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7491 - acc: 0.6286 - val_loss: 4.5068 - val_acc: 0.3778\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 10.7459 - acc: 0.6286 - val_loss: 4.4999 - val_acc: 0.3778\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 143us/sample - loss: 10.7642 - acc: 0.6095 - val_loss: 4.5226 - val_acc: 0.3778\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.7511 - acc: 0.6190 - val_loss: 4.5228 - val_acc: 0.3778\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.5926 - acc: 0.6286 - val_loss: 4.5159 - val_acc: 0.3778\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.5083 - val_acc: 0.3778\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.5009 - val_acc: 0.3778\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 10.5775 - acc: 0.71 - 0s 114us/sample - loss: 10.7492 - acc: 0.6190 - val_loss: 4.4918 - val_acc: 0.3778\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.9001 - acc: 0.6286 - val_loss: 4.4811 - val_acc: 0.3778\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.4736 - val_acc: 0.3778\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.4661 - val_acc: 0.3778\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.4384 - acc: 0.6286 - val_loss: 4.4581 - val_acc: 0.3778\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.6005 - acc: 0.6190 - val_loss: 4.4495 - val_acc: 0.3778\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.4418 - val_acc: 0.3778\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.4339 - val_acc: 0.3778\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.4259 - val_acc: 0.3778\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.4185 - val_acc: 0.3778\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6190 - val_loss: 4.4101 - val_acc: 0.3778\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8999 - acc: 0.6286 - val_loss: 4.3993 - val_acc: 0.3778\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.3912 - val_acc: 0.3778\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.3831 - val_acc: 0.3778\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.3748 - val_acc: 0.3778\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.3668 - val_acc: 0.3778\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.7457 - acc: 0.6286 - val_loss: 4.3591 - val_acc: 0.3778\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.9009 - acc: 0.6190 - val_loss: 4.3459 - val_acc: 0.3778\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8990 - acc: 0.6286 - val_loss: 4.3375 - val_acc: 0.3778\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.5919 - acc: 0.6286 - val_loss: 4.3291 - val_acc: 0.3778\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8991 - acc: 0.6286 - val_loss: 4.3150 - val_acc: 0.3778\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.5944 - acc: 0.6286 - val_loss: 4.2977 - val_acc: 0.3778\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 11.0533 - acc: 0.6286 - val_loss: 4.2884 - val_acc: 0.3778\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.2790 - val_acc: 0.3778\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.2696 - val_acc: 0.3778\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.5919 - acc: 0.6286 - val_loss: 4.2601 - val_acc: 0.3778\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7477 - acc: 0.6190 - val_loss: 4.2528 - val_acc: 0.3778\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.2427 - val_acc: 0.3778\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.2320 - val_acc: 0.3778\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.2217 - val_acc: 0.3778\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 10.8999 - acc: 0.6286 - val_loss: 4.2134 - val_acc: 0.4000\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 10.5922 - acc: 0.6286 - val_loss: 4.2039 - val_acc: 0.4000\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1936 - val_acc: 0.4000\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1833 - val_acc: 0.4222\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.1718 - val_acc: 0.4222\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1596 - val_acc: 0.4222\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1486 - val_acc: 0.4222\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1384 - val_acc: 0.4222\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.1279 - val_acc: 0.4444\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.1172 - val_acc: 0.4444\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.1061 - val_acc: 0.4444\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.0948 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.0832 - val_acc: 0.4889\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0721 - val_acc: 0.5111\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 12.0886 - acc: 0.59 - 0s 114us/sample - loss: 11.2125 - acc: 0.6095 - val_loss: 4.1621 - val_acc: 0.4222\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.7548 - acc: 0.6095 - val_loss: 4.1280 - val_acc: 0.4444\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.5964 - acc: 0.6190 - val_loss: 4.1062 - val_acc: 0.4444\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0959 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 10.7516 - acc: 0.6190 - val_loss: 4.0762 - val_acc: 0.4889\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0654 - val_acc: 0.5111\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0544 - val_acc: 0.5111\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9030 - acc: 0.6190 - val_loss: 4.0327 - val_acc: 0.5778\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.9004 - acc: 0.6190 - val_loss: 4.0272 - val_acc: 0.5778\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0187 - val_acc: 0.6000\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.0104 - val_acc: 0.6000\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.9000 - acc: 0.6190 - val_loss: 4.0063 - val_acc: 0.6000\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 3.9984 - val_acc: 0.6222\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 3.9919 - val_acc: 0.6222\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 3.9872 - val_acc: 0.6222\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 3.9820 - val_acc: 0.6222\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 3.9774 - val_acc: 0.6222\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 3.9729 - val_acc: 0.6222\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.3262 - val_acc: 0.6667\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.3219 - val_acc: 0.7111\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 10.7454 - acc: 0.6286 - val_loss: 4.3165 - val_acc: 0.7333\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.6697 - val_acc: 0.7333\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.6665 - val_acc: 0.7333\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.6651 - val_acc: 0.7333\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 85us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.6640 - val_acc: 0.7333\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 4.6633 - val_acc: 0.7333\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 11.0524 - acc: 0.6286 - val_loss: 4.6627 - val_acc: 0.7333\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.6041 - acc: 0.6190 - val_loss: 5.0181 - val_acc: 0.7333\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 10.9042 - acc: 0.6190 - val_loss: 6.8054 - val_acc: 0.7556\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 10.8989 - acc: 0.6286 - val_loss: 7.5218 - val_acc: 0.7556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e74923b70>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,          \n",
    "          validation_data=(X_test,y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBgKZkhkcih6"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37937382,  0.01876831, -0.27195364]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save('iris_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

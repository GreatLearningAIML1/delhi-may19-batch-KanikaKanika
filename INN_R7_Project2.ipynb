{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtHH8TqEN-Xg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1576953294099,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "H_9STNtpOLzV",
    "outputId": "e7de15bc-e24b-4c93-bb44-ef5812a6b8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KA2_Nt7TN-Xs"
   },
   "outputs": [],
   "source": [
    "#file=h5py.File(\"/gdrive/My Drive/SVHN_single_grey1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=h5py.File(\"SVHN_single_grey1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1158,
     "status": "ok",
     "timestamp": 1576950607892,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "tZpszvMuN-Xv",
    "outputId": "b27e0b1e-11f6-4986-89f2-48229bfe7c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKKMmpFEN-Xz"
   },
   "outputs": [],
   "source": [
    "X_test=np.asarray(file[\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9pZMbomN-X2"
   },
   "outputs": [],
   "source": [
    "X_train=np.asarray(file[\"X_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coYnNa5nN-X5"
   },
   "outputs": [],
   "source": [
    "y_train=np.asarray(file[\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXer2E7MN-X8"
   },
   "outputs": [],
   "source": [
    "y_test=np.asarray(file[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1576950616443,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "t7bn1e6hN-X_",
    "outputId": "894d2ed5-3b78-4681-da3d-71494564dbc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1576950618114,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "JIlmKhrTN-YC",
    "outputId": "cfb650ad-aba9-49cd-c400-86579df7ae97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 32, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1386,
     "status": "ok",
     "timestamp": 1576950621299,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "lTV4amU0N-YF",
    "outputId": "f665ba1a-c69b-44a1-e025-e922e2086b32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1576950622352,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "uF3iAop4N-YI",
    "outputId": "b766e5b7-111e-4346-9689-02a15138dd22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1576828813291,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "heHGpbyeN-YL",
    "outputId": "89f02fde-db97-467c-f8cf-ffb08fe286ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3JWmltaPqKj"
   },
   "source": [
    "**Using KNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMIUCA7lN-YO"
   },
   "outputs": [],
   "source": [
    "nsamples,nx,ny=X_train.shape\n",
    "knn_train = X_train.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1576828820203,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "zu6BK05UN-YR",
    "outputId": "4e540216-a6df-4b0f-c75a-510f267f46b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1647,
     "status": "ok",
     "timestamp": 1576828823331,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "JJdgf1SKN-YU",
    "outputId": "2e40f2f5-a2d3-4f9c-c281-1a6dd0838420",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.0704, 30.2601, 26.852 , ..., 49.6682, 50.853 , 53.0377],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MILoXXC8N-YX"
   },
   "outputs": [],
   "source": [
    "nsamples,nx,ny=X_test.shape\n",
    "knn_test=X_test.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1460,
     "status": "ok",
     "timestamp": 1576828829005,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "LOX7WPu7N-Ya",
    "outputId": "17d19ac4-5d94-4fe7-f4fc-5005246268b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1537,
     "status": "ok",
     "timestamp": 1576828831469,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "4f4bP9o_N-Yc",
    "outputId": "0226a336-edb2-42a4-ba74-487561759925"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40.558 ,  46.7917,  48.9764, ..., 110.0169, 111.2017, 114.1906],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1A8PcAr3UEq"
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qX5kW5sh3YHR"
   },
   "outputs": [],
   "source": [
    "knn_train_scaled=scaler.fit_transform(knn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZAilwAE3lLh"
   },
   "outputs": [],
   "source": [
    "knn_test_scaled=scaler.fit_transform(knn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zE8I4vMuWHRs"
   },
   "outputs": [],
   "source": [
    "kVals = range(1, 30, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HErA7j5BWRVk"
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "colab_type": "code",
    "id": "NdBEenjdWWgn",
    "outputId": "86ef4ea7-b9fc-40ef-99f7-f6caf8a00fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=46.77%\n",
      "k=3, accuracy=47.19%\n",
      "k=5, accuracy=50.03%\n",
      "k=7, accuracy=51.52%\n",
      "k=9, accuracy=52.18%\n",
      "k=11, accuracy=52.83%\n",
      "k=13, accuracy=52.97%\n",
      "k=15, accuracy=53.44%\n",
      "k=17, accuracy=53.46%\n",
      "k=19, accuracy=53.76%\n",
      "k=21, accuracy=54.13%\n",
      "k=23, accuracy=54.10%\n",
      "k=25, accuracy=53.98%\n",
      "k=27, accuracy=54.30%\n",
      "k=29, accuracy=54.43%\n",
      "k=31, accuracy=54.52%\n",
      "k=33, accuracy=54.47%\n",
      "k=35, accuracy=54.62%\n",
      "k=37, accuracy=54.81%\n",
      "k=39, accuracy=54.66%\n",
      "k=41, accuracy=54.64%\n",
      "k=43, accuracy=54.93%\n",
      "k=45, accuracy=54.91%\n",
      "k=47, accuracy=54.93%\n",
      "k=49, accuracy=54.85%\n",
      "k=51, accuracy=54.79%\n",
      "k=53, accuracy=54.82%\n",
      "k=55, accuracy=54.72%\n",
      "k=57, accuracy=54.83%\n",
      "k=59, accuracy=54.67%\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 60, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(knn_train_scaled,y_train)\n",
    "    knn_predictions=knn_model.predict(knn_test_scaled)\n",
    "    knn_score = accuracy_score(knn_predictions,y_test)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, knn_score * 100))\n",
    "    accuracies.append(knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJJujGSpi6Xu"
   },
   "source": [
    "Trying the value of k as square root of training samples: sqrt of 4200 : 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEbwvuSXi4_2"
   },
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=205)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5185,
     "status": "ok",
     "timestamp": 1576913402946,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "RSU6xeugN-Yl",
    "outputId": "429e50fe-c77a-4750-8ad0-11c77855bf45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=205, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.fit(knn_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93MKzfjTO7-l"
   },
   "outputs": [],
   "source": [
    "knn_predictions = knn_model.predict(knn_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39F1ryDwN-Ys"
   },
   "outputs": [],
   "source": [
    "knn_score =accuracy_score(knn_predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1576915967476,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "7aOtn_hEVOWT",
    "outputId": "f475b38e-5ce1-4fb5-a770-c473896024e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5249444444444444"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4LbJ-ZSrE_y"
   },
   "source": [
    "Observations: Even after taking optimal value of k as 209 seems like square root of number of training samples, accuracy has decreased to 51%. So we will proceed with the value of k as 29 as accuracy of model reaches to 53% \n",
    "\n",
    "Building final model with k=29 and then calaulating the classification report for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvPpDpDmrnRQ"
   },
   "outputs": [],
   "source": [
    "final_knn_model=KNeighborsClassifier(n_neighbors=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5694,
     "status": "ok",
     "timestamp": 1576917833872,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "q7ylade-rnpU",
    "outputId": "32c09d9d-594e-4cb4-c23c-409f0c256bb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=47, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_knn_model.fit(knn_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkFzG7GFrn3g"
   },
   "outputs": [],
   "source": [
    "final_knn_predictions=final_knn_model.predict(knn_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Q7uaC6NxiYW"
   },
   "outputs": [],
   "source": [
    "final_knn_score=accuracy_score(final_knn_predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1576919893411,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "lWkonokOxqsh",
    "outputId": "f13707cc-56c2-4f8d-a319-0270fb2e3c73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492777777777778"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_knn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1576919896466,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "YcrGGmZlxxoL",
    "outputId": "0f10f216-8e2c-4b43-cdc4-6b6c0f709bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.72      0.52      1814\n",
      "           1       0.43      0.72      0.54      1828\n",
      "           2       0.65      0.49      0.56      1803\n",
      "           3       0.49      0.42      0.45      1719\n",
      "           4       0.66      0.65      0.65      1812\n",
      "           5       0.54      0.40      0.46      1768\n",
      "           6       0.54      0.40      0.46      1832\n",
      "           7       0.67      0.61      0.64      1808\n",
      "           8       0.52      0.39      0.44      1812\n",
      "           9       0.55      0.45      0.50      1804\n",
      "\n",
      "    accuracy                           0.52     18000\n",
      "   macro avg       0.55      0.52      0.52     18000\n",
      "weighted avg       0.55      0.52      0.52     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pudJYceh25tf"
   },
   "source": [
    "**Building Deep Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byto33UxSmYM"
   },
   "outputs": [],
   "source": [
    "#X_train_deep = X_train.reshape(X_train.shape[0], 32, 32, 1).astype('float32')\n",
    "#X_test_deep = X_test.reshape(X_test.shape[0], 32, 32, 1).astype('float32')\n",
    "X_train_model =X_train/255\n",
    "X_test_model  =X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEBcf_3eN-Y2"
   },
   "outputs": [],
   "source": [
    "y_train_model = np_utils.to_categorical(y_train, 10)\n",
    "y_test_model = np_utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_CPN3pKN-Y7"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGsEAWYll2-B"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Reshape((1024,),input_shape=(32,32,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nl3XbZDYmAaH"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bu8GJh6lmCcE"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(200, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzCdPuuDmX8F"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeLHdWoGmay7"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(60, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYDFkw4bmc6A"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YDd6NvpmttV"
   },
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03, decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqYPpm5DmygF"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1755134,
     "status": "ok",
     "timestamp": 1576955088268,
     "user": {
      "displayName": "Kanika Goyal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD23uD5ycwQ4q7rvT5AsPfvM8CDuMJRqSCUlRSRzg=s64",
      "userId": "17517065079268107762"
     },
     "user_tz": -330
    },
    "id": "HkmQNPvIm3on",
    "outputId": "33b01ff3-c939-447a-90a7-f9286192e53a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 1.4605 - acc: 0.5142 - val_loss: 1.0302 - val_acc: 0.6818\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.9798 - acc: 0.6947 - val_loss: 0.8574 - val_acc: 0.7394\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 12s 296us/sample - loss: 0.8490 - acc: 0.7374 - val_loss: 0.7784 - val_acc: 0.7694\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 11s 257us/sample - loss: 0.7775 - acc: 0.7599 - val_loss: 0.7355 - val_acc: 0.7857\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.7341 - acc: 0.7727 - val_loss: 0.6972 - val_acc: 0.7992\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 12s 274us/sample - loss: 0.7004 - acc: 0.7845 - val_loss: 0.6691 - val_acc: 0.8073\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 11s 254us/sample - loss: 0.6732 - acc: 0.7922 - val_loss: 0.6630 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.6506 - acc: 0.7997 - val_loss: 0.6585 - val_acc: 0.8102\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.6397 - acc: 0.8029 - val_loss: 0.6336 - val_acc: 0.8198\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.6189 - acc: 0.8095 - val_loss: 0.6340 - val_acc: 0.8179\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.6102 - acc: 0.8126 - val_loss: 0.6351 - val_acc: 0.8187\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.5998 - acc: 0.8155 - val_loss: 0.6225 - val_acc: 0.8252\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.5923 - acc: 0.8165 - val_loss: 0.6173 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.5821 - acc: 0.8184 - val_loss: 0.6080 - val_acc: 0.8288\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 11s 253us/sample - loss: 0.5751 - acc: 0.8233 - val_loss: 0.6168 - val_acc: 0.8244\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.5680 - acc: 0.8245 - val_loss: 0.5986 - val_acc: 0.8333\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 7s 155us/sample - loss: 0.5560 - acc: 0.8297 - val_loss: 0.5981 - val_acc: 0.8319\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 6s 153us/sample - loss: 0.5480 - acc: 0.8318 - val_loss: 0.5954 - val_acc: 0.8334\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.5448 - acc: 0.8326 - val_loss: 0.5902 - val_acc: 0.8354\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.5396 - acc: 0.8326 - val_loss: 0.5922 - val_acc: 0.8363\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 6s 155us/sample - loss: 0.5361 - acc: 0.8345 - val_loss: 0.5887 - val_acc: 0.8353\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.5317 - acc: 0.8342 - val_loss: 0.5851 - val_acc: 0.8364\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5243 - acc: 0.8384 - val_loss: 0.5825 - val_acc: 0.8380\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5252 - acc: 0.8371 - val_loss: 0.5800 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.5153 - acc: 0.8400 - val_loss: 0.5783 - val_acc: 0.8397\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.5095 - acc: 0.8433 - val_loss: 0.5836 - val_acc: 0.8382\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 7s 178us/sample - loss: 0.5197 - acc: 0.8387 - val_loss: 0.5807 - val_acc: 0.8372\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.5110 - acc: 0.8425 - val_loss: 0.5736 - val_acc: 0.8403\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.5060 - acc: 0.8453 - val_loss: 0.5761 - val_acc: 0.8388\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.5043 - acc: 0.8446 - val_loss: 0.5711 - val_acc: 0.8413\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4989 - acc: 0.8466 - val_loss: 0.5730 - val_acc: 0.8432\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.5007 - acc: 0.8453 - val_loss: 0.5724 - val_acc: 0.8433\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4961 - acc: 0.8482 - val_loss: 0.5685 - val_acc: 0.8442\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4934 - acc: 0.8467 - val_loss: 0.5671 - val_acc: 0.8434\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.4923 - acc: 0.8478 - val_loss: 0.5701 - val_acc: 0.8416\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4966 - acc: 0.8461 - val_loss: 0.5680 - val_acc: 0.8427\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4914 - acc: 0.8474 - val_loss: 0.5664 - val_acc: 0.8453\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.4847 - acc: 0.8507 - val_loss: 0.5644 - val_acc: 0.8443\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.4850 - acc: 0.8507 - val_loss: 0.5637 - val_acc: 0.8452\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4785 - acc: 0.8532 - val_loss: 0.5644 - val_acc: 0.8450\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4750 - acc: 0.8515 - val_loss: 0.5695 - val_acc: 0.8423\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.4783 - acc: 0.8532 - val_loss: 0.5625 - val_acc: 0.8459\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4732 - acc: 0.8531 - val_loss: 0.5613 - val_acc: 0.8472\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4720 - acc: 0.8544 - val_loss: 0.5624 - val_acc: 0.8469\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4688 - acc: 0.8551 - val_loss: 0.5613 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4733 - acc: 0.8540 - val_loss: 0.5577 - val_acc: 0.8464\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4644 - acc: 0.8568 - val_loss: 0.5598 - val_acc: 0.8484\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.4686 - acc: 0.8561 - val_loss: 0.5625 - val_acc: 0.8466\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4631 - acc: 0.8567 - val_loss: 0.5606 - val_acc: 0.8480\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4599 - acc: 0.8579 - val_loss: 0.5594 - val_acc: 0.8469\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 7s 173us/sample - loss: 0.4594 - acc: 0.8570 - val_loss: 0.5578 - val_acc: 0.8478\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4568 - acc: 0.8600 - val_loss: 0.5582 - val_acc: 0.8481\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4609 - acc: 0.8583 - val_loss: 0.5576 - val_acc: 0.8490\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.4583 - acc: 0.8585 - val_loss: 0.5563 - val_acc: 0.8486\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4580 - acc: 0.8572 - val_loss: 0.5592 - val_acc: 0.8481\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4516 - acc: 0.8597 - val_loss: 0.5568 - val_acc: 0.8488\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4545 - acc: 0.8588 - val_loss: 0.5554 - val_acc: 0.8482\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4509 - acc: 0.8614 - val_loss: 0.5557 - val_acc: 0.8490\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4491 - acc: 0.8617 - val_loss: 0.5555 - val_acc: 0.8497\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4514 - acc: 0.8605 - val_loss: 0.5572 - val_acc: 0.8486\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4471 - acc: 0.8632 - val_loss: 0.5525 - val_acc: 0.8491\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4485 - acc: 0.8615 - val_loss: 0.5550 - val_acc: 0.8495\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.4465 - acc: 0.8612 - val_loss: 0.5539 - val_acc: 0.8487\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4487 - acc: 0.8611 - val_loss: 0.5540 - val_acc: 0.8502\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4417 - acc: 0.8655 - val_loss: 0.5529 - val_acc: 0.8498\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4438 - acc: 0.8635 - val_loss: 0.5516 - val_acc: 0.8513\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4470 - acc: 0.8625 - val_loss: 0.5539 - val_acc: 0.8499\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4413 - acc: 0.8639 - val_loss: 0.5533 - val_acc: 0.8523\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4410 - acc: 0.8635 - val_loss: 0.5520 - val_acc: 0.8509\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.4447 - acc: 0.8622 - val_loss: 0.5522 - val_acc: 0.8504\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4378 - acc: 0.8628 - val_loss: 0.5518 - val_acc: 0.8510\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4452 - acc: 0.8610 - val_loss: 0.5515 - val_acc: 0.8503\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4380 - acc: 0.8644 - val_loss: 0.5516 - val_acc: 0.8508\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4361 - acc: 0.8661 - val_loss: 0.5518 - val_acc: 0.8516\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.4336 - acc: 0.8656 - val_loss: 0.5517 - val_acc: 0.8517\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4380 - acc: 0.8640 - val_loss: 0.5517 - val_acc: 0.8518\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4342 - acc: 0.8646 - val_loss: 0.5509 - val_acc: 0.8517\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.4332 - acc: 0.8654 - val_loss: 0.5501 - val_acc: 0.8514\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.4331 - acc: 0.8650 - val_loss: 0.5498 - val_acc: 0.8523\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4366 - acc: 0.8639 - val_loss: 0.5506 - val_acc: 0.8518\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4358 - acc: 0.8652 - val_loss: 0.5508 - val_acc: 0.8514\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.4332 - acc: 0.8647 - val_loss: 0.5507 - val_acc: 0.8516\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4337 - acc: 0.8644 - val_loss: 0.5501 - val_acc: 0.8524\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4312 - acc: 0.8656 - val_loss: 0.5505 - val_acc: 0.8513\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4276 - acc: 0.8686 - val_loss: 0.5478 - val_acc: 0.8533\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4268 - acc: 0.8677 - val_loss: 0.5489 - val_acc: 0.8520\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4291 - acc: 0.8652 - val_loss: 0.5472 - val_acc: 0.8533\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.4247 - acc: 0.8689 - val_loss: 0.5487 - val_acc: 0.8512\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4291 - acc: 0.8676 - val_loss: 0.5466 - val_acc: 0.8524\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4229 - acc: 0.8696 - val_loss: 0.5471 - val_acc: 0.8522\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4265 - acc: 0.8663 - val_loss: 0.5469 - val_acc: 0.8524\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4208 - acc: 0.8691 - val_loss: 0.5477 - val_acc: 0.8533\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4231 - acc: 0.8678 - val_loss: 0.5476 - val_acc: 0.8531\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 7s 173us/sample - loss: 0.4217 - acc: 0.8703 - val_loss: 0.5486 - val_acc: 0.8521\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4245 - acc: 0.8679 - val_loss: 0.5466 - val_acc: 0.8534\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.4228 - acc: 0.8687 - val_loss: 0.5465 - val_acc: 0.8533\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.4214 - acc: 0.8692 - val_loss: 0.5479 - val_acc: 0.8543\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4220 - acc: 0.8682 - val_loss: 0.5478 - val_acc: 0.8528\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 10s 245us/sample - loss: 0.4212 - acc: 0.8695 - val_loss: 0.5476 - val_acc: 0.8532\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.4181 - acc: 0.8702 - val_loss: 0.5466 - val_acc: 0.8533\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.4228 - acc: 0.8688 - val_loss: 0.5446 - val_acc: 0.8541\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.4202 - acc: 0.8704 - val_loss: 0.5462 - val_acc: 0.8543\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.4171 - acc: 0.8711 - val_loss: 0.5455 - val_acc: 0.8539\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4161 - acc: 0.8698 - val_loss: 0.5480 - val_acc: 0.8538\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4176 - acc: 0.8710 - val_loss: 0.5457 - val_acc: 0.8543\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4159 - acc: 0.8699 - val_loss: 0.5451 - val_acc: 0.8549\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4145 - acc: 0.8702 - val_loss: 0.5452 - val_acc: 0.8545\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 7s 156us/sample - loss: 0.4140 - acc: 0.8705 - val_loss: 0.5458 - val_acc: 0.8551\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 7s 156us/sample - loss: 0.4140 - acc: 0.8717 - val_loss: 0.5449 - val_acc: 0.8544\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4132 - acc: 0.8722 - val_loss: 0.5454 - val_acc: 0.8554\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4151 - acc: 0.8712 - val_loss: 0.5460 - val_acc: 0.8533\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4135 - acc: 0.8715 - val_loss: 0.5466 - val_acc: 0.8553\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4116 - acc: 0.8730 - val_loss: 0.5447 - val_acc: 0.8548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4148 - acc: 0.8719 - val_loss: 0.5451 - val_acc: 0.8544\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.4124 - acc: 0.8713 - val_loss: 0.5450 - val_acc: 0.8554\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4105 - acc: 0.8717 - val_loss: 0.5462 - val_acc: 0.8551\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4120 - acc: 0.8717 - val_loss: 0.5459 - val_acc: 0.8551\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4090 - acc: 0.8736 - val_loss: 0.5443 - val_acc: 0.8561\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.4082 - acc: 0.8741 - val_loss: 0.5452 - val_acc: 0.8554\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4100 - acc: 0.8729 - val_loss: 0.5438 - val_acc: 0.8557\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4118 - acc: 0.8720 - val_loss: 0.5444 - val_acc: 0.8563\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4061 - acc: 0.8745 - val_loss: 0.5434 - val_acc: 0.8559\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.4033 - acc: 0.8765 - val_loss: 0.5446 - val_acc: 0.8548\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4035 - acc: 0.8745 - val_loss: 0.5452 - val_acc: 0.8553\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.4068 - acc: 0.8714 - val_loss: 0.5464 - val_acc: 0.8542\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4047 - acc: 0.8733 - val_loss: 0.5461 - val_acc: 0.8548\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.4073 - acc: 0.8740 - val_loss: 0.5437 - val_acc: 0.8567\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.4037 - acc: 0.8742 - val_loss: 0.5428 - val_acc: 0.8568\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.4078 - acc: 0.8733 - val_loss: 0.5449 - val_acc: 0.8557\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.4012 - acc: 0.8730 - val_loss: 0.5461 - val_acc: 0.8558\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.4074 - acc: 0.8710 - val_loss: 0.5439 - val_acc: 0.8554\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.4022 - acc: 0.8745 - val_loss: 0.5445 - val_acc: 0.8566\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.4063 - acc: 0.8739 - val_loss: 0.5436 - val_acc: 0.8561\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 11s 252us/sample - loss: 0.4015 - acc: 0.8741 - val_loss: 0.5441 - val_acc: 0.8552\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.4026 - acc: 0.8738 - val_loss: 0.5444 - val_acc: 0.8554\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.4069 - acc: 0.8730 - val_loss: 0.5431 - val_acc: 0.8560\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.4005 - acc: 0.8741 - val_loss: 0.5455 - val_acc: 0.8549\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.4024 - acc: 0.8752 - val_loss: 0.5436 - val_acc: 0.8559\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.4010 - acc: 0.8768 - val_loss: 0.5453 - val_acc: 0.8556\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.3996 - acc: 0.8760 - val_loss: 0.5441 - val_acc: 0.8557\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.4019 - acc: 0.8752 - val_loss: 0.5433 - val_acc: 0.8551\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3961 - acc: 0.8768 - val_loss: 0.5432 - val_acc: 0.8566\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.4003 - acc: 0.8756 - val_loss: 0.5420 - val_acc: 0.8561\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.3970 - acc: 0.8755 - val_loss: 0.5451 - val_acc: 0.8551\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.3983 - acc: 0.8761 - val_loss: 0.5440 - val_acc: 0.8560\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 6s 141us/sample - loss: 0.4001 - acc: 0.8754 - val_loss: 0.5433 - val_acc: 0.8564\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.4003 - acc: 0.8755 - val_loss: 0.5432 - val_acc: 0.8551\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3984 - acc: 0.8749 - val_loss: 0.5413 - val_acc: 0.8569\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.3992 - acc: 0.8756 - val_loss: 0.5418 - val_acc: 0.8562\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3958 - acc: 0.8769 - val_loss: 0.5430 - val_acc: 0.8558\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3948 - acc: 0.8775 - val_loss: 0.5424 - val_acc: 0.8557\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3968 - acc: 0.8752 - val_loss: 0.5441 - val_acc: 0.8554\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.4020 - acc: 0.8759 - val_loss: 0.5457 - val_acc: 0.8556\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3990 - acc: 0.8735 - val_loss: 0.5444 - val_acc: 0.8556\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3952 - acc: 0.8769 - val_loss: 0.5437 - val_acc: 0.8554\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3960 - acc: 0.8770 - val_loss: 0.5440 - val_acc: 0.8564\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.3957 - acc: 0.8751 - val_loss: 0.5447 - val_acc: 0.8563\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3938 - acc: 0.8760 - val_loss: 0.5429 - val_acc: 0.8569\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.3956 - acc: 0.8769 - val_loss: 0.5411 - val_acc: 0.8578\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 11s 266us/sample - loss: 0.3935 - acc: 0.8779 - val_loss: 0.5441 - val_acc: 0.8563\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3975 - acc: 0.8765 - val_loss: 0.5427 - val_acc: 0.8564\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 7s 176us/sample - loss: 0.3963 - acc: 0.8766 - val_loss: 0.5434 - val_acc: 0.8568\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.3940 - acc: 0.8773 - val_loss: 0.5443 - val_acc: 0.8572\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.3934 - acc: 0.8751 - val_loss: 0.5425 - val_acc: 0.8571\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.3909 - acc: 0.8794 - val_loss: 0.5434 - val_acc: 0.8558\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 10s 248us/sample - loss: 0.3940 - acc: 0.8767 - val_loss: 0.5421 - val_acc: 0.8566\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3908 - acc: 0.8777 - val_loss: 0.5432 - val_acc: 0.8564\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.3927 - acc: 0.8770 - val_loss: 0.5423 - val_acc: 0.8563\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3920 - acc: 0.8774 - val_loss: 0.5436 - val_acc: 0.8557\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 12s 280us/sample - loss: 0.3956 - acc: 0.8775 - val_loss: 0.5433 - val_acc: 0.8566\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.3916 - acc: 0.8785 - val_loss: 0.5437 - val_acc: 0.8573\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.3911 - acc: 0.8769 - val_loss: 0.5419 - val_acc: 0.8559\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3913 - acc: 0.8771 - val_loss: 0.5408 - val_acc: 0.8577\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.3948 - acc: 0.8768 - val_loss: 0.5423 - val_acc: 0.8570\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3923 - acc: 0.8781 - val_loss: 0.5418 - val_acc: 0.8576\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3928 - acc: 0.8774 - val_loss: 0.5421 - val_acc: 0.8571\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3888 - acc: 0.8765 - val_loss: 0.5428 - val_acc: 0.8569\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3888 - acc: 0.8781 - val_loss: 0.5418 - val_acc: 0.8573\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3869 - acc: 0.8796 - val_loss: 0.5431 - val_acc: 0.8566\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3868 - acc: 0.8796 - val_loss: 0.5426 - val_acc: 0.8574\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3908 - acc: 0.8778 - val_loss: 0.5432 - val_acc: 0.8562\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.3895 - acc: 0.8773 - val_loss: 0.5430 - val_acc: 0.8566\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3888 - acc: 0.8769 - val_loss: 0.5439 - val_acc: 0.8561\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3890 - acc: 0.8788 - val_loss: 0.5435 - val_acc: 0.8563\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3873 - acc: 0.8796 - val_loss: 0.5421 - val_acc: 0.8571\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3865 - acc: 0.8793 - val_loss: 0.5439 - val_acc: 0.8561\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3859 - acc: 0.8780 - val_loss: 0.5436 - val_acc: 0.8568\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3840 - acc: 0.8819 - val_loss: 0.5443 - val_acc: 0.8566\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3868 - acc: 0.8789 - val_loss: 0.5416 - val_acc: 0.8575\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3878 - acc: 0.8776 - val_loss: 0.5442 - val_acc: 0.8569\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3841 - acc: 0.8811 - val_loss: 0.5420 - val_acc: 0.8582\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3867 - acc: 0.8803 - val_loss: 0.5432 - val_acc: 0.8574\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3865 - acc: 0.8798 - val_loss: 0.5423 - val_acc: 0.8579\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 7s 173us/sample - loss: 0.3881 - acc: 0.8793 - val_loss: 0.5441 - val_acc: 0.8579\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 7s 173us/sample - loss: 0.3853 - acc: 0.8810 - val_loss: 0.5421 - val_acc: 0.8577\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 0.3872 - acc: 0.8802 - val_loss: 0.5438 - val_acc: 0.8581\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.3886 - acc: 0.8782 - val_loss: 0.5420 - val_acc: 0.8569\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.3886 - acc: 0.8782 - val_loss: 0.5419 - val_acc: 0.8577\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 8s 180us/sample - loss: 0.3854 - acc: 0.8793 - val_loss: 0.5412 - val_acc: 0.8573\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3825 - acc: 0.8807 - val_loss: 0.5423 - val_acc: 0.8569\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 7s 173us/sample - loss: 0.3834 - acc: 0.8800 - val_loss: 0.5411 - val_acc: 0.8583\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3854 - acc: 0.8782 - val_loss: 0.5419 - val_acc: 0.8578\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3838 - acc: 0.8793 - val_loss: 0.5437 - val_acc: 0.8570\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.3876 - acc: 0.8780 - val_loss: 0.5410 - val_acc: 0.8578\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3836 - acc: 0.8800 - val_loss: 0.5419 - val_acc: 0.8575\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3837 - acc: 0.8808 - val_loss: 0.5426 - val_acc: 0.8572\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3814 - acc: 0.8814 - val_loss: 0.5420 - val_acc: 0.8574\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.3830 - acc: 0.8814 - val_loss: 0.5424 - val_acc: 0.8573\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.3785 - acc: 0.8794 - val_loss: 0.5423 - val_acc: 0.8572\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.3773 - acc: 0.8810 - val_loss: 0.5417 - val_acc: 0.8582\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.3816 - acc: 0.8804 - val_loss: 0.5430 - val_acc: 0.8576\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3805 - acc: 0.8815 - val_loss: 0.5406 - val_acc: 0.8572\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3816 - acc: 0.8808 - val_loss: 0.5415 - val_acc: 0.8576\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 0.3795 - acc: 0.8802 - val_loss: 0.5418 - val_acc: 0.8574\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.3805 - acc: 0.8808 - val_loss: 0.5420 - val_acc: 0.8570\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 0.3787 - acc: 0.8816 - val_loss: 0.5417 - val_acc: 0.8583\n",
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3775 - acc: 0.8816 - val_loss: 0.5429 - val_acc: 0.8573\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3816 - acc: 0.8809 - val_loss: 0.5422 - val_acc: 0.8581\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3798 - acc: 0.8829 - val_loss: 0.5400 - val_acc: 0.8573\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 7s 175us/sample - loss: 0.3803 - acc: 0.8832 - val_loss: 0.5421 - val_acc: 0.8570\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 7s 168us/sample - loss: 0.3780 - acc: 0.8807 - val_loss: 0.5426 - val_acc: 0.8571\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 0.3802 - acc: 0.8799 - val_loss: 0.5418 - val_acc: 0.8578\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3765 - acc: 0.8811 - val_loss: 0.5414 - val_acc: 0.8576\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.3824 - acc: 0.8791 - val_loss: 0.5418 - val_acc: 0.8572\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3806 - acc: 0.8812 - val_loss: 0.5422 - val_acc: 0.8569\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 0.3780 - acc: 0.8809 - val_loss: 0.5413 - val_acc: 0.8569\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.3811 - acc: 0.8785 - val_loss: 0.5409 - val_acc: 0.8585\n",
      "Epoch 228/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3767 - acc: 0.8820 - val_loss: 0.5411 - val_acc: 0.8577\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3810 - acc: 0.8804 - val_loss: 0.5422 - val_acc: 0.8570\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3778 - acc: 0.8803 - val_loss: 0.5397 - val_acc: 0.8576\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.3757 - acc: 0.8832 - val_loss: 0.5416 - val_acc: 0.8578\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3756 - acc: 0.8831 - val_loss: 0.5432 - val_acc: 0.8576\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3796 - acc: 0.8815 - val_loss: 0.5413 - val_acc: 0.8576\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3735 - acc: 0.8845 - val_loss: 0.5423 - val_acc: 0.8569\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3756 - acc: 0.8812 - val_loss: 0.5419 - val_acc: 0.8585\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3771 - acc: 0.8819 - val_loss: 0.5406 - val_acc: 0.8589\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3737 - acc: 0.8827 - val_loss: 0.5422 - val_acc: 0.8583\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3707 - acc: 0.8839 - val_loss: 0.5404 - val_acc: 0.8581\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3786 - acc: 0.8811 - val_loss: 0.5420 - val_acc: 0.8577\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3725 - acc: 0.8836 - val_loss: 0.5417 - val_acc: 0.8578\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 7s 178us/sample - loss: 0.3743 - acc: 0.8829 - val_loss: 0.5426 - val_acc: 0.8578\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3723 - acc: 0.8825 - val_loss: 0.5426 - val_acc: 0.8580\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3736 - acc: 0.8830 - val_loss: 0.5416 - val_acc: 0.8577\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3779 - acc: 0.8818 - val_loss: 0.5405 - val_acc: 0.8582\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3739 - acc: 0.8828 - val_loss: 0.5411 - val_acc: 0.8576\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3738 - acc: 0.8825 - val_loss: 0.5412 - val_acc: 0.8582\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3715 - acc: 0.8828 - val_loss: 0.5410 - val_acc: 0.8582\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 7s 170us/sample - loss: 0.3751 - acc: 0.8820 - val_loss: 0.5422 - val_acc: 0.8573\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3745 - acc: 0.8832 - val_loss: 0.5405 - val_acc: 0.8581\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3711 - acc: 0.8842 - val_loss: 0.5419 - val_acc: 0.8582\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3712 - acc: 0.8839 - val_loss: 0.5417 - val_acc: 0.8591\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3742 - acc: 0.8820 - val_loss: 0.5400 - val_acc: 0.8586\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3733 - acc: 0.8848 - val_loss: 0.5417 - val_acc: 0.8568\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3758 - acc: 0.8838 - val_loss: 0.5416 - val_acc: 0.8577\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3703 - acc: 0.8834 - val_loss: 0.5398 - val_acc: 0.8581\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.3726 - acc: 0.8835 - val_loss: 0.5420 - val_acc: 0.8584\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3738 - acc: 0.8828 - val_loss: 0.5403 - val_acc: 0.8583\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3714 - acc: 0.8824 - val_loss: 0.5410 - val_acc: 0.8583\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 7s 172us/sample - loss: 0.3745 - acc: 0.8832 - val_loss: 0.5401 - val_acc: 0.8582\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3715 - acc: 0.8825 - val_loss: 0.5401 - val_acc: 0.8575\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.3719 - acc: 0.8836 - val_loss: 0.5403 - val_acc: 0.8589\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3717 - acc: 0.8844 - val_loss: 0.5404 - val_acc: 0.8578\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3706 - acc: 0.8847 - val_loss: 0.5398 - val_acc: 0.8583\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3711 - acc: 0.8842 - val_loss: 0.5411 - val_acc: 0.8580\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3672 - acc: 0.8843 - val_loss: 0.5417 - val_acc: 0.8587\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3734 - acc: 0.8828 - val_loss: 0.5419 - val_acc: 0.8583\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3732 - acc: 0.8834 - val_loss: 0.5427 - val_acc: 0.8579\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3679 - acc: 0.8864 - val_loss: 0.5416 - val_acc: 0.8590\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3657 - acc: 0.8857 - val_loss: 0.5414 - val_acc: 0.8585\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3688 - acc: 0.8840 - val_loss: 0.5398 - val_acc: 0.8587\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3696 - acc: 0.8835 - val_loss: 0.5409 - val_acc: 0.8581\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3711 - acc: 0.8835 - val_loss: 0.5413 - val_acc: 0.8586\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3694 - acc: 0.8855 - val_loss: 0.5403 - val_acc: 0.8579\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3682 - acc: 0.8841 - val_loss: 0.5389 - val_acc: 0.8589\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3699 - acc: 0.8843 - val_loss: 0.5408 - val_acc: 0.8582\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.3735 - acc: 0.8822 - val_loss: 0.5410 - val_acc: 0.8588\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3708 - acc: 0.8824 - val_loss: 0.5416 - val_acc: 0.8584\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3668 - acc: 0.8850 - val_loss: 0.5404 - val_acc: 0.8568\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3675 - acc: 0.8852 - val_loss: 0.5408 - val_acc: 0.8584\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3678 - acc: 0.8850 - val_loss: 0.5411 - val_acc: 0.8592\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3681 - acc: 0.8847 - val_loss: 0.5397 - val_acc: 0.8586\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.3672 - acc: 0.8861 - val_loss: 0.5401 - val_acc: 0.8582\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.3704 - acc: 0.8837 - val_loss: 0.5411 - val_acc: 0.8586\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 7s 155us/sample - loss: 0.3627 - acc: 0.8868 - val_loss: 0.5411 - val_acc: 0.8582\n",
      "Epoch 285/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.3711 - acc: 0.8834 - val_loss: 0.5407 - val_acc: 0.8579\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3651 - acc: 0.8855 - val_loss: 0.5401 - val_acc: 0.8578\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3682 - acc: 0.8863 - val_loss: 0.5423 - val_acc: 0.8580\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.3656 - acc: 0.8849 - val_loss: 0.5427 - val_acc: 0.8588\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3658 - acc: 0.8863 - val_loss: 0.5412 - val_acc: 0.8586\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3664 - acc: 0.8853 - val_loss: 0.5414 - val_acc: 0.8581\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 7s 165us/sample - loss: 0.3690 - acc: 0.8849 - val_loss: 0.5422 - val_acc: 0.8577\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.3666 - acc: 0.8842 - val_loss: 0.5415 - val_acc: 0.8582\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.3686 - acc: 0.8850 - val_loss: 0.5416 - val_acc: 0.8591\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 7s 177us/sample - loss: 0.3627 - acc: 0.8854 - val_loss: 0.5412 - val_acc: 0.8582\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3658 - acc: 0.8854 - val_loss: 0.5437 - val_acc: 0.8581\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 7s 169us/sample - loss: 0.3663 - acc: 0.8832 - val_loss: 0.5405 - val_acc: 0.8586\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.3637 - acc: 0.8864 - val_loss: 0.5406 - val_acc: 0.8573\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.3654 - acc: 0.8859 - val_loss: 0.5407 - val_acc: 0.8585\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 7s 163us/sample - loss: 0.3673 - acc: 0.8838 - val_loss: 0.5424 - val_acc: 0.8588\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 7s 167us/sample - loss: 0.3658 - acc: 0.8852 - val_loss: 0.5400 - val_acc: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d42d89aa58>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_model,y_train_model,          \n",
    "          validation_data=(X_test_model,y_test_model),\n",
    "          epochs=300,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1637   24    9   14   14    4   23   22   23   44]\n",
      " [  27 1603   22   29   46    7   12   47   21   14]\n",
      " [  17   22 1578   39   21   14    5   46   28   33]\n",
      " [  21   36   31 1397   19  107   16   34   37   21]\n",
      " [  21   53   24   23 1603    9   19   13   12   35]\n",
      " [  15   10   11   86   12 1510   46   11   41   26]\n",
      " [  43   23   10   24   39   62 1531    9   71   20]\n",
      " [  14   64   31   28   13    9    9 1611   12   17]\n",
      " [  37   38   24   54   12   30   76    9 1483   49]\n",
      " [  64   37   28   39   19   47   12   21   39 1498]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      1814\n",
      "           1       0.84      0.88      0.86      1828\n",
      "           2       0.89      0.88      0.88      1803\n",
      "           3       0.81      0.81      0.81      1719\n",
      "           4       0.89      0.88      0.89      1812\n",
      "           5       0.84      0.85      0.85      1768\n",
      "           6       0.88      0.84      0.86      1832\n",
      "           7       0.88      0.89      0.89      1808\n",
      "           8       0.84      0.82      0.83      1812\n",
      "           9       0.85      0.83      0.84      1804\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     18000\n",
      "   macro avg       0.86      0.86      0.86     18000\n",
      "weighted avg       0.86      0.86      0.86     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6P2r4oBnDKR"
   },
   "source": [
    "# Implementing the complete network using Tensorflow: Manually Creating Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.asarray(file[\"X_train\"])\n",
    "X_test=np.asarray(file[\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.asarray(file[\"y_train\"])\n",
    "y_test=np.asarray(file[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_shape=32*32\n",
    "n_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=[None,n_input_shape],name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=[None,n_classes],name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(name,shape):\n",
    "    initer=tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable(\"W_\"+name,dtype=tf.float32,shape=shape,initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(name,shape):\n",
    "    initial=tf.constant(0.,shape=shape,dtype=tf.float32)\n",
    "    return tf.get_variable(\"b_\"+name,dtype=tf.float32,initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x,nodes,name,use_relu=True):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    w=weight_variable(name,shape=[in_dim,nodes])\n",
    "    b=bias(name,[nodes])\n",
    "    layer=tf.add(tf.matmul(x,w),b)\n",
    "    if use_relu:\n",
    "        layer=tf.nn.relu(layer)\n",
    "    return layer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "fc1=fc_layer(X,100,\"layer1\",use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2=fc_layer(fc1,200,\"layer2\",use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc3=fc_layer(fc2,150,\"layer3\",use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc4=fc_layer(fc3,60,\"layer4\",use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer=fc_layer(fc4,n_classes,\"output\",use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-ecd0e0f6a838>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction=tf.equal(tf.argmax(output_layer,1),tf.argmax(y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(init)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1024)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 1024)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.30193 TrainAcc: 0.12 TestLoss: 2.3031788 TestAcc: 0.12\n",
      "TrainLoss: 2.301789 TrainAcc: 0.08 TestLoss: 2.302671 TestAcc: 0.16\n",
      "TrainLoss: 2.3023276 TrainAcc: 0.06 TestLoss: 2.3026538 TestAcc: 0.08\n",
      "TrainLoss: 2.3025098 TrainAcc: 0.1 TestLoss: 2.298702 TestAcc: 0.18\n",
      "TrainLoss: 2.3025777 TrainAcc: 0.1 TestLoss: 2.3019164 TestAcc: 0.04\n",
      "TrainLoss: 2.3038473 TrainAcc: 0.04 TestLoss: 2.2996626 TestAcc: 0.1\n",
      "TrainLoss: 2.304449 TrainAcc: 0.06 TestLoss: 2.305266 TestAcc: 0.04\n",
      "TrainLoss: 2.3036685 TrainAcc: 0.08 TestLoss: 2.3018804 TestAcc: 0.12\n",
      "TrainLoss: 2.3008163 TrainAcc: 0.18 TestLoss: 2.3024664 TestAcc: 0.14\n",
      "TrainLoss: 2.302521 TrainAcc: 0.1 TestLoss: 2.30062 TestAcc: 0.12\n",
      "TrainLoss: 2.3029077 TrainAcc: 0.08 TestLoss: 2.302767 TestAcc: 0.1\n",
      "TrainLoss: 2.3045735 TrainAcc: 0.06 TestLoss: 2.3033228 TestAcc: 0.12\n",
      "TrainLoss: 2.3035517 TrainAcc: 0.14 TestLoss: 2.3005548 TestAcc: 0.2\n",
      "TrainLoss: 2.3011057 TrainAcc: 0.12 TestLoss: 2.299236 TestAcc: 0.24\n",
      "TrainLoss: 2.300847 TrainAcc: 0.1 TestLoss: 2.2986956 TestAcc: 0.18\n",
      "TrainLoss: 2.3020747 TrainAcc: 0.1 TestLoss: 2.3028896 TestAcc: 0.12\n",
      "TrainLoss: 2.2997358 TrainAcc: 0.12 TestLoss: 2.302713 TestAcc: 0.14\n",
      "TrainLoss: 2.3011124 TrainAcc: 0.12 TestLoss: 2.30303 TestAcc: 0.1\n",
      "TrainLoss: 2.304661 TrainAcc: 0.08 TestLoss: 2.3035166 TestAcc: 0.1\n",
      "TrainLoss: 2.300131 TrainAcc: 0.12 TestLoss: 2.3022401 TestAcc: 0.18\n",
      "TrainLoss: 2.303372 TrainAcc: 0.1 TestLoss: 2.3018622 TestAcc: 0.08\n",
      "TrainLoss: 2.3018796 TrainAcc: 0.14 TestLoss: 2.3009696 TestAcc: 0.12\n",
      "TrainLoss: 2.3030312 TrainAcc: 0.1 TestLoss: 2.3012006 TestAcc: 0.14\n",
      "TrainLoss: 2.3028617 TrainAcc: 0.04 TestLoss: 2.3064523 TestAcc: 0.06\n",
      "TrainLoss: 2.3012545 TrainAcc: 0.14 TestLoss: 2.3015993 TestAcc: 0.12\n",
      "TrainLoss: 2.3033154 TrainAcc: 0.12 TestLoss: 2.3022199 TestAcc: 0.06\n",
      "TrainLoss: 2.2993188 TrainAcc: 0.18 TestLoss: 2.3049643 TestAcc: 0.08\n",
      "TrainLoss: 2.3064175 TrainAcc: 0.04 TestLoss: 2.3025594 TestAcc: 0.14\n",
      "TrainLoss: 2.3017685 TrainAcc: 0.12 TestLoss: 2.304427 TestAcc: 0.08\n",
      "TrainLoss: 2.3011265 TrainAcc: 0.1 TestLoss: 2.3012426 TestAcc: 0.14\n",
      "TrainLoss: 2.3020408 TrainAcc: 0.08 TestLoss: 2.2988634 TestAcc: 0.08\n",
      "TrainLoss: 2.3032477 TrainAcc: 0.04 TestLoss: 2.3029253 TestAcc: 0.06\n",
      "TrainLoss: 2.3013542 TrainAcc: 0.08 TestLoss: 2.3039315 TestAcc: 0.06\n",
      "TrainLoss: 2.303258 TrainAcc: 0.02 TestLoss: 2.302497 TestAcc: 0.08\n",
      "TrainLoss: 2.3009338 TrainAcc: 0.12 TestLoss: 2.3035636 TestAcc: 0.08\n",
      "TrainLoss: 2.3024633 TrainAcc: 0.16 TestLoss: 2.3027236 TestAcc: 0.14\n",
      "TrainLoss: 2.3051815 TrainAcc: 0.12 TestLoss: 2.3002672 TestAcc: 0.08\n",
      "TrainLoss: 2.30505 TrainAcc: 0.08 TestLoss: 2.300901 TestAcc: 0.16\n",
      "TrainLoss: 2.2995312 TrainAcc: 0.06 TestLoss: 2.304803 TestAcc: 0.08\n",
      "TrainLoss: 2.2998426 TrainAcc: 0.16 TestLoss: 2.3003106 TestAcc: 0.1\n",
      "TrainLoss: 2.302614 TrainAcc: 0.14 TestLoss: 2.300701 TestAcc: 0.16\n",
      "TrainLoss: 2.2982981 TrainAcc: 0.18 TestLoss: 2.3044004 TestAcc: 0.06\n",
      "TrainLoss: 2.305177 TrainAcc: 0.02 TestLoss: 2.3036528 TestAcc: 0.1\n",
      "TrainLoss: 2.3022263 TrainAcc: 0.14 TestLoss: 2.3053904 TestAcc: 0.04\n",
      "TrainLoss: 2.303274 TrainAcc: 0.08 TestLoss: 2.304488 TestAcc: 0.06\n",
      "TrainLoss: 2.300597 TrainAcc: 0.12 TestLoss: 2.2998977 TestAcc: 0.1\n",
      "TrainLoss: 2.3026628 TrainAcc: 0.1 TestLoss: 2.3007321 TestAcc: 0.12\n",
      "TrainLoss: 2.3020387 TrainAcc: 0.1 TestLoss: 2.3038037 TestAcc: 0.04\n",
      "TrainLoss: 2.3025858 TrainAcc: 0.06 TestLoss: 2.3025477 TestAcc: 0.06\n",
      "TrainLoss: 2.3020542 TrainAcc: 0.1 TestLoss: 2.3020165 TestAcc: 0.08\n",
      "TrainLoss: 2.3028438 TrainAcc: 0.06 TestLoss: 2.3001244 TestAcc: 0.1\n",
      "TrainLoss: 2.3016837 TrainAcc: 0.1 TestLoss: 2.3037302 TestAcc: 0.12\n",
      "TrainLoss: 2.301508 TrainAcc: 0.1 TestLoss: 2.3025532 TestAcc: 0.04\n",
      "TrainLoss: 2.3016691 TrainAcc: 0.16 TestLoss: 2.301451 TestAcc: 0.14\n",
      "TrainLoss: 2.3024623 TrainAcc: 0.14 TestLoss: 2.2974806 TestAcc: 0.16\n",
      "TrainLoss: 2.3012092 TrainAcc: 0.16 TestLoss: 2.2999587 TestAcc: 0.12\n",
      "TrainLoss: 2.3019917 TrainAcc: 0.08 TestLoss: 2.3036942 TestAcc: 0.08\n",
      "TrainLoss: 2.2992604 TrainAcc: 0.22 TestLoss: 2.3012705 TestAcc: 0.12\n",
      "TrainLoss: 2.301879 TrainAcc: 0.08 TestLoss: 2.3028102 TestAcc: 0.12\n",
      "TrainLoss: 2.304887 TrainAcc: 0.1 TestLoss: 2.3054476 TestAcc: 0.0\n",
      "TrainLoss: 2.3046865 TrainAcc: 0.04 TestLoss: 2.3041544 TestAcc: 0.08\n",
      "TrainLoss: 2.299967 TrainAcc: 0.14 TestLoss: 2.3023171 TestAcc: 0.1\n",
      "TrainLoss: 2.3025503 TrainAcc: 0.08 TestLoss: 2.3017902 TestAcc: 0.1\n",
      "TrainLoss: 2.298628 TrainAcc: 0.2 TestLoss: 2.3000896 TestAcc: 0.12\n",
      "TrainLoss: 2.301286 TrainAcc: 0.08 TestLoss: 2.3019698 TestAcc: 0.12\n",
      "TrainLoss: 2.2987378 TrainAcc: 0.14 TestLoss: 2.2998617 TestAcc: 0.08\n",
      "TrainLoss: 2.3025215 TrainAcc: 0.12 TestLoss: 2.305178 TestAcc: 0.08\n",
      "TrainLoss: 2.2993786 TrainAcc: 0.12 TestLoss: 2.3057756 TestAcc: 0.06\n",
      "TrainLoss: 2.3032157 TrainAcc: 0.12 TestLoss: 2.3019598 TestAcc: 0.12\n",
      "TrainLoss: 2.303731 TrainAcc: 0.06 TestLoss: 2.301999 TestAcc: 0.12\n",
      "TrainLoss: 2.3005366 TrainAcc: 0.14 TestLoss: 2.3028812 TestAcc: 0.14\n",
      "TrainLoss: 2.3025813 TrainAcc: 0.1 TestLoss: 2.3031847 TestAcc: 0.16\n",
      "TrainLoss: 2.3036878 TrainAcc: 0.12 TestLoss: 2.3064191 TestAcc: 0.08\n",
      "TrainLoss: 2.3024774 TrainAcc: 0.12 TestLoss: 2.3030474 TestAcc: 0.06\n",
      "TrainLoss: 2.3051338 TrainAcc: 0.12 TestLoss: 2.3043723 TestAcc: 0.0\n",
      "TrainLoss: 2.3051527 TrainAcc: 0.1 TestLoss: 2.3022068 TestAcc: 0.1\n",
      "TrainLoss: 2.3019595 TrainAcc: 0.1 TestLoss: 2.3032987 TestAcc: 0.12\n",
      "TrainLoss: 2.3038087 TrainAcc: 0.04 TestLoss: 2.302358 TestAcc: 0.12\n",
      "TrainLoss: 2.302293 TrainAcc: 0.1 TestLoss: 2.3031106 TestAcc: 0.12\n",
      "TrainLoss: 2.3020654 TrainAcc: 0.14 TestLoss: 2.306971 TestAcc: 0.08\n",
      "TrainLoss: 2.3012521 TrainAcc: 0.12 TestLoss: 2.3034487 TestAcc: 0.1\n",
      "TrainLoss: 2.3024278 TrainAcc: 0.12 TestLoss: 2.301316 TestAcc: 0.1\n",
      "TrainLoss: 2.3046923 TrainAcc: 0.02 TestLoss: 2.3036458 TestAcc: 0.08\n",
      "TrainLoss: 2.3025239 TrainAcc: 0.14 TestLoss: 2.3028786 TestAcc: 0.06\n",
      "TrainLoss: 2.3044336 TrainAcc: 0.1 TestLoss: 2.302732 TestAcc: 0.08\n",
      "TrainLoss: 2.3010933 TrainAcc: 0.14 TestLoss: 2.303915 TestAcc: 0.12\n",
      "TrainLoss: 2.300525 TrainAcc: 0.12 TestLoss: 2.3019576 TestAcc: 0.1\n",
      "TrainLoss: 2.3066218 TrainAcc: 0.04 TestLoss: 2.3025737 TestAcc: 0.1\n",
      "TrainLoss: 2.3032212 TrainAcc: 0.08 TestLoss: 2.3027778 TestAcc: 0.12\n",
      "TrainLoss: 2.3067648 TrainAcc: 0.06 TestLoss: 2.3011146 TestAcc: 0.12\n",
      "TrainLoss: 2.3017814 TrainAcc: 0.08 TestLoss: 2.3037698 TestAcc: 0.12\n",
      "TrainLoss: 2.3048568 TrainAcc: 0.04 TestLoss: 2.3014317 TestAcc: 0.14\n",
      "TrainLoss: 2.3034627 TrainAcc: 0.06 TestLoss: 2.3025196 TestAcc: 0.1\n",
      "TrainLoss: 2.306373 TrainAcc: 0.04 TestLoss: 2.3064327 TestAcc: 0.04\n",
      "TrainLoss: 2.3024452 TrainAcc: 0.08 TestLoss: 2.3011167 TestAcc: 0.12\n",
      "TrainLoss: 2.303464 TrainAcc: 0.12 TestLoss: 2.3034763 TestAcc: 0.14\n",
      "TrainLoss: 2.3009408 TrainAcc: 0.16 TestLoss: 2.3012273 TestAcc: 0.1\n",
      "TrainLoss: 2.3048916 TrainAcc: 0.06 TestLoss: 2.3019056 TestAcc: 0.14\n",
      "TrainLoss: 2.3017678 TrainAcc: 0.14 TestLoss: 2.3031287 TestAcc: 0.1\n",
      "TrainLoss: 2.3021834 TrainAcc: 0.08 TestLoss: 2.3007662 TestAcc: 0.18\n",
      "TrainLoss: 2.3039844 TrainAcc: 0.04 TestLoss: 2.2997327 TestAcc: 0.16\n",
      "TrainLoss: 2.3031185 TrainAcc: 0.04 TestLoss: 2.304385 TestAcc: 0.04\n",
      "TrainLoss: 2.3009484 TrainAcc: 0.08 TestLoss: 2.3000345 TestAcc: 0.12\n",
      "TrainLoss: 2.3015046 TrainAcc: 0.12 TestLoss: 2.3014617 TestAcc: 0.1\n",
      "TrainLoss: 2.300452 TrainAcc: 0.16 TestLoss: 2.3035169 TestAcc: 0.06\n",
      "TrainLoss: 2.3022072 TrainAcc: 0.1 TestLoss: 2.3002992 TestAcc: 0.1\n",
      "TrainLoss: 2.3031645 TrainAcc: 0.14 TestLoss: 2.302496 TestAcc: 0.12\n",
      "TrainLoss: 2.3036923 TrainAcc: 0.08 TestLoss: 2.3045578 TestAcc: 0.06\n",
      "TrainLoss: 2.3012342 TrainAcc: 0.22 TestLoss: 2.3007348 TestAcc: 0.16\n",
      "TrainLoss: 2.301152 TrainAcc: 0.14 TestLoss: 2.303185 TestAcc: 0.08\n",
      "TrainLoss: 2.301076 TrainAcc: 0.08 TestLoss: 2.301275 TestAcc: 0.14\n",
      "TrainLoss: 2.298854 TrainAcc: 0.2 TestLoss: 2.3000276 TestAcc: 0.12\n",
      "TrainLoss: 2.3029354 TrainAcc: 0.12 TestLoss: 2.304406 TestAcc: 0.04\n",
      "TrainLoss: 2.3021178 TrainAcc: 0.16 TestLoss: 2.30354 TestAcc: 0.08\n",
      "TrainLoss: 2.3005471 TrainAcc: 0.14 TestLoss: 2.2997081 TestAcc: 0.16\n",
      "TrainLoss: 2.2991838 TrainAcc: 0.16 TestLoss: 2.3006024 TestAcc: 0.18\n",
      "TrainLoss: 2.2986856 TrainAcc: 0.18 TestLoss: 2.3038735 TestAcc: 0.06\n",
      "TrainLoss: 2.3009403 TrainAcc: 0.12 TestLoss: 2.3034327 TestAcc: 0.04\n",
      "TrainLoss: 2.304586 TrainAcc: 0.04 TestLoss: 2.3007452 TestAcc: 0.1\n",
      "TrainLoss: 2.3003435 TrainAcc: 0.16 TestLoss: 2.3033159 TestAcc: 0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3017454 TrainAcc: 0.04 TestLoss: 2.3053129 TestAcc: 0.02\n",
      "TrainLoss: 2.300864 TrainAcc: 0.1 TestLoss: 2.3071609 TestAcc: 0.06\n",
      "TrainLoss: 2.301652 TrainAcc: 0.14 TestLoss: 2.3037531 TestAcc: 0.02\n",
      "TrainLoss: 2.3025146 TrainAcc: 0.08 TestLoss: 2.3029609 TestAcc: 0.12\n",
      "TrainLoss: 2.3027527 TrainAcc: 0.14 TestLoss: 2.3034928 TestAcc: 0.06\n",
      "TrainLoss: 2.3011231 TrainAcc: 0.12 TestLoss: 2.3026924 TestAcc: 0.06\n",
      "TrainLoss: 2.302585 TrainAcc: 0.1 TestLoss: 2.3050692 TestAcc: 0.06\n",
      "TrainLoss: 2.3028197 TrainAcc: 0.14 TestLoss: 2.3013809 TestAcc: 0.2\n",
      "TrainLoss: 2.3023715 TrainAcc: 0.06 TestLoss: 2.3063805 TestAcc: 0.04\n",
      "TrainLoss: 2.2990081 TrainAcc: 0.16 TestLoss: 2.3002279 TestAcc: 0.12\n",
      "TrainLoss: 2.3053193 TrainAcc: 0.04 TestLoss: 2.304141 TestAcc: 0.06\n",
      "TrainLoss: 2.3004823 TrainAcc: 0.12 TestLoss: 2.3025596 TestAcc: 0.1\n",
      "TrainLoss: 2.3041573 TrainAcc: 0.1 TestLoss: 2.3055758 TestAcc: 0.06\n",
      "TrainLoss: 2.3052158 TrainAcc: 0.08 TestLoss: 2.2974322 TestAcc: 0.18\n",
      "TrainLoss: 2.3006122 TrainAcc: 0.18 TestLoss: 2.3055348 TestAcc: 0.04\n",
      "TrainLoss: 2.3008416 TrainAcc: 0.12 TestLoss: 2.3047817 TestAcc: 0.06\n",
      "TrainLoss: 2.3035178 TrainAcc: 0.08 TestLoss: 2.304834 TestAcc: 0.04\n",
      "TrainLoss: 2.300915 TrainAcc: 0.14 TestLoss: 2.300891 TestAcc: 0.12\n",
      "TrainLoss: 2.3024514 TrainAcc: 0.12 TestLoss: 2.3028402 TestAcc: 0.1\n",
      "TrainLoss: 2.304153 TrainAcc: 0.06 TestLoss: 2.3036437 TestAcc: 0.06\n",
      "TrainLoss: 2.3004463 TrainAcc: 0.1 TestLoss: 2.3037853 TestAcc: 0.06\n",
      "TrainLoss: 2.301706 TrainAcc: 0.12 TestLoss: 2.3027346 TestAcc: 0.08\n",
      "TrainLoss: 2.301887 TrainAcc: 0.14 TestLoss: 2.3039594 TestAcc: 0.08\n",
      "TrainLoss: 2.3042207 TrainAcc: 0.04 TestLoss: 2.3045695 TestAcc: 0.06\n",
      "TrainLoss: 2.3037782 TrainAcc: 0.08 TestLoss: 2.3024986 TestAcc: 0.1\n",
      "TrainLoss: 2.302543 TrainAcc: 0.1 TestLoss: 2.3036714 TestAcc: 0.12\n",
      "TrainLoss: 2.3032491 TrainAcc: 0.14 TestLoss: 2.3038008 TestAcc: 0.08\n",
      "TrainLoss: 2.305199 TrainAcc: 0.1 TestLoss: 2.3025854 TestAcc: 0.1\n",
      "TrainLoss: 2.3005135 TrainAcc: 0.1 TestLoss: 2.3014774 TestAcc: 0.16\n",
      "TrainLoss: 2.3032537 TrainAcc: 0.08 TestLoss: 2.3001313 TestAcc: 0.12\n",
      "TrainLoss: 2.3010488 TrainAcc: 0.1 TestLoss: 2.3041816 TestAcc: 0.08\n",
      "TrainLoss: 2.3021092 TrainAcc: 0.14 TestLoss: 2.3044934 TestAcc: 0.08\n",
      "TrainLoss: 2.3026943 TrainAcc: 0.04 TestLoss: 2.3029046 TestAcc: 0.08\n",
      "TrainLoss: 2.3061483 TrainAcc: 0.04 TestLoss: 2.3027947 TestAcc: 0.06\n",
      "TrainLoss: 2.3025832 TrainAcc: 0.14 TestLoss: 2.300889 TestAcc: 0.16\n",
      "TrainLoss: 2.3007135 TrainAcc: 0.12 TestLoss: 2.304914 TestAcc: 0.14\n",
      "TrainLoss: 2.3009493 TrainAcc: 0.12 TestLoss: 2.3021255 TestAcc: 0.12\n",
      "TrainLoss: 2.3015032 TrainAcc: 0.12 TestLoss: 2.2990491 TestAcc: 0.14\n",
      "TrainLoss: 2.3050542 TrainAcc: 0.06 TestLoss: 2.3020215 TestAcc: 0.1\n",
      "TrainLoss: 2.301058 TrainAcc: 0.14 TestLoss: 2.3024752 TestAcc: 0.08\n",
      "TrainLoss: 2.3015745 TrainAcc: 0.12 TestLoss: 2.30292 TestAcc: 0.1\n",
      "TrainLoss: 2.299952 TrainAcc: 0.24 TestLoss: 2.3006883 TestAcc: 0.1\n",
      "TrainLoss: 2.3064678 TrainAcc: 0.08 TestLoss: 2.30041 TestAcc: 0.18\n",
      "TrainLoss: 2.3022738 TrainAcc: 0.14 TestLoss: 2.3039308 TestAcc: 0.06\n",
      "TrainLoss: 2.3022513 TrainAcc: 0.08 TestLoss: 2.3034787 TestAcc: 0.06\n",
      "TrainLoss: 2.3005586 TrainAcc: 0.08 TestLoss: 2.302593 TestAcc: 0.12\n",
      "TrainLoss: 2.30049 TrainAcc: 0.14 TestLoss: 2.3020408 TestAcc: 0.1\n",
      "TrainLoss: 2.3007383 TrainAcc: 0.2 TestLoss: 2.30392 TestAcc: 0.08\n",
      "TrainLoss: 2.3065739 TrainAcc: 0.04 TestLoss: 2.3021734 TestAcc: 0.14\n",
      "TrainLoss: 2.3027303 TrainAcc: 0.08 TestLoss: 2.3010263 TestAcc: 0.18\n",
      "TrainLoss: 2.3023386 TrainAcc: 0.1 TestLoss: 2.302377 TestAcc: 0.08\n",
      "TrainLoss: 2.2982664 TrainAcc: 0.14 TestLoss: 2.301675 TestAcc: 0.08\n",
      "TrainLoss: 2.3008838 TrainAcc: 0.16 TestLoss: 2.302648 TestAcc: 0.06\n",
      "TrainLoss: 2.3022385 TrainAcc: 0.1 TestLoss: 2.300515 TestAcc: 0.12\n",
      "TrainLoss: 2.3012762 TrainAcc: 0.08 TestLoss: 2.3017232 TestAcc: 0.14\n",
      "TrainLoss: 2.3045993 TrainAcc: 0.08 TestLoss: 2.3051429 TestAcc: 0.1\n",
      "TrainLoss: 2.2977166 TrainAcc: 0.26 TestLoss: 2.2983673 TestAcc: 0.14\n",
      "TrainLoss: 2.3020048 TrainAcc: 0.08 TestLoss: 2.3079894 TestAcc: 0.04\n",
      "TrainLoss: 2.3043408 TrainAcc: 0.06 TestLoss: 2.3050234 TestAcc: 0.08\n",
      "TrainLoss: 2.3036695 TrainAcc: 0.08 TestLoss: 2.301325 TestAcc: 0.08\n",
      "TrainLoss: 2.3038251 TrainAcc: 0.1 TestLoss: 2.3040237 TestAcc: 0.12\n",
      "TrainLoss: 2.3031669 TrainAcc: 0.08 TestLoss: 2.3032105 TestAcc: 0.1\n",
      "TrainLoss: 2.3053548 TrainAcc: 0.06 TestLoss: 2.3054838 TestAcc: 0.04\n",
      "TrainLoss: 2.3040354 TrainAcc: 0.1 TestLoss: 2.3019996 TestAcc: 0.08\n",
      "TrainLoss: 2.2974017 TrainAcc: 0.28 TestLoss: 2.3010151 TestAcc: 0.12\n",
      "TrainLoss: 2.3050556 TrainAcc: 0.08 TestLoss: 2.3042426 TestAcc: 0.04\n",
      "TrainLoss: 2.3046148 TrainAcc: 0.02 TestLoss: 2.301238 TestAcc: 0.12\n",
      "TrainLoss: 2.2991648 TrainAcc: 0.16 TestLoss: 2.3017924 TestAcc: 0.12\n",
      "TrainLoss: 2.3021083 TrainAcc: 0.16 TestLoss: 2.3009248 TestAcc: 0.12\n",
      "TrainLoss: 2.3020458 TrainAcc: 0.08 TestLoss: 2.3011582 TestAcc: 0.16\n",
      "TrainLoss: 2.3037412 TrainAcc: 0.08 TestLoss: 2.3022783 TestAcc: 0.08\n",
      "TrainLoss: 2.2993188 TrainAcc: 0.14 TestLoss: 2.3025734 TestAcc: 0.08\n",
      "TrainLoss: 2.3035064 TrainAcc: 0.06 TestLoss: 2.303346 TestAcc: 0.1\n",
      "TrainLoss: 2.3039856 TrainAcc: 0.08 TestLoss: 2.3033204 TestAcc: 0.12\n",
      "TrainLoss: 2.3033292 TrainAcc: 0.02 TestLoss: 2.3043058 TestAcc: 0.04\n",
      "TrainLoss: 2.3043587 TrainAcc: 0.12 TestLoss: 2.3001776 TestAcc: 0.1\n",
      "TrainLoss: 2.302092 TrainAcc: 0.1 TestLoss: 2.302568 TestAcc: 0.06\n",
      "TrainLoss: 2.3033054 TrainAcc: 0.1 TestLoss: 2.306153 TestAcc: 0.04\n",
      "TrainLoss: 2.3010175 TrainAcc: 0.08 TestLoss: 2.3011575 TestAcc: 0.14\n",
      "TrainLoss: 2.303537 TrainAcc: 0.12 TestLoss: 2.3006365 TestAcc: 0.1\n",
      "TrainLoss: 2.3044355 TrainAcc: 0.08 TestLoss: 2.3030558 TestAcc: 0.12\n",
      "TrainLoss: 2.3042707 TrainAcc: 0.02 TestLoss: 2.303567 TestAcc: 0.04\n",
      "TrainLoss: 2.3020022 TrainAcc: 0.1 TestLoss: 2.3022192 TestAcc: 0.14\n",
      "TrainLoss: 2.3045013 TrainAcc: 0.08 TestLoss: 2.3020852 TestAcc: 0.12\n",
      "TrainLoss: 2.3014164 TrainAcc: 0.1 TestLoss: 2.3019798 TestAcc: 0.1\n",
      "TrainLoss: 2.3022866 TrainAcc: 0.06 TestLoss: 2.3011596 TestAcc: 0.12\n",
      "TrainLoss: 2.299782 TrainAcc: 0.12 TestLoss: 2.3067603 TestAcc: 0.06\n",
      "TrainLoss: 2.303988 TrainAcc: 0.04 TestLoss: 2.3047268 TestAcc: 0.08\n",
      "TrainLoss: 2.3000393 TrainAcc: 0.16 TestLoss: 2.3006492 TestAcc: 0.16\n",
      "TrainLoss: 2.3015287 TrainAcc: 0.16 TestLoss: 2.3050823 TestAcc: 0.06\n",
      "TrainLoss: 2.302228 TrainAcc: 0.12 TestLoss: 2.301418 TestAcc: 0.14\n",
      "TrainLoss: 2.3021863 TrainAcc: 0.1 TestLoss: 2.3063445 TestAcc: 0.04\n",
      "TrainLoss: 2.2984838 TrainAcc: 0.16 TestLoss: 2.297392 TestAcc: 0.18\n",
      "TrainLoss: 2.2985451 TrainAcc: 0.18 TestLoss: 2.3027914 TestAcc: 0.06\n",
      "TrainLoss: 2.3022866 TrainAcc: 0.06 TestLoss: 2.302548 TestAcc: 0.12\n",
      "TrainLoss: 2.3015997 TrainAcc: 0.12 TestLoss: 2.3036408 TestAcc: 0.06\n",
      "TrainLoss: 2.3026574 TrainAcc: 0.1 TestLoss: 2.2996404 TestAcc: 0.16\n",
      "TrainLoss: 2.3044922 TrainAcc: 0.06 TestLoss: 2.3035157 TestAcc: 0.06\n",
      "TrainLoss: 2.3012533 TrainAcc: 0.16 TestLoss: 2.3035553 TestAcc: 0.12\n",
      "TrainLoss: 2.3001852 TrainAcc: 0.16 TestLoss: 2.3042045 TestAcc: 0.1\n",
      "TrainLoss: 2.3031244 TrainAcc: 0.04 TestLoss: 2.304089 TestAcc: 0.06\n",
      "TrainLoss: 2.30186 TrainAcc: 0.08 TestLoss: 2.3018868 TestAcc: 0.1\n",
      "TrainLoss: 2.2998042 TrainAcc: 0.12 TestLoss: 2.300061 TestAcc: 0.08\n",
      "TrainLoss: 2.3014452 TrainAcc: 0.12 TestLoss: 2.3030393 TestAcc: 0.08\n",
      "TrainLoss: 2.3015814 TrainAcc: 0.12 TestLoss: 2.303053 TestAcc: 0.06\n",
      "TrainLoss: 2.299042 TrainAcc: 0.14 TestLoss: 2.2992775 TestAcc: 0.2\n",
      "TrainLoss: 2.3034432 TrainAcc: 0.04 TestLoss: 2.3010836 TestAcc: 0.06\n",
      "TrainLoss: 2.3021803 TrainAcc: 0.2 TestLoss: 2.3014457 TestAcc: 0.1\n",
      "TrainLoss: 2.3038158 TrainAcc: 0.06 TestLoss: 2.3058536 TestAcc: 0.06\n",
      "TrainLoss: 2.3019648 TrainAcc: 0.08 TestLoss: 2.3029723 TestAcc: 0.14\n",
      "TrainLoss: 2.3004327 TrainAcc: 0.14 TestLoss: 2.3022516 TestAcc: 0.14\n",
      "TrainLoss: 2.302066 TrainAcc: 0.06 TestLoss: 2.3006628 TestAcc: 0.1\n",
      "TrainLoss: 2.302886 TrainAcc: 0.1 TestLoss: 2.3029358 TestAcc: 0.1\n",
      "TrainLoss: 2.300717 TrainAcc: 0.1 TestLoss: 2.3041549 TestAcc: 0.12\n",
      "TrainLoss: 2.3035145 TrainAcc: 0.1 TestLoss: 2.302724 TestAcc: 0.1\n",
      "TrainLoss: 2.302228 TrainAcc: 0.1 TestLoss: 2.3019843 TestAcc: 0.1\n",
      "TrainLoss: 2.3023672 TrainAcc: 0.14 TestLoss: 2.30311 TestAcc: 0.1\n",
      "TrainLoss: 2.3030028 TrainAcc: 0.1 TestLoss: 2.3030913 TestAcc: 0.08\n",
      "TrainLoss: 2.302079 TrainAcc: 0.1 TestLoss: 2.3041553 TestAcc: 0.08\n",
      "TrainLoss: 2.3021598 TrainAcc: 0.08 TestLoss: 2.2993505 TestAcc: 0.18\n",
      "TrainLoss: 2.3045642 TrainAcc: 0.1 TestLoss: 2.3008456 TestAcc: 0.18\n",
      "TrainLoss: 2.3024554 TrainAcc: 0.12 TestLoss: 2.304467 TestAcc: 0.06\n",
      "TrainLoss: 2.3018975 TrainAcc: 0.06 TestLoss: 2.3042817 TestAcc: 0.1\n",
      "TrainLoss: 2.3027444 TrainAcc: 0.1 TestLoss: 2.300105 TestAcc: 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3061182 TrainAcc: 0.04 TestLoss: 2.3052778 TestAcc: 0.06\n",
      "TrainLoss: 2.3011649 TrainAcc: 0.16 TestLoss: 2.3050857 TestAcc: 0.04\n",
      "TrainLoss: 2.2996418 TrainAcc: 0.14 TestLoss: 2.3031275 TestAcc: 0.16\n",
      "TrainLoss: 2.2994783 TrainAcc: 0.12 TestLoss: 2.3025854 TestAcc: 0.12\n",
      "TrainLoss: 2.3002768 TrainAcc: 0.14 TestLoss: 2.3031123 TestAcc: 0.1\n",
      "TrainLoss: 2.3038893 TrainAcc: 0.06 TestLoss: 2.302474 TestAcc: 0.12\n",
      "TrainLoss: 2.3029346 TrainAcc: 0.1 TestLoss: 2.3057663 TestAcc: 0.1\n",
      "TrainLoss: 2.3024337 TrainAcc: 0.14 TestLoss: 2.3011482 TestAcc: 0.12\n",
      "TrainLoss: 2.299629 TrainAcc: 0.08 TestLoss: 2.3039355 TestAcc: 0.06\n",
      "TrainLoss: 2.301599 TrainAcc: 0.1 TestLoss: 2.2997324 TestAcc: 0.22\n",
      "TrainLoss: 2.301552 TrainAcc: 0.08 TestLoss: 2.3055964 TestAcc: 0.02\n",
      "TrainLoss: 2.3014228 TrainAcc: 0.12 TestLoss: 2.3038123 TestAcc: 0.08\n",
      "TrainLoss: 2.3017514 TrainAcc: 0.12 TestLoss: 2.3021286 TestAcc: 0.08\n",
      "TrainLoss: 2.304114 TrainAcc: 0.04 TestLoss: 2.3002388 TestAcc: 0.1\n",
      "TrainLoss: 2.3034494 TrainAcc: 0.08 TestLoss: 2.3046248 TestAcc: 0.02\n",
      "TrainLoss: 2.3012779 TrainAcc: 0.1 TestLoss: 2.3003678 TestAcc: 0.14\n",
      "TrainLoss: 2.299735 TrainAcc: 0.08 TestLoss: 2.3031843 TestAcc: 0.12\n",
      "TrainLoss: 2.302374 TrainAcc: 0.1 TestLoss: 2.3025956 TestAcc: 0.1\n",
      "TrainLoss: 2.3041108 TrainAcc: 0.08 TestLoss: 2.3010645 TestAcc: 0.14\n",
      "TrainLoss: 2.3028586 TrainAcc: 0.08 TestLoss: 2.3044562 TestAcc: 0.08\n",
      "TrainLoss: 2.3024447 TrainAcc: 0.1 TestLoss: 2.3074834 TestAcc: 0.02\n",
      "TrainLoss: 2.3013623 TrainAcc: 0.08 TestLoss: 2.3019822 TestAcc: 0.08\n",
      "TrainLoss: 2.3017137 TrainAcc: 0.12 TestLoss: 2.3033254 TestAcc: 0.12\n",
      "TrainLoss: 2.2988887 TrainAcc: 0.16 TestLoss: 2.2988503 TestAcc: 0.14\n",
      "TrainLoss: 2.301291 TrainAcc: 0.08 TestLoss: 2.304749 TestAcc: 0.06\n",
      "TrainLoss: 2.3065836 TrainAcc: 0.04 TestLoss: 2.3015695 TestAcc: 0.1\n",
      "TrainLoss: 2.2989724 TrainAcc: 0.18 TestLoss: 2.3041916 TestAcc: 0.06\n",
      "TrainLoss: 2.305193 TrainAcc: 0.08 TestLoss: 2.3053713 TestAcc: 0.06\n",
      "TrainLoss: 2.3039765 TrainAcc: 0.1 TestLoss: 2.2997534 TestAcc: 0.14\n",
      "TrainLoss: 2.30329 TrainAcc: 0.12 TestLoss: 2.3053842 TestAcc: 0.12\n",
      "TrainLoss: 2.3005984 TrainAcc: 0.12 TestLoss: 2.3060536 TestAcc: 0.06\n",
      "TrainLoss: 2.297703 TrainAcc: 0.12 TestLoss: 2.302289 TestAcc: 0.1\n",
      "TrainLoss: 2.299581 TrainAcc: 0.08 TestLoss: 2.3037543 TestAcc: 0.08\n",
      "TrainLoss: 2.3050916 TrainAcc: 0.06 TestLoss: 2.2982752 TestAcc: 0.16\n",
      "TrainLoss: 2.3021462 TrainAcc: 0.1 TestLoss: 2.3071518 TestAcc: 0.02\n",
      "TrainLoss: 2.3041341 TrainAcc: 0.1 TestLoss: 2.3041239 TestAcc: 0.02\n",
      "TrainLoss: 2.3026626 TrainAcc: 0.08 TestLoss: 2.3058524 TestAcc: 0.04\n",
      "TrainLoss: 2.3014715 TrainAcc: 0.18 TestLoss: 2.300917 TestAcc: 0.16\n",
      "TrainLoss: 2.3036466 TrainAcc: 0.04 TestLoss: 2.3003886 TestAcc: 0.14\n",
      "TrainLoss: 2.2966115 TrainAcc: 0.16 TestLoss: 2.303432 TestAcc: 0.12\n",
      "TrainLoss: 2.3041608 TrainAcc: 0.04 TestLoss: 2.3043983 TestAcc: 0.02\n",
      "TrainLoss: 2.3005354 TrainAcc: 0.14 TestLoss: 2.3084724 TestAcc: 0.06\n",
      "TrainLoss: 2.3028998 TrainAcc: 0.12 TestLoss: 2.3026917 TestAcc: 0.12\n",
      "TrainLoss: 2.3033767 TrainAcc: 0.06 TestLoss: 2.3044944 TestAcc: 0.08\n",
      "TrainLoss: 2.3004475 TrainAcc: 0.14 TestLoss: 2.3029413 TestAcc: 0.12\n",
      "TrainLoss: 2.2985697 TrainAcc: 0.1 TestLoss: 2.30165 TestAcc: 0.08\n",
      "TrainLoss: 2.3010292 TrainAcc: 0.12 TestLoss: 2.3063269 TestAcc: 0.04\n",
      "TrainLoss: 2.3043888 TrainAcc: 0.06 TestLoss: 2.3019614 TestAcc: 0.08\n",
      "TrainLoss: 2.3048093 TrainAcc: 0.14 TestLoss: 2.3021994 TestAcc: 0.1\n",
      "TrainLoss: 2.3015053 TrainAcc: 0.08 TestLoss: 2.302886 TestAcc: 0.12\n",
      "TrainLoss: 2.3001623 TrainAcc: 0.08 TestLoss: 2.299602 TestAcc: 0.14\n",
      "TrainLoss: 2.300231 TrainAcc: 0.1 TestLoss: 2.303484 TestAcc: 0.1\n",
      "TrainLoss: 2.3059494 TrainAcc: 0.08 TestLoss: 2.3010268 TestAcc: 0.06\n",
      "TrainLoss: 2.3014622 TrainAcc: 0.1 TestLoss: 2.3036792 TestAcc: 0.08\n",
      "TrainLoss: 2.301427 TrainAcc: 0.12 TestLoss: 2.3044288 TestAcc: 0.08\n",
      "TrainLoss: 2.3072922 TrainAcc: 0.04 TestLoss: 2.2992158 TestAcc: 0.16\n",
      "TrainLoss: 2.3038604 TrainAcc: 0.12 TestLoss: 2.3045266 TestAcc: 0.1\n",
      "TrainLoss: 2.3050582 TrainAcc: 0.06 TestLoss: 2.3045995 TestAcc: 0.06\n",
      "TrainLoss: 2.3042693 TrainAcc: 0.04 TestLoss: 2.3062644 TestAcc: 0.04\n",
      "TrainLoss: 2.303336 TrainAcc: 0.1 TestLoss: 2.3024561 TestAcc: 0.06\n",
      "TrainLoss: 2.30178 TrainAcc: 0.1 TestLoss: 2.3040512 TestAcc: 0.1\n",
      "TrainLoss: 2.298129 TrainAcc: 0.16 TestLoss: 2.304487 TestAcc: 0.06\n",
      "TrainLoss: 2.301845 TrainAcc: 0.14 TestLoss: 2.3025978 TestAcc: 0.1\n",
      "TrainLoss: 2.298826 TrainAcc: 0.16 TestLoss: 2.305541 TestAcc: 0.04\n",
      "TrainLoss: 2.299575 TrainAcc: 0.18 TestLoss: 2.3043718 TestAcc: 0.08\n",
      "TrainLoss: 2.3039849 TrainAcc: 0.06 TestLoss: 2.3029757 TestAcc: 0.08\n",
      "TrainLoss: 2.3067038 TrainAcc: 0.04 TestLoss: 2.3043375 TestAcc: 0.02\n",
      "TrainLoss: 2.303521 TrainAcc: 0.08 TestLoss: 2.2998056 TestAcc: 0.12\n",
      "TrainLoss: 2.3038757 TrainAcc: 0.04 TestLoss: 2.3055773 TestAcc: 0.02\n",
      "TrainLoss: 2.3013635 TrainAcc: 0.1 TestLoss: 2.301749 TestAcc: 0.06\n",
      "TrainLoss: 2.2982986 TrainAcc: 0.18 TestLoss: 2.3050857 TestAcc: 0.08\n",
      "TrainLoss: 2.3017695 TrainAcc: 0.14 TestLoss: 2.302203 TestAcc: 0.1\n",
      "TrainLoss: 2.3056078 TrainAcc: 0.02 TestLoss: 2.3025856 TestAcc: 0.06\n",
      "TrainLoss: 2.300338 TrainAcc: 0.16 TestLoss: 2.3027277 TestAcc: 0.12\n",
      "TrainLoss: 2.3003628 TrainAcc: 0.1 TestLoss: 2.3026304 TestAcc: 0.1\n",
      "TrainLoss: 2.3029203 TrainAcc: 0.14 TestLoss: 2.3019783 TestAcc: 0.14\n",
      "TrainLoss: 2.3020887 TrainAcc: 0.12 TestLoss: 2.3041904 TestAcc: 0.06\n",
      "TrainLoss: 2.3045514 TrainAcc: 0.06 TestLoss: 2.301921 TestAcc: 0.16\n",
      "TrainLoss: 2.3008966 TrainAcc: 0.06 TestLoss: 2.3026807 TestAcc: 0.04\n",
      "TrainLoss: 2.3030512 TrainAcc: 0.12 TestLoss: 2.3039167 TestAcc: 0.06\n",
      "TrainLoss: 2.302194 TrainAcc: 0.08 TestLoss: 2.3031476 TestAcc: 0.1\n",
      "TrainLoss: 2.3011372 TrainAcc: 0.1 TestLoss: 2.3037367 TestAcc: 0.08\n",
      "TrainLoss: 2.3035889 TrainAcc: 0.16 TestLoss: 2.3000336 TestAcc: 0.16\n",
      "TrainLoss: 2.3001134 TrainAcc: 0.18 TestLoss: 2.3057907 TestAcc: 0.1\n",
      "TrainLoss: 2.3013241 TrainAcc: 0.2 TestLoss: 2.3040793 TestAcc: 0.1\n",
      "TrainLoss: 2.3066897 TrainAcc: 0.0 TestLoss: 2.304846 TestAcc: 0.08\n",
      "TrainLoss: 2.3030634 TrainAcc: 0.08 TestLoss: 2.2995772 TestAcc: 0.12\n",
      "TrainLoss: 2.2970152 TrainAcc: 0.16 TestLoss: 2.299741 TestAcc: 0.1\n",
      "TrainLoss: 2.3024802 TrainAcc: 0.1 TestLoss: 2.3053367 TestAcc: 0.12\n",
      "TrainLoss: 2.3042686 TrainAcc: 0.08 TestLoss: 2.304742 TestAcc: 0.08\n",
      "TrainLoss: 2.3019438 TrainAcc: 0.12 TestLoss: 2.3044307 TestAcc: 0.06\n",
      "TrainLoss: 2.3015978 TrainAcc: 0.06 TestLoss: 2.3070087 TestAcc: 0.08\n",
      "TrainLoss: 2.306082 TrainAcc: 0.02 TestLoss: 2.3021626 TestAcc: 0.1\n",
      "TrainLoss: 2.3015366 TrainAcc: 0.18 TestLoss: 2.3056633 TestAcc: 0.06\n",
      "TrainLoss: 2.3009975 TrainAcc: 0.12 TestLoss: 2.3043694 TestAcc: 0.1\n",
      "TrainLoss: 2.304475 TrainAcc: 0.06 TestLoss: 2.3021545 TestAcc: 0.14\n",
      "TrainLoss: 2.3026497 TrainAcc: 0.1 TestLoss: 2.3037918 TestAcc: 0.1\n",
      "TrainLoss: 2.2997477 TrainAcc: 0.1 TestLoss: 2.3009155 TestAcc: 0.06\n",
      "TrainLoss: 2.3024337 TrainAcc: 0.14 TestLoss: 2.305754 TestAcc: 0.04\n",
      "TrainLoss: 2.3036804 TrainAcc: 0.08 TestLoss: 2.3006055 TestAcc: 0.18\n",
      "TrainLoss: 2.3031874 TrainAcc: 0.12 TestLoss: 2.3035233 TestAcc: 0.14\n",
      "TrainLoss: 2.3004732 TrainAcc: 0.14 TestLoss: 2.3022676 TestAcc: 0.14\n",
      "TrainLoss: 2.2994244 TrainAcc: 0.18 TestLoss: 2.3031726 TestAcc: 0.08\n",
      "TrainLoss: 2.3026464 TrainAcc: 0.08 TestLoss: 2.3011048 TestAcc: 0.14\n",
      "TrainLoss: 2.2978258 TrainAcc: 0.08 TestLoss: 2.3070583 TestAcc: 0.06\n",
      "TrainLoss: 2.3061113 TrainAcc: 0.06 TestLoss: 2.3040588 TestAcc: 0.1\n",
      "TrainLoss: 2.3023546 TrainAcc: 0.1 TestLoss: 2.3011866 TestAcc: 0.14\n",
      "TrainLoss: 2.3054428 TrainAcc: 0.14 TestLoss: 2.3033774 TestAcc: 0.1\n",
      "TrainLoss: 2.3023415 TrainAcc: 0.14 TestLoss: 2.3050573 TestAcc: 0.06\n",
      "TrainLoss: 2.2989068 TrainAcc: 0.14 TestLoss: 2.3020597 TestAcc: 0.1\n",
      "TrainLoss: 2.302231 TrainAcc: 0.12 TestLoss: 2.3046484 TestAcc: 0.12\n",
      "TrainLoss: 2.304278 TrainAcc: 0.06 TestLoss: 2.304315 TestAcc: 0.14\n",
      "TrainLoss: 2.3027794 TrainAcc: 0.12 TestLoss: 2.304137 TestAcc: 0.1\n",
      "TrainLoss: 2.3049498 TrainAcc: 0.1 TestLoss: 2.3027334 TestAcc: 0.06\n",
      "TrainLoss: 2.3009388 TrainAcc: 0.12 TestLoss: 2.3023844 TestAcc: 0.1\n",
      "TrainLoss: 2.3000095 TrainAcc: 0.08 TestLoss: 2.306713 TestAcc: 0.04\n",
      "TrainLoss: 2.3034241 TrainAcc: 0.12 TestLoss: 2.3028479 TestAcc: 0.12\n",
      "TrainLoss: 2.3022048 TrainAcc: 0.08 TestLoss: 2.3030949 TestAcc: 0.06\n",
      "TrainLoss: 2.3040175 TrainAcc: 0.1 TestLoss: 2.304544 TestAcc: 0.08\n",
      "TrainLoss: 2.3014026 TrainAcc: 0.12 TestLoss: 2.307131 TestAcc: 0.06\n",
      "TrainLoss: 2.3027039 TrainAcc: 0.12 TestLoss: 2.3030424 TestAcc: 0.12\n",
      "TrainLoss: 2.304498 TrainAcc: 0.1 TestLoss: 2.303883 TestAcc: 0.12\n",
      "TrainLoss: 2.302394 TrainAcc: 0.14 TestLoss: 2.3026016 TestAcc: 0.1\n",
      "TrainLoss: 2.3047292 TrainAcc: 0.12 TestLoss: 2.3029037 TestAcc: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3029644 TrainAcc: 0.08 TestLoss: 2.3008785 TestAcc: 0.12\n",
      "TrainLoss: 2.3008273 TrainAcc: 0.12 TestLoss: 2.3010933 TestAcc: 0.08\n",
      "TrainLoss: 2.30391 TrainAcc: 0.04 TestLoss: 2.3041282 TestAcc: 0.06\n",
      "TrainLoss: 2.299287 TrainAcc: 0.16 TestLoss: 2.3043349 TestAcc: 0.08\n",
      "TrainLoss: 2.300955 TrainAcc: 0.08 TestLoss: 2.3034284 TestAcc: 0.04\n",
      "TrainLoss: 2.302041 TrainAcc: 0.02 TestLoss: 2.3013556 TestAcc: 0.12\n",
      "TrainLoss: 2.302963 TrainAcc: 0.12 TestLoss: 2.305487 TestAcc: 0.08\n",
      "TrainLoss: 2.3005967 TrainAcc: 0.12 TestLoss: 2.299915 TestAcc: 0.14\n",
      "TrainLoss: 2.3033946 TrainAcc: 0.02 TestLoss: 2.3023992 TestAcc: 0.14\n",
      "TrainLoss: 2.3009677 TrainAcc: 0.1 TestLoss: 2.3051689 TestAcc: 0.1\n",
      "TrainLoss: 2.3047793 TrainAcc: 0.1 TestLoss: 2.301249 TestAcc: 0.04\n",
      "TrainLoss: 2.3013604 TrainAcc: 0.12 TestLoss: 2.3030493 TestAcc: 0.06\n",
      "TrainLoss: 2.3007221 TrainAcc: 0.08 TestLoss: 2.305031 TestAcc: 0.06\n",
      "TrainLoss: 2.3018346 TrainAcc: 0.14 TestLoss: 2.3028252 TestAcc: 0.12\n",
      "TrainLoss: 2.3013058 TrainAcc: 0.16 TestLoss: 2.3009646 TestAcc: 0.16\n",
      "TrainLoss: 2.3031359 TrainAcc: 0.14 TestLoss: 2.303356 TestAcc: 0.08\n",
      "TrainLoss: 2.3032465 TrainAcc: 0.14 TestLoss: 2.3037233 TestAcc: 0.1\n",
      "TrainLoss: 2.3010197 TrainAcc: 0.08 TestLoss: 2.3058512 TestAcc: 0.1\n",
      "TrainLoss: 2.2987645 TrainAcc: 0.1 TestLoss: 2.305229 TestAcc: 0.1\n",
      "TrainLoss: 2.303235 TrainAcc: 0.08 TestLoss: 2.303345 TestAcc: 0.1\n",
      "TrainLoss: 2.3009841 TrainAcc: 0.1 TestLoss: 2.3025417 TestAcc: 0.06\n",
      "TrainLoss: 2.3026583 TrainAcc: 0.12 TestLoss: 2.3038595 TestAcc: 0.06\n",
      "TrainLoss: 2.3025277 TrainAcc: 0.1 TestLoss: 2.3002636 TestAcc: 0.14\n",
      "TrainLoss: 2.3016815 TrainAcc: 0.14 TestLoss: 2.3042483 TestAcc: 0.08\n",
      "TrainLoss: 2.3020408 TrainAcc: 0.1 TestLoss: 2.3010998 TestAcc: 0.12\n",
      "TrainLoss: 2.3033237 TrainAcc: 0.08 TestLoss: 2.306013 TestAcc: 0.06\n",
      "TrainLoss: 2.302192 TrainAcc: 0.1 TestLoss: 2.3002388 TestAcc: 0.12\n",
      "TrainLoss: 2.3036985 TrainAcc: 0.08 TestLoss: 2.3052726 TestAcc: 0.08\n",
      "TrainLoss: 2.3010657 TrainAcc: 0.08 TestLoss: 2.2994566 TestAcc: 0.16\n",
      "TrainLoss: 2.3039696 TrainAcc: 0.08 TestLoss: 2.3012192 TestAcc: 0.12\n",
      "TrainLoss: 2.305593 TrainAcc: 0.06 TestLoss: 2.3042698 TestAcc: 0.06\n",
      "TrainLoss: 2.306566 TrainAcc: 0.08 TestLoss: 2.3064387 TestAcc: 0.08\n",
      "TrainLoss: 2.3035288 TrainAcc: 0.14 TestLoss: 2.301125 TestAcc: 0.14\n",
      "TrainLoss: 2.301346 TrainAcc: 0.06 TestLoss: 2.3028562 TestAcc: 0.08\n",
      "TrainLoss: 2.3025143 TrainAcc: 0.1 TestLoss: 2.3036282 TestAcc: 0.12\n",
      "TrainLoss: 2.2991707 TrainAcc: 0.18 TestLoss: 2.3040965 TestAcc: 0.08\n",
      "TrainLoss: 2.303466 TrainAcc: 0.08 TestLoss: 2.303958 TestAcc: 0.06\n",
      "TrainLoss: 2.2999032 TrainAcc: 0.16 TestLoss: 2.3019552 TestAcc: 0.16\n",
      "TrainLoss: 2.300561 TrainAcc: 0.14 TestLoss: 2.3058088 TestAcc: 0.12\n",
      "TrainLoss: 2.3039925 TrainAcc: 0.08 TestLoss: 2.3048725 TestAcc: 0.08\n",
      "TrainLoss: 2.3006647 TrainAcc: 0.18 TestLoss: 2.305977 TestAcc: 0.08\n",
      "TrainLoss: 2.3007798 TrainAcc: 0.12 TestLoss: 2.305978 TestAcc: 0.04\n",
      "TrainLoss: 2.3044655 TrainAcc: 0.06 TestLoss: 2.2992175 TestAcc: 0.14\n",
      "TrainLoss: 2.3007193 TrainAcc: 0.12 TestLoss: 2.3043683 TestAcc: 0.06\n",
      "TrainLoss: 2.3059337 TrainAcc: 0.08 TestLoss: 2.30649 TestAcc: 0.04\n",
      "TrainLoss: 2.307868 TrainAcc: 0.02 TestLoss: 2.30302 TestAcc: 0.08\n",
      "TrainLoss: 2.3014672 TrainAcc: 0.12 TestLoss: 2.300238 TestAcc: 0.12\n",
      "TrainLoss: 2.3041902 TrainAcc: 0.1 TestLoss: 2.3043222 TestAcc: 0.04\n",
      "TrainLoss: 2.3018136 TrainAcc: 0.08 TestLoss: 2.3033035 TestAcc: 0.12\n",
      "TrainLoss: 2.3018737 TrainAcc: 0.08 TestLoss: 2.3060503 TestAcc: 0.1\n",
      "TrainLoss: 2.3043535 TrainAcc: 0.08 TestLoss: 2.302338 TestAcc: 0.14\n",
      "TrainLoss: 2.303383 TrainAcc: 0.1 TestLoss: 2.30181 TestAcc: 0.16\n",
      "TrainLoss: 2.3032012 TrainAcc: 0.08 TestLoss: 2.3030164 TestAcc: 0.08\n",
      "TrainLoss: 2.3026116 TrainAcc: 0.12 TestLoss: 2.3027592 TestAcc: 0.1\n",
      "TrainLoss: 2.3033533 TrainAcc: 0.04 TestLoss: 2.305172 TestAcc: 0.1\n",
      "TrainLoss: 2.3027983 TrainAcc: 0.08 TestLoss: 2.3039277 TestAcc: 0.1\n",
      "TrainLoss: 2.3027408 TrainAcc: 0.12 TestLoss: 2.3028476 TestAcc: 0.08\n",
      "TrainLoss: 2.3027833 TrainAcc: 0.1 TestLoss: 2.3025799 TestAcc: 0.08\n",
      "TrainLoss: 2.301129 TrainAcc: 0.06 TestLoss: 2.3010197 TestAcc: 0.12\n",
      "TrainLoss: 2.302104 TrainAcc: 0.12 TestLoss: 2.305964 TestAcc: 0.0\n",
      "TrainLoss: 2.3012207 TrainAcc: 0.14 TestLoss: 2.3025544 TestAcc: 0.1\n",
      "TrainLoss: 2.3033566 TrainAcc: 0.12 TestLoss: 2.3013172 TestAcc: 0.1\n",
      "TrainLoss: 2.2998185 TrainAcc: 0.12 TestLoss: 2.3082972 TestAcc: 0.06\n",
      "TrainLoss: 2.3058386 TrainAcc: 0.06 TestLoss: 2.3020988 TestAcc: 0.08\n",
      "TrainLoss: 2.3043962 TrainAcc: 0.14 TestLoss: 2.3059764 TestAcc: 0.02\n",
      "TrainLoss: 2.302705 TrainAcc: 0.08 TestLoss: 2.2996132 TestAcc: 0.12\n",
      "TrainLoss: 2.304734 TrainAcc: 0.08 TestLoss: 2.3040059 TestAcc: 0.08\n",
      "TrainLoss: 2.3039913 TrainAcc: 0.06 TestLoss: 2.3007333 TestAcc: 0.14\n",
      "TrainLoss: 2.3041062 TrainAcc: 0.1 TestLoss: 2.3017912 TestAcc: 0.1\n",
      "TrainLoss: 2.2995954 TrainAcc: 0.12 TestLoss: 2.3052676 TestAcc: 0.06\n",
      "TrainLoss: 2.3027074 TrainAcc: 0.14 TestLoss: 2.303349 TestAcc: 0.08\n",
      "TrainLoss: 2.3036242 TrainAcc: 0.08 TestLoss: 2.3022892 TestAcc: 0.12\n",
      "TrainLoss: 2.302941 TrainAcc: 0.06 TestLoss: 2.302719 TestAcc: 0.12\n",
      "TrainLoss: 2.3029945 TrainAcc: 0.08 TestLoss: 2.2994816 TestAcc: 0.16\n",
      "TrainLoss: 2.302918 TrainAcc: 0.12 TestLoss: 2.302365 TestAcc: 0.14\n",
      "TrainLoss: 2.3048244 TrainAcc: 0.06 TestLoss: 2.3042774 TestAcc: 0.08\n",
      "TrainLoss: 2.3016028 TrainAcc: 0.12 TestLoss: 2.3033538 TestAcc: 0.04\n",
      "TrainLoss: 2.3032815 TrainAcc: 0.08 TestLoss: 2.3020513 TestAcc: 0.16\n",
      "TrainLoss: 2.3024817 TrainAcc: 0.12 TestLoss: 2.2984805 TestAcc: 0.18\n",
      "TrainLoss: 2.3043945 TrainAcc: 0.06 TestLoss: 2.3030813 TestAcc: 0.12\n",
      "TrainLoss: 2.3023782 TrainAcc: 0.12 TestLoss: 2.300116 TestAcc: 0.18\n",
      "TrainLoss: 2.301063 TrainAcc: 0.12 TestLoss: 2.300733 TestAcc: 0.16\n",
      "TrainLoss: 2.3009765 TrainAcc: 0.08 TestLoss: 2.3026435 TestAcc: 0.12\n",
      "TrainLoss: 2.3037717 TrainAcc: 0.08 TestLoss: 2.302724 TestAcc: 0.08\n",
      "TrainLoss: 2.301736 TrainAcc: 0.1 TestLoss: 2.3020682 TestAcc: 0.14\n",
      "TrainLoss: 2.3034098 TrainAcc: 0.12 TestLoss: 2.3058405 TestAcc: 0.08\n",
      "TrainLoss: 2.3045492 TrainAcc: 0.1 TestLoss: 2.30549 TestAcc: 0.06\n",
      "TrainLoss: 2.303072 TrainAcc: 0.1 TestLoss: 2.3042083 TestAcc: 0.1\n",
      "TrainLoss: 2.30141 TrainAcc: 0.1 TestLoss: 2.303005 TestAcc: 0.06\n",
      "TrainLoss: 2.2997074 TrainAcc: 0.22 TestLoss: 2.3020566 TestAcc: 0.12\n",
      "TrainLoss: 2.3014517 TrainAcc: 0.08 TestLoss: 2.3021111 TestAcc: 0.14\n",
      "TrainLoss: 2.3028271 TrainAcc: 0.08 TestLoss: 2.303412 TestAcc: 0.1\n",
      "TrainLoss: 2.3014414 TrainAcc: 0.12 TestLoss: 2.3031845 TestAcc: 0.06\n",
      "TrainLoss: 2.3016796 TrainAcc: 0.1 TestLoss: 2.3037243 TestAcc: 0.08\n",
      "TrainLoss: 2.3016055 TrainAcc: 0.12 TestLoss: 2.3026977 TestAcc: 0.1\n",
      "TrainLoss: 2.3069124 TrainAcc: 0.06 TestLoss: 2.3012893 TestAcc: 0.1\n",
      "TrainLoss: 2.3028083 TrainAcc: 0.06 TestLoss: 2.303792 TestAcc: 0.06\n",
      "TrainLoss: 2.2996676 TrainAcc: 0.12 TestLoss: 2.3019583 TestAcc: 0.08\n",
      "TrainLoss: 2.3052363 TrainAcc: 0.1 TestLoss: 2.3020968 TestAcc: 0.08\n",
      "TrainLoss: 2.3042862 TrainAcc: 0.12 TestLoss: 2.2999372 TestAcc: 0.1\n",
      "TrainLoss: 2.305713 TrainAcc: 0.06 TestLoss: 2.300925 TestAcc: 0.12\n",
      "TrainLoss: 2.3026261 TrainAcc: 0.08 TestLoss: 2.302905 TestAcc: 0.06\n",
      "TrainLoss: 2.3016484 TrainAcc: 0.08 TestLoss: 2.3029249 TestAcc: 0.08\n",
      "TrainLoss: 2.3028455 TrainAcc: 0.08 TestLoss: 2.3030787 TestAcc: 0.14\n",
      "TrainLoss: 2.2987564 TrainAcc: 0.08 TestLoss: 2.2988634 TestAcc: 0.18\n",
      "TrainLoss: 2.3017797 TrainAcc: 0.14 TestLoss: 2.3024492 TestAcc: 0.08\n",
      "TrainLoss: 2.3033526 TrainAcc: 0.04 TestLoss: 2.300437 TestAcc: 0.14\n",
      "TrainLoss: 2.3050294 TrainAcc: 0.02 TestLoss: 2.302151 TestAcc: 0.18\n",
      "TrainLoss: 2.3018394 TrainAcc: 0.06 TestLoss: 2.3028178 TestAcc: 0.14\n",
      "TrainLoss: 2.3044045 TrainAcc: 0.08 TestLoss: 2.3007486 TestAcc: 0.14\n",
      "TrainLoss: 2.3008888 TrainAcc: 0.14 TestLoss: 2.3010795 TestAcc: 0.08\n",
      "TrainLoss: 2.2986903 TrainAcc: 0.14 TestLoss: 2.306203 TestAcc: 0.02\n",
      "TrainLoss: 2.304338 TrainAcc: 0.04 TestLoss: 2.304471 TestAcc: 0.12\n",
      "TrainLoss: 2.3001537 TrainAcc: 0.16 TestLoss: 2.3037174 TestAcc: 0.04\n",
      "TrainLoss: 2.300658 TrainAcc: 0.16 TestLoss: 2.3018486 TestAcc: 0.12\n",
      "TrainLoss: 2.2991369 TrainAcc: 0.12 TestLoss: 2.3012977 TestAcc: 0.14\n",
      "TrainLoss: 2.304781 TrainAcc: 0.02 TestLoss: 2.299629 TestAcc: 0.22\n",
      "TrainLoss: 2.3044064 TrainAcc: 0.14 TestLoss: 2.3049781 TestAcc: 0.1\n",
      "TrainLoss: 2.3024156 TrainAcc: 0.06 TestLoss: 2.302551 TestAcc: 0.14\n",
      "TrainLoss: 2.3028905 TrainAcc: 0.08 TestLoss: 2.306511 TestAcc: 0.04\n",
      "TrainLoss: 2.3055155 TrainAcc: 0.12 TestLoss: 2.3050005 TestAcc: 0.08\n",
      "TrainLoss: 2.3028438 TrainAcc: 0.08 TestLoss: 2.304326 TestAcc: 0.08\n",
      "TrainLoss: 2.3032026 TrainAcc: 0.08 TestLoss: 2.3010867 TestAcc: 0.08\n",
      "TrainLoss: 2.3006182 TrainAcc: 0.14 TestLoss: 2.3037646 TestAcc: 0.02\n",
      "TrainLoss: 2.3037763 TrainAcc: 0.12 TestLoss: 2.304151 TestAcc: 0.1\n",
      "TrainLoss: 2.3020823 TrainAcc: 0.1 TestLoss: 2.30087 TestAcc: 0.14\n",
      "TrainLoss: 2.30223 TrainAcc: 0.14 TestLoss: 2.301741 TestAcc: 0.1\n",
      "TrainLoss: 2.3001654 TrainAcc: 0.14 TestLoss: 2.3026998 TestAcc: 0.02\n",
      "TrainLoss: 2.3047707 TrainAcc: 0.1 TestLoss: 2.3021429 TestAcc: 0.04\n",
      "TrainLoss: 2.3024735 TrainAcc: 0.1 TestLoss: 2.3046453 TestAcc: 0.06\n",
      "TrainLoss: 2.2989745 TrainAcc: 0.18 TestLoss: 2.301719 TestAcc: 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.2963226 TrainAcc: 0.18 TestLoss: 2.303818 TestAcc: 0.08\n",
      "TrainLoss: 2.3009408 TrainAcc: 0.08 TestLoss: 2.302812 TestAcc: 0.12\n",
      "TrainLoss: 2.3034556 TrainAcc: 0.1 TestLoss: 2.3033082 TestAcc: 0.08\n",
      "TrainLoss: 2.3028548 TrainAcc: 0.06 TestLoss: 2.3055835 TestAcc: 0.1\n",
      "TrainLoss: 2.3016176 TrainAcc: 0.12 TestLoss: 2.3029094 TestAcc: 0.08\n",
      "TrainLoss: 2.303773 TrainAcc: 0.08 TestLoss: 2.3048391 TestAcc: 0.04\n",
      "TrainLoss: 2.3043516 TrainAcc: 0.08 TestLoss: 2.3026009 TestAcc: 0.08\n",
      "TrainLoss: 2.3017623 TrainAcc: 0.12 TestLoss: 2.3028603 TestAcc: 0.06\n",
      "TrainLoss: 2.3014748 TrainAcc: 0.08 TestLoss: 2.3031943 TestAcc: 0.14\n",
      "TrainLoss: 2.3033674 TrainAcc: 0.1 TestLoss: 2.303956 TestAcc: 0.08\n",
      "TrainLoss: 2.3024511 TrainAcc: 0.14 TestLoss: 2.3057702 TestAcc: 0.04\n",
      "TrainLoss: 2.302988 TrainAcc: 0.08 TestLoss: 2.3021374 TestAcc: 0.06\n",
      "TrainLoss: 2.301902 TrainAcc: 0.12 TestLoss: 2.3026545 TestAcc: 0.06\n",
      "TrainLoss: 2.3023179 TrainAcc: 0.08 TestLoss: 2.303395 TestAcc: 0.1\n",
      "TrainLoss: 2.3020327 TrainAcc: 0.08 TestLoss: 2.3030894 TestAcc: 0.06\n",
      "TrainLoss: 2.3042567 TrainAcc: 0.1 TestLoss: 2.3079565 TestAcc: 0.0\n",
      "TrainLoss: 2.3042727 TrainAcc: 0.04 TestLoss: 2.3017032 TestAcc: 0.08\n",
      "TrainLoss: 2.30178 TrainAcc: 0.1 TestLoss: 2.3039138 TestAcc: 0.12\n",
      "TrainLoss: 2.3034885 TrainAcc: 0.02 TestLoss: 2.3013325 TestAcc: 0.1\n",
      "TrainLoss: 2.3029222 TrainAcc: 0.1 TestLoss: 2.3039255 TestAcc: 0.16\n",
      "TrainLoss: 2.3062947 TrainAcc: 0.02 TestLoss: 2.3043709 TestAcc: 0.02\n",
      "TrainLoss: 2.3018675 TrainAcc: 0.1 TestLoss: 2.3015358 TestAcc: 0.14\n",
      "TrainLoss: 2.3023465 TrainAcc: 0.08 TestLoss: 2.302489 TestAcc: 0.08\n",
      "TrainLoss: 2.2995307 TrainAcc: 0.14 TestLoss: 2.3033037 TestAcc: 0.08\n",
      "TrainLoss: 2.3038092 TrainAcc: 0.06 TestLoss: 2.3045723 TestAcc: 0.04\n",
      "TrainLoss: 2.3017979 TrainAcc: 0.14 TestLoss: 2.3065255 TestAcc: 0.08\n",
      "TrainLoss: 2.3022492 TrainAcc: 0.12 TestLoss: 2.3069623 TestAcc: 0.08\n",
      "TrainLoss: 2.3058486 TrainAcc: 0.06 TestLoss: 2.3023171 TestAcc: 0.14\n",
      "TrainLoss: 2.298429 TrainAcc: 0.16 TestLoss: 2.3022199 TestAcc: 0.06\n",
      "TrainLoss: 2.3020203 TrainAcc: 0.1 TestLoss: 2.305239 TestAcc: 0.04\n",
      "TrainLoss: 2.3052125 TrainAcc: 0.06 TestLoss: 2.3029063 TestAcc: 0.1\n",
      "TrainLoss: 2.30094 TrainAcc: 0.18 TestLoss: 2.3016768 TestAcc: 0.12\n",
      "TrainLoss: 2.3023014 TrainAcc: 0.06 TestLoss: 2.3023295 TestAcc: 0.06\n",
      "TrainLoss: 2.300189 TrainAcc: 0.14 TestLoss: 2.3011823 TestAcc: 0.16\n",
      "TrainLoss: 2.3010705 TrainAcc: 0.18 TestLoss: 2.301025 TestAcc: 0.12\n",
      "TrainLoss: 2.3022847 TrainAcc: 0.12 TestLoss: 2.3020103 TestAcc: 0.04\n",
      "TrainLoss: 2.2984242 TrainAcc: 0.18 TestLoss: 2.305872 TestAcc: 0.06\n",
      "TrainLoss: 2.3023105 TrainAcc: 0.14 TestLoss: 2.3032527 TestAcc: 0.04\n",
      "TrainLoss: 2.3025625 TrainAcc: 0.04 TestLoss: 2.3040252 TestAcc: 0.06\n",
      "TrainLoss: 2.3016732 TrainAcc: 0.1 TestLoss: 2.3027244 TestAcc: 0.12\n",
      "TrainLoss: 2.30497 TrainAcc: 0.1 TestLoss: 2.3053486 TestAcc: 0.04\n",
      "TrainLoss: 2.3031328 TrainAcc: 0.14 TestLoss: 2.3013215 TestAcc: 0.1\n",
      "TrainLoss: 2.3019478 TrainAcc: 0.16 TestLoss: 2.3038049 TestAcc: 0.12\n",
      "TrainLoss: 2.3021057 TrainAcc: 0.14 TestLoss: 2.3036253 TestAcc: 0.08\n",
      "TrainLoss: 2.3013327 TrainAcc: 0.06 TestLoss: 2.3037348 TestAcc: 0.1\n",
      "TrainLoss: 2.2996628 TrainAcc: 0.16 TestLoss: 2.3041618 TestAcc: 0.08\n",
      "TrainLoss: 2.3023498 TrainAcc: 0.16 TestLoss: 2.3010433 TestAcc: 0.08\n",
      "TrainLoss: 2.3040285 TrainAcc: 0.12 TestLoss: 2.3039367 TestAcc: 0.12\n",
      "TrainLoss: 2.301992 TrainAcc: 0.12 TestLoss: 2.3018417 TestAcc: 0.02\n",
      "TrainLoss: 2.302729 TrainAcc: 0.16 TestLoss: 2.3042693 TestAcc: 0.06\n",
      "TrainLoss: 2.3003821 TrainAcc: 0.16 TestLoss: 2.3041472 TestAcc: 0.08\n",
      "TrainLoss: 2.3045743 TrainAcc: 0.08 TestLoss: 2.3021393 TestAcc: 0.1\n",
      "TrainLoss: 2.301489 TrainAcc: 0.1 TestLoss: 2.303905 TestAcc: 0.08\n",
      "TrainLoss: 2.3061152 TrainAcc: 0.1 TestLoss: 2.3081872 TestAcc: 0.08\n",
      "TrainLoss: 2.302722 TrainAcc: 0.08 TestLoss: 2.3072906 TestAcc: 0.04\n",
      "TrainLoss: 2.3000557 TrainAcc: 0.14 TestLoss: 2.3032594 TestAcc: 0.12\n",
      "TrainLoss: 2.301433 TrainAcc: 0.12 TestLoss: 2.3024168 TestAcc: 0.12\n",
      "TrainLoss: 2.3027391 TrainAcc: 0.12 TestLoss: 2.3002894 TestAcc: 0.14\n",
      "TrainLoss: 2.3015537 TrainAcc: 0.14 TestLoss: 2.303684 TestAcc: 0.06\n",
      "TrainLoss: 2.3025045 TrainAcc: 0.1 TestLoss: 2.3028336 TestAcc: 0.04\n",
      "TrainLoss: 2.30169 TrainAcc: 0.16 TestLoss: 2.3054097 TestAcc: 0.06\n",
      "TrainLoss: 2.304328 TrainAcc: 0.08 TestLoss: 2.3021314 TestAcc: 0.06\n",
      "TrainLoss: 2.3031416 TrainAcc: 0.12 TestLoss: 2.3018486 TestAcc: 0.16\n",
      "TrainLoss: 2.3027601 TrainAcc: 0.02 TestLoss: 2.301641 TestAcc: 0.12\n",
      "TrainLoss: 2.3039396 TrainAcc: 0.1 TestLoss: 2.304898 TestAcc: 0.06\n",
      "TrainLoss: 2.3039398 TrainAcc: 0.04 TestLoss: 2.302991 TestAcc: 0.1\n",
      "TrainLoss: 2.3068666 TrainAcc: 0.02 TestLoss: 2.3006463 TestAcc: 0.06\n",
      "TrainLoss: 2.2998872 TrainAcc: 0.16 TestLoss: 2.3043168 TestAcc: 0.06\n",
      "TrainLoss: 2.305766 TrainAcc: 0.04 TestLoss: 2.3043852 TestAcc: 0.06\n",
      "TrainLoss: 2.306236 TrainAcc: 0.04 TestLoss: 2.3045177 TestAcc: 0.08\n",
      "TrainLoss: 2.303592 TrainAcc: 0.1 TestLoss: 2.3033242 TestAcc: 0.06\n",
      "TrainLoss: 2.303521 TrainAcc: 0.08 TestLoss: 2.306655 TestAcc: 0.04\n",
      "TrainLoss: 2.3039722 TrainAcc: 0.04 TestLoss: 2.302561 TestAcc: 0.1\n",
      "TrainLoss: 2.3027723 TrainAcc: 0.08 TestLoss: 2.306575 TestAcc: 0.04\n",
      "TrainLoss: 2.301559 TrainAcc: 0.16 TestLoss: 2.301984 TestAcc: 0.1\n",
      "TrainLoss: 2.3013859 TrainAcc: 0.12 TestLoss: 2.3040698 TestAcc: 0.12\n",
      "TrainLoss: 2.3042998 TrainAcc: 0.06 TestLoss: 2.3073351 TestAcc: 0.04\n",
      "TrainLoss: 2.302267 TrainAcc: 0.04 TestLoss: 2.3072155 TestAcc: 0.02\n",
      "TrainLoss: 2.301309 TrainAcc: 0.14 TestLoss: 2.3058572 TestAcc: 0.08\n",
      "TrainLoss: 2.3019829 TrainAcc: 0.18 TestLoss: 2.2995481 TestAcc: 0.16\n",
      "TrainLoss: 2.3050368 TrainAcc: 0.06 TestLoss: 2.3072 TestAcc: 0.02\n",
      "TrainLoss: 2.303539 TrainAcc: 0.08 TestLoss: 2.3026972 TestAcc: 0.02\n",
      "TrainLoss: 2.304353 TrainAcc: 0.06 TestLoss: 2.3030908 TestAcc: 0.08\n",
      "TrainLoss: 2.3029428 TrainAcc: 0.06 TestLoss: 2.299841 TestAcc: 0.22\n",
      "TrainLoss: 2.3051596 TrainAcc: 0.06 TestLoss: 2.3028915 TestAcc: 0.1\n",
      "TrainLoss: 2.3046942 TrainAcc: 0.1 TestLoss: 2.2992926 TestAcc: 0.1\n",
      "TrainLoss: 2.3052032 TrainAcc: 0.02 TestLoss: 2.3049836 TestAcc: 0.08\n",
      "TrainLoss: 2.3012087 TrainAcc: 0.1 TestLoss: 2.3038058 TestAcc: 0.12\n",
      "TrainLoss: 2.3043575 TrainAcc: 0.12 TestLoss: 2.3037798 TestAcc: 0.04\n",
      "TrainLoss: 2.3018403 TrainAcc: 0.08 TestLoss: 2.3004088 TestAcc: 0.16\n",
      "TrainLoss: 2.3021102 TrainAcc: 0.1 TestLoss: 2.3024085 TestAcc: 0.12\n",
      "TrainLoss: 2.3012536 TrainAcc: 0.18 TestLoss: 2.3023455 TestAcc: 0.14\n",
      "TrainLoss: 2.3030686 TrainAcc: 0.02 TestLoss: 2.3034377 TestAcc: 0.12\n",
      "TrainLoss: 2.3022335 TrainAcc: 0.08 TestLoss: 2.3018954 TestAcc: 0.12\n",
      "TrainLoss: 2.302543 TrainAcc: 0.08 TestLoss: 2.3027914 TestAcc: 0.12\n",
      "TrainLoss: 2.3020437 TrainAcc: 0.12 TestLoss: 2.29994 TestAcc: 0.14\n",
      "TrainLoss: 2.300969 TrainAcc: 0.1 TestLoss: 2.2984207 TestAcc: 0.16\n",
      "TrainLoss: 2.3042307 TrainAcc: 0.08 TestLoss: 2.3015063 TestAcc: 0.14\n",
      "TrainLoss: 2.3034487 TrainAcc: 0.12 TestLoss: 2.302556 TestAcc: 0.08\n",
      "TrainLoss: 2.3023026 TrainAcc: 0.08 TestLoss: 2.3039837 TestAcc: 0.06\n",
      "TrainLoss: 2.3040774 TrainAcc: 0.14 TestLoss: 2.3024077 TestAcc: 0.12\n",
      "TrainLoss: 2.30366 TrainAcc: 0.1 TestLoss: 2.3034182 TestAcc: 0.1\n",
      "TrainLoss: 2.3020992 TrainAcc: 0.06 TestLoss: 2.2999816 TestAcc: 0.22\n",
      "TrainLoss: 2.3026001 TrainAcc: 0.06 TestLoss: 2.3026397 TestAcc: 0.12\n",
      "TrainLoss: 2.304425 TrainAcc: 0.06 TestLoss: 2.3014681 TestAcc: 0.1\n",
      "TrainLoss: 2.301296 TrainAcc: 0.16 TestLoss: 2.3025494 TestAcc: 0.14\n",
      "TrainLoss: 2.303891 TrainAcc: 0.08 TestLoss: 2.304211 TestAcc: 0.06\n",
      "TrainLoss: 2.3023689 TrainAcc: 0.08 TestLoss: 2.3051991 TestAcc: 0.06\n",
      "TrainLoss: 2.3025904 TrainAcc: 0.1 TestLoss: 2.3035347 TestAcc: 0.1\n",
      "TrainLoss: 2.300788 TrainAcc: 0.1 TestLoss: 2.3015156 TestAcc: 0.02\n",
      "TrainLoss: 2.3041832 TrainAcc: 0.12 TestLoss: 2.3007164 TestAcc: 0.12\n",
      "TrainLoss: 2.305119 TrainAcc: 0.06 TestLoss: 2.3010046 TestAcc: 0.14\n",
      "TrainLoss: 2.2979822 TrainAcc: 0.28 TestLoss: 2.3018432 TestAcc: 0.1\n",
      "TrainLoss: 2.3003836 TrainAcc: 0.16 TestLoss: 2.3034515 TestAcc: 0.12\n",
      "TrainLoss: 2.2997596 TrainAcc: 0.16 TestLoss: 2.3032844 TestAcc: 0.14\n",
      "TrainLoss: 2.302553 TrainAcc: 0.1 TestLoss: 2.3024805 TestAcc: 0.12\n",
      "TrainLoss: 2.3002458 TrainAcc: 0.16 TestLoss: 2.3032763 TestAcc: 0.06\n",
      "TrainLoss: 2.3030334 TrainAcc: 0.1 TestLoss: 2.3058212 TestAcc: 0.02\n",
      "TrainLoss: 2.3014839 TrainAcc: 0.1 TestLoss: 2.3015897 TestAcc: 0.14\n",
      "TrainLoss: 2.304574 TrainAcc: 0.06 TestLoss: 2.30387 TestAcc: 0.08\n",
      "TrainLoss: 2.3012278 TrainAcc: 0.1 TestLoss: 2.303145 TestAcc: 0.1\n",
      "TrainLoss: 2.304097 TrainAcc: 0.04 TestLoss: 2.302232 TestAcc: 0.14\n",
      "TrainLoss: 2.302461 TrainAcc: 0.08 TestLoss: 2.3034866 TestAcc: 0.02\n",
      "TrainLoss: 2.3013744 TrainAcc: 0.12 TestLoss: 2.301746 TestAcc: 0.1\n",
      "TrainLoss: 2.3024862 TrainAcc: 0.08 TestLoss: 2.3015614 TestAcc: 0.08\n",
      "TrainLoss: 2.3007498 TrainAcc: 0.2 TestLoss: 2.3002932 TestAcc: 0.14\n",
      "TrainLoss: 2.3015444 TrainAcc: 0.12 TestLoss: 2.3009102 TestAcc: 0.08\n",
      "TrainLoss: 2.3033035 TrainAcc: 0.06 TestLoss: 2.3047137 TestAcc: 0.06\n",
      "TrainLoss: 2.30149 TrainAcc: 0.12 TestLoss: 2.3002758 TestAcc: 0.16\n",
      "TrainLoss: 2.3037906 TrainAcc: 0.1 TestLoss: 2.3020186 TestAcc: 0.1\n",
      "TrainLoss: 2.3027544 TrainAcc: 0.1 TestLoss: 2.3019707 TestAcc: 0.12\n",
      "TrainLoss: 2.300025 TrainAcc: 0.16 TestLoss: 2.3041472 TestAcc: 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3030508 TrainAcc: 0.04 TestLoss: 2.3032982 TestAcc: 0.06\n",
      "TrainLoss: 2.3028631 TrainAcc: 0.06 TestLoss: 2.3010278 TestAcc: 0.14\n",
      "TrainLoss: 2.3000555 TrainAcc: 0.16 TestLoss: 2.3030603 TestAcc: 0.14\n",
      "TrainLoss: 2.3028715 TrainAcc: 0.08 TestLoss: 2.3023582 TestAcc: 0.08\n",
      "TrainLoss: 2.29795 TrainAcc: 0.18 TestLoss: 2.3022306 TestAcc: 0.1\n",
      "TrainLoss: 2.3045735 TrainAcc: 0.06 TestLoss: 2.3029275 TestAcc: 0.1\n",
      "TrainLoss: 2.299163 TrainAcc: 0.18 TestLoss: 2.3020616 TestAcc: 0.12\n",
      "TrainLoss: 2.2997847 TrainAcc: 0.08 TestLoss: 2.2996435 TestAcc: 0.12\n",
      "TrainLoss: 2.3036563 TrainAcc: 0.12 TestLoss: 2.3031178 TestAcc: 0.06\n",
      "TrainLoss: 2.300756 TrainAcc: 0.12 TestLoss: 2.3035727 TestAcc: 0.06\n",
      "TrainLoss: 2.3012352 TrainAcc: 0.08 TestLoss: 2.3047333 TestAcc: 0.08\n",
      "TrainLoss: 2.3016906 TrainAcc: 0.06 TestLoss: 2.302776 TestAcc: 0.12\n",
      "TrainLoss: 2.302157 TrainAcc: 0.1 TestLoss: 2.3042994 TestAcc: 0.08\n",
      "TrainLoss: 2.3001812 TrainAcc: 0.16 TestLoss: 2.3049178 TestAcc: 0.12\n",
      "TrainLoss: 2.3036854 TrainAcc: 0.1 TestLoss: 2.3017004 TestAcc: 0.12\n",
      "TrainLoss: 2.300311 TrainAcc: 0.14 TestLoss: 2.301274 TestAcc: 0.08\n",
      "TrainLoss: 2.303153 TrainAcc: 0.08 TestLoss: 2.3043296 TestAcc: 0.08\n",
      "TrainLoss: 2.2987242 TrainAcc: 0.2 TestLoss: 2.3044074 TestAcc: 0.04\n",
      "TrainLoss: 2.3030267 TrainAcc: 0.12 TestLoss: 2.2994468 TestAcc: 0.12\n",
      "TrainLoss: 2.3017466 TrainAcc: 0.08 TestLoss: 2.303942 TestAcc: 0.1\n",
      "TrainLoss: 2.2994552 TrainAcc: 0.14 TestLoss: 2.3032594 TestAcc: 0.04\n",
      "TrainLoss: 2.3010144 TrainAcc: 0.1 TestLoss: 2.302068 TestAcc: 0.14\n",
      "TrainLoss: 2.3028033 TrainAcc: 0.1 TestLoss: 2.3023903 TestAcc: 0.14\n",
      "TrainLoss: 2.3018405 TrainAcc: 0.08 TestLoss: 2.3028936 TestAcc: 0.06\n",
      "TrainLoss: 2.3043282 TrainAcc: 0.12 TestLoss: 2.3057325 TestAcc: 0.06\n",
      "TrainLoss: 2.3026843 TrainAcc: 0.06 TestLoss: 2.3021317 TestAcc: 0.1\n",
      "TrainLoss: 2.3043745 TrainAcc: 0.04 TestLoss: 2.3022287 TestAcc: 0.02\n",
      "TrainLoss: 2.3030663 TrainAcc: 0.08 TestLoss: 2.3057382 TestAcc: 0.04\n",
      "TrainLoss: 2.3034737 TrainAcc: 0.04 TestLoss: 2.3037152 TestAcc: 0.08\n",
      "TrainLoss: 2.3024163 TrainAcc: 0.08 TestLoss: 2.304383 TestAcc: 0.1\n",
      "TrainLoss: 2.2998857 TrainAcc: 0.1 TestLoss: 2.3052084 TestAcc: 0.06\n",
      "TrainLoss: 2.304931 TrainAcc: 0.02 TestLoss: 2.3022878 TestAcc: 0.06\n",
      "TrainLoss: 2.2993767 TrainAcc: 0.12 TestLoss: 2.3002145 TestAcc: 0.08\n",
      "TrainLoss: 2.3039825 TrainAcc: 0.04 TestLoss: 2.3008711 TestAcc: 0.14\n",
      "TrainLoss: 2.3017101 TrainAcc: 0.06 TestLoss: 2.3050988 TestAcc: 0.1\n",
      "TrainLoss: 2.3001902 TrainAcc: 0.06 TestLoss: 2.3031592 TestAcc: 0.1\n",
      "TrainLoss: 2.3008716 TrainAcc: 0.1 TestLoss: 2.3037505 TestAcc: 0.1\n",
      "TrainLoss: 2.3045416 TrainAcc: 0.1 TestLoss: 2.3053973 TestAcc: 0.12\n",
      "TrainLoss: 2.3062234 TrainAcc: 0.1 TestLoss: 2.302276 TestAcc: 0.14\n",
      "TrainLoss: 2.3015578 TrainAcc: 0.14 TestLoss: 2.3025174 TestAcc: 0.06\n",
      "TrainLoss: 2.3037717 TrainAcc: 0.04 TestLoss: 2.3049445 TestAcc: 0.08\n",
      "TrainLoss: 2.3012707 TrainAcc: 0.1 TestLoss: 2.2996428 TestAcc: 0.12\n",
      "TrainLoss: 2.301809 TrainAcc: 0.1 TestLoss: 2.30659 TestAcc: 0.04\n",
      "TrainLoss: 2.3042028 TrainAcc: 0.06 TestLoss: 2.301699 TestAcc: 0.2\n",
      "TrainLoss: 2.3054914 TrainAcc: 0.06 TestLoss: 2.3012066 TestAcc: 0.1\n",
      "TrainLoss: 2.2996457 TrainAcc: 0.18 TestLoss: 2.3029811 TestAcc: 0.06\n",
      "TrainLoss: 2.305063 TrainAcc: 0.1 TestLoss: 2.3051255 TestAcc: 0.08\n",
      "TrainLoss: 2.3013327 TrainAcc: 0.1 TestLoss: 2.299649 TestAcc: 0.04\n",
      "TrainLoss: 2.301387 TrainAcc: 0.16 TestLoss: 2.30739 TestAcc: 0.08\n",
      "TrainLoss: 2.300211 TrainAcc: 0.08 TestLoss: 2.304579 TestAcc: 0.02\n",
      "TrainLoss: 2.3032758 TrainAcc: 0.06 TestLoss: 2.3008964 TestAcc: 0.14\n",
      "TrainLoss: 2.302767 TrainAcc: 0.08 TestLoss: 2.2996042 TestAcc: 0.14\n",
      "TrainLoss: 2.3040671 TrainAcc: 0.08 TestLoss: 2.303391 TestAcc: 0.06\n",
      "TrainLoss: 2.2998066 TrainAcc: 0.14 TestLoss: 2.302495 TestAcc: 0.04\n",
      "TrainLoss: 2.29863 TrainAcc: 0.12 TestLoss: 2.302944 TestAcc: 0.12\n",
      "TrainLoss: 2.3042796 TrainAcc: 0.1 TestLoss: 2.3017292 TestAcc: 0.1\n",
      "TrainLoss: 2.3020434 TrainAcc: 0.14 TestLoss: 2.3027205 TestAcc: 0.08\n",
      "TrainLoss: 2.304238 TrainAcc: 0.18 TestLoss: 2.3028898 TestAcc: 0.1\n",
      "TrainLoss: 2.3024492 TrainAcc: 0.16 TestLoss: 2.3001878 TestAcc: 0.12\n",
      "TrainLoss: 2.3033977 TrainAcc: 0.1 TestLoss: 2.2996943 TestAcc: 0.24\n",
      "TrainLoss: 2.2984924 TrainAcc: 0.18 TestLoss: 2.3074503 TestAcc: 0.0\n",
      "TrainLoss: 2.3013172 TrainAcc: 0.08 TestLoss: 2.3046331 TestAcc: 0.12\n",
      "TrainLoss: 2.3024735 TrainAcc: 0.0 TestLoss: 2.3016086 TestAcc: 0.16\n",
      "TrainLoss: 2.304057 TrainAcc: 0.06 TestLoss: 2.3015602 TestAcc: 0.06\n",
      "TrainLoss: 2.3011565 TrainAcc: 0.16 TestLoss: 2.3050988 TestAcc: 0.06\n",
      "TrainLoss: 2.3023436 TrainAcc: 0.12 TestLoss: 2.3048072 TestAcc: 0.12\n",
      "TrainLoss: 2.299713 TrainAcc: 0.14 TestLoss: 2.3061836 TestAcc: 0.08\n",
      "TrainLoss: 2.2979686 TrainAcc: 0.12 TestLoss: 2.3040023 TestAcc: 0.08\n",
      "TrainLoss: 2.3041015 TrainAcc: 0.04 TestLoss: 2.3033812 TestAcc: 0.16\n",
      "TrainLoss: 2.3028092 TrainAcc: 0.08 TestLoss: 2.300278 TestAcc: 0.1\n",
      "TrainLoss: 2.30374 TrainAcc: 0.16 TestLoss: 2.302674 TestAcc: 0.1\n",
      "TrainLoss: 2.3033216 TrainAcc: 0.14 TestLoss: 2.3053174 TestAcc: 0.08\n",
      "TrainLoss: 2.3021605 TrainAcc: 0.18 TestLoss: 2.3020935 TestAcc: 0.1\n",
      "TrainLoss: 2.2992477 TrainAcc: 0.14 TestLoss: 2.303481 TestAcc: 0.06\n",
      "TrainLoss: 2.3031092 TrainAcc: 0.06 TestLoss: 2.2997367 TestAcc: 0.16\n",
      "TrainLoss: 2.3010976 TrainAcc: 0.12 TestLoss: 2.302793 TestAcc: 0.1\n",
      "TrainLoss: 2.3010757 TrainAcc: 0.08 TestLoss: 2.3048189 TestAcc: 0.08\n",
      "TrainLoss: 2.3027794 TrainAcc: 0.08 TestLoss: 2.3032615 TestAcc: 0.08\n",
      "TrainLoss: 2.302152 TrainAcc: 0.04 TestLoss: 2.3001492 TestAcc: 0.18\n",
      "TrainLoss: 2.303757 TrainAcc: 0.08 TestLoss: 2.3020408 TestAcc: 0.08\n",
      "TrainLoss: 2.3037317 TrainAcc: 0.14 TestLoss: 2.3069189 TestAcc: 0.06\n",
      "TrainLoss: 2.3038926 TrainAcc: 0.12 TestLoss: 2.303353 TestAcc: 0.04\n",
      "TrainLoss: 2.3022687 TrainAcc: 0.1 TestLoss: 2.303485 TestAcc: 0.06\n",
      "TrainLoss: 2.3022416 TrainAcc: 0.16 TestLoss: 2.302406 TestAcc: 0.1\n",
      "TrainLoss: 2.3008032 TrainAcc: 0.16 TestLoss: 2.3028867 TestAcc: 0.06\n",
      "TrainLoss: 2.3022647 TrainAcc: 0.06 TestLoss: 2.3065937 TestAcc: 0.06\n",
      "TrainLoss: 2.303297 TrainAcc: 0.06 TestLoss: 2.3042262 TestAcc: 0.06\n",
      "TrainLoss: 2.3059258 TrainAcc: 0.06 TestLoss: 2.3048732 TestAcc: 0.02\n",
      "TrainLoss: 2.3046339 TrainAcc: 0.02 TestLoss: 2.3028116 TestAcc: 0.12\n",
      "TrainLoss: 2.3075128 TrainAcc: 0.04 TestLoss: 2.3001826 TestAcc: 0.18\n",
      "TrainLoss: 2.2997377 TrainAcc: 0.22 TestLoss: 2.2986438 TestAcc: 0.18\n",
      "TrainLoss: 2.300249 TrainAcc: 0.12 TestLoss: 2.3046088 TestAcc: 0.06\n",
      "TrainLoss: 2.3033123 TrainAcc: 0.16 TestLoss: 2.2982955 TestAcc: 0.14\n",
      "TrainLoss: 2.3033721 TrainAcc: 0.08 TestLoss: 2.2999942 TestAcc: 0.14\n",
      "TrainLoss: 2.3037543 TrainAcc: 0.06 TestLoss: 2.2983143 TestAcc: 0.14\n",
      "TrainLoss: 2.3064785 TrainAcc: 0.08 TestLoss: 2.3029258 TestAcc: 0.06\n",
      "TrainLoss: 2.30672 TrainAcc: 0.08 TestLoss: 2.3036535 TestAcc: 0.08\n",
      "TrainLoss: 2.2994025 TrainAcc: 0.16 TestLoss: 2.301749 TestAcc: 0.08\n",
      "TrainLoss: 2.3048377 TrainAcc: 0.06 TestLoss: 2.3040664 TestAcc: 0.06\n",
      "TrainLoss: 2.3000362 TrainAcc: 0.12 TestLoss: 2.3035429 TestAcc: 0.06\n",
      "TrainLoss: 2.3009913 TrainAcc: 0.18 TestLoss: 2.304605 TestAcc: 0.06\n",
      "TrainLoss: 2.303858 TrainAcc: 0.1 TestLoss: 2.3026235 TestAcc: 0.08\n",
      "TrainLoss: 2.3033462 TrainAcc: 0.1 TestLoss: 2.3048217 TestAcc: 0.08\n",
      "TrainLoss: 2.3029664 TrainAcc: 0.1 TestLoss: 2.3048332 TestAcc: 0.06\n",
      "TrainLoss: 2.302691 TrainAcc: 0.1 TestLoss: 2.3017886 TestAcc: 0.16\n",
      "TrainLoss: 2.3028214 TrainAcc: 0.1 TestLoss: 2.303253 TestAcc: 0.08\n",
      "TrainLoss: 2.2997134 TrainAcc: 0.08 TestLoss: 2.3046236 TestAcc: 0.1\n",
      "TrainLoss: 2.302829 TrainAcc: 0.08 TestLoss: 2.3009374 TestAcc: 0.12\n",
      "TrainLoss: 2.3019717 TrainAcc: 0.1 TestLoss: 2.3022218 TestAcc: 0.16\n",
      "TrainLoss: 2.3013282 TrainAcc: 0.08 TestLoss: 2.3021417 TestAcc: 0.1\n",
      "TrainLoss: 2.297158 TrainAcc: 0.14 TestLoss: 2.3024147 TestAcc: 0.12\n",
      "TrainLoss: 2.3030875 TrainAcc: 0.06 TestLoss: 2.303163 TestAcc: 0.12\n",
      "TrainLoss: 2.3050127 TrainAcc: 0.04 TestLoss: 2.2995234 TestAcc: 0.18\n",
      "TrainLoss: 2.3048577 TrainAcc: 0.04 TestLoss: 2.3041453 TestAcc: 0.08\n",
      "TrainLoss: 2.2979412 TrainAcc: 0.22 TestLoss: 2.3005292 TestAcc: 0.18\n",
      "TrainLoss: 2.3051836 TrainAcc: 0.12 TestLoss: 2.3019636 TestAcc: 0.14\n",
      "TrainLoss: 2.3013492 TrainAcc: 0.14 TestLoss: 2.3014853 TestAcc: 0.1\n",
      "TrainLoss: 2.301539 TrainAcc: 0.12 TestLoss: 2.3027647 TestAcc: 0.1\n",
      "TrainLoss: 2.3026605 TrainAcc: 0.14 TestLoss: 2.30297 TestAcc: 0.08\n",
      "TrainLoss: 2.304222 TrainAcc: 0.02 TestLoss: 2.3009312 TestAcc: 0.06\n",
      "TrainLoss: 2.301982 TrainAcc: 0.16 TestLoss: 2.3043902 TestAcc: 0.04\n",
      "TrainLoss: 2.3051574 TrainAcc: 0.06 TestLoss: 2.3065095 TestAcc: 0.02\n",
      "TrainLoss: 2.301049 TrainAcc: 0.1 TestLoss: 2.3018556 TestAcc: 0.1\n",
      "TrainLoss: 2.303194 TrainAcc: 0.14 TestLoss: 2.3040104 TestAcc: 0.1\n",
      "TrainLoss: 2.3066227 TrainAcc: 0.08 TestLoss: 2.3021688 TestAcc: 0.06\n",
      "TrainLoss: 2.3046763 TrainAcc: 0.06 TestLoss: 2.3039026 TestAcc: 0.14\n",
      "TrainLoss: 2.3048007 TrainAcc: 0.04 TestLoss: 2.301652 TestAcc: 0.16\n",
      "TrainLoss: 2.301963 TrainAcc: 0.08 TestLoss: 2.3010776 TestAcc: 0.16\n",
      "TrainLoss: 2.3041368 TrainAcc: 0.08 TestLoss: 2.305219 TestAcc: 0.08\n",
      "TrainLoss: 2.3021927 TrainAcc: 0.08 TestLoss: 2.3024175 TestAcc: 0.08\n",
      "TrainLoss: 2.303022 TrainAcc: 0.12 TestLoss: 2.3042212 TestAcc: 0.06\n",
      "TrainLoss: 2.2993183 TrainAcc: 0.14 TestLoss: 2.3018742 TestAcc: 0.1\n",
      "TrainLoss: 2.303431 TrainAcc: 0.12 TestLoss: 2.302309 TestAcc: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.2988489 TrainAcc: 0.2 TestLoss: 2.3032856 TestAcc: 0.08\n",
      "TrainLoss: 2.3038168 TrainAcc: 0.1 TestLoss: 2.3006392 TestAcc: 0.12\n",
      "TrainLoss: 2.2970273 TrainAcc: 0.24 TestLoss: 2.299855 TestAcc: 0.14\n",
      "TrainLoss: 2.304259 TrainAcc: 0.04 TestLoss: 2.3028584 TestAcc: 0.1\n",
      "TrainLoss: 2.3047574 TrainAcc: 0.04 TestLoss: 2.3006487 TestAcc: 0.14\n",
      "TrainLoss: 2.301296 TrainAcc: 0.08 TestLoss: 2.3040667 TestAcc: 0.06\n",
      "TrainLoss: 2.2997868 TrainAcc: 0.16 TestLoss: 2.3029542 TestAcc: 0.08\n",
      "TrainLoss: 2.3001566 TrainAcc: 0.04 TestLoss: 2.304045 TestAcc: 0.04\n",
      "TrainLoss: 2.3019874 TrainAcc: 0.1 TestLoss: 2.3017457 TestAcc: 0.08\n",
      "TrainLoss: 2.304776 TrainAcc: 0.08 TestLoss: 2.3052328 TestAcc: 0.08\n",
      "TrainLoss: 2.3017225 TrainAcc: 0.06 TestLoss: 2.3020155 TestAcc: 0.12\n",
      "TrainLoss: 2.301909 TrainAcc: 0.14 TestLoss: 2.3055658 TestAcc: 0.02\n",
      "TrainLoss: 2.303638 TrainAcc: 0.06 TestLoss: 2.3023107 TestAcc: 0.04\n",
      "TrainLoss: 2.2996023 TrainAcc: 0.18 TestLoss: 2.3038113 TestAcc: 0.08\n",
      "TrainLoss: 2.302153 TrainAcc: 0.1 TestLoss: 2.3031895 TestAcc: 0.12\n",
      "TrainLoss: 2.3010976 TrainAcc: 0.06 TestLoss: 2.303029 TestAcc: 0.12\n",
      "TrainLoss: 2.304055 TrainAcc: 0.04 TestLoss: 2.299082 TestAcc: 0.12\n",
      "TrainLoss: 2.3041713 TrainAcc: 0.04 TestLoss: 2.3019152 TestAcc: 0.1\n",
      "TrainLoss: 2.3007479 TrainAcc: 0.04 TestLoss: 2.2999225 TestAcc: 0.2\n",
      "TrainLoss: 2.3048098 TrainAcc: 0.08 TestLoss: 2.3012917 TestAcc: 0.1\n",
      "TrainLoss: 2.302176 TrainAcc: 0.12 TestLoss: 2.3033476 TestAcc: 0.02\n",
      "TrainLoss: 2.3040023 TrainAcc: 0.06 TestLoss: 2.3044038 TestAcc: 0.1\n",
      "TrainLoss: 2.303481 TrainAcc: 0.08 TestLoss: 2.3032384 TestAcc: 0.12\n",
      "TrainLoss: 2.2991657 TrainAcc: 0.16 TestLoss: 2.2990897 TestAcc: 0.14\n",
      "TrainLoss: 2.3048584 TrainAcc: 0.1 TestLoss: 2.3020644 TestAcc: 0.06\n",
      "TrainLoss: 2.3027534 TrainAcc: 0.12 TestLoss: 2.3048043 TestAcc: 0.14\n",
      "TrainLoss: 2.3019528 TrainAcc: 0.12 TestLoss: 2.3028479 TestAcc: 0.06\n",
      "TrainLoss: 2.3033218 TrainAcc: 0.12 TestLoss: 2.3000154 TestAcc: 0.14\n",
      "TrainLoss: 2.3000321 TrainAcc: 0.16 TestLoss: 2.3039396 TestAcc: 0.08\n",
      "TrainLoss: 2.3016386 TrainAcc: 0.12 TestLoss: 2.2997222 TestAcc: 0.18\n",
      "TrainLoss: 2.305006 TrainAcc: 0.06 TestLoss: 2.3042145 TestAcc: 0.06\n",
      "TrainLoss: 2.3058589 TrainAcc: 0.04 TestLoss: 2.3056087 TestAcc: 0.06\n",
      "TrainLoss: 2.2977507 TrainAcc: 0.18 TestLoss: 2.30071 TestAcc: 0.12\n",
      "TrainLoss: 2.3029757 TrainAcc: 0.08 TestLoss: 2.3025086 TestAcc: 0.08\n",
      "TrainLoss: 2.3030236 TrainAcc: 0.12 TestLoss: 2.3053386 TestAcc: 0.02\n",
      "TrainLoss: 2.3003895 TrainAcc: 0.12 TestLoss: 2.2996366 TestAcc: 0.24\n",
      "TrainLoss: 2.3014002 TrainAcc: 0.14 TestLoss: 2.3022196 TestAcc: 0.08\n",
      "TrainLoss: 2.3037596 TrainAcc: 0.12 TestLoss: 2.3010998 TestAcc: 0.22\n",
      "TrainLoss: 2.3053908 TrainAcc: 0.06 TestLoss: 2.3028736 TestAcc: 0.06\n",
      "TrainLoss: 2.3003085 TrainAcc: 0.12 TestLoss: 2.3038602 TestAcc: 0.08\n",
      "TrainLoss: 2.2993822 TrainAcc: 0.1 TestLoss: 2.3033664 TestAcc: 0.08\n",
      "TrainLoss: 2.3024867 TrainAcc: 0.06 TestLoss: 2.3031197 TestAcc: 0.06\n",
      "TrainLoss: 2.299787 TrainAcc: 0.18 TestLoss: 2.3018003 TestAcc: 0.1\n",
      "TrainLoss: 2.3024683 TrainAcc: 0.08 TestLoss: 2.3033485 TestAcc: 0.08\n",
      "TrainLoss: 2.3048332 TrainAcc: 0.04 TestLoss: 2.304314 TestAcc: 0.08\n",
      "TrainLoss: 2.3015745 TrainAcc: 0.14 TestLoss: 2.3004384 TestAcc: 0.16\n",
      "TrainLoss: 2.302343 TrainAcc: 0.12 TestLoss: 2.3014143 TestAcc: 0.14\n",
      "TrainLoss: 2.3022153 TrainAcc: 0.1 TestLoss: 2.30449 TestAcc: 0.12\n",
      "TrainLoss: 2.3013184 TrainAcc: 0.06 TestLoss: 2.3001509 TestAcc: 0.14\n",
      "TrainLoss: 2.3035347 TrainAcc: 0.04 TestLoss: 2.3066785 TestAcc: 0.04\n",
      "TrainLoss: 2.3015528 TrainAcc: 0.1 TestLoss: 2.301695 TestAcc: 0.12\n",
      "TrainLoss: 2.3032787 TrainAcc: 0.04 TestLoss: 2.3025236 TestAcc: 0.12\n",
      "TrainLoss: 2.3021543 TrainAcc: 0.12 TestLoss: 2.3050334 TestAcc: 0.1\n",
      "TrainLoss: 2.3011339 TrainAcc: 0.08 TestLoss: 2.3035426 TestAcc: 0.16\n",
      "TrainLoss: 2.302307 TrainAcc: 0.14 TestLoss: 2.3026838 TestAcc: 0.16\n",
      "TrainLoss: 2.2999933 TrainAcc: 0.16 TestLoss: 2.3016014 TestAcc: 0.14\n",
      "TrainLoss: 2.3035102 TrainAcc: 0.12 TestLoss: 2.3030708 TestAcc: 0.08\n",
      "TrainLoss: 2.3043766 TrainAcc: 0.1 TestLoss: 2.3051293 TestAcc: 0.06\n",
      "TrainLoss: 2.3042572 TrainAcc: 0.08 TestLoss: 2.300187 TestAcc: 0.14\n",
      "TrainLoss: 2.304043 TrainAcc: 0.06 TestLoss: 2.3029902 TestAcc: 0.12\n",
      "TrainLoss: 2.303857 TrainAcc: 0.06 TestLoss: 2.302352 TestAcc: 0.12\n",
      "TrainLoss: 2.3026361 TrainAcc: 0.1 TestLoss: 2.3013816 TestAcc: 0.14\n",
      "TrainLoss: 2.300252 TrainAcc: 0.06 TestLoss: 2.301517 TestAcc: 0.06\n",
      "TrainLoss: 2.3035479 TrainAcc: 0.16 TestLoss: 2.3016863 TestAcc: 0.08\n",
      "TrainLoss: 2.3002143 TrainAcc: 0.12 TestLoss: 2.3034184 TestAcc: 0.08\n",
      "TrainLoss: 2.2984483 TrainAcc: 0.18 TestLoss: 2.3069165 TestAcc: 0.06\n",
      "TrainLoss: 2.3024461 TrainAcc: 0.08 TestLoss: 2.3023672 TestAcc: 0.1\n",
      "TrainLoss: 2.3038301 TrainAcc: 0.08 TestLoss: 2.302946 TestAcc: 0.1\n",
      "TrainLoss: 2.3034005 TrainAcc: 0.08 TestLoss: 2.3047695 TestAcc: 0.1\n",
      "TrainLoss: 2.3013244 TrainAcc: 0.12 TestLoss: 2.3058093 TestAcc: 0.06\n",
      "TrainLoss: 2.302247 TrainAcc: 0.1 TestLoss: 2.3031244 TestAcc: 0.16\n",
      "TrainLoss: 2.30295 TrainAcc: 0.08 TestLoss: 2.3061907 TestAcc: 0.04\n",
      "TrainLoss: 2.3026323 TrainAcc: 0.06 TestLoss: 2.3060448 TestAcc: 0.04\n",
      "TrainLoss: 2.3010755 TrainAcc: 0.22 TestLoss: 2.3002424 TestAcc: 0.18\n",
      "TrainLoss: 2.3031273 TrainAcc: 0.14 TestLoss: 2.302878 TestAcc: 0.04\n",
      "TrainLoss: 2.3037612 TrainAcc: 0.06 TestLoss: 2.3026803 TestAcc: 0.06\n",
      "TrainLoss: 2.303581 TrainAcc: 0.1 TestLoss: 2.3050828 TestAcc: 0.06\n",
      "TrainLoss: 2.302952 TrainAcc: 0.1 TestLoss: 2.304491 TestAcc: 0.06\n",
      "TrainLoss: 2.3060286 TrainAcc: 0.06 TestLoss: 2.3049073 TestAcc: 0.06\n",
      "TrainLoss: 2.3033843 TrainAcc: 0.16 TestLoss: 2.3025987 TestAcc: 0.1\n",
      "TrainLoss: 2.2999392 TrainAcc: 0.14 TestLoss: 2.3054082 TestAcc: 0.08\n",
      "TrainLoss: 2.3030422 TrainAcc: 0.16 TestLoss: 2.3045287 TestAcc: 0.12\n",
      "TrainLoss: 2.3056037 TrainAcc: 0.02 TestLoss: 2.3022213 TestAcc: 0.08\n",
      "TrainLoss: 2.3006191 TrainAcc: 0.14 TestLoss: 2.3029602 TestAcc: 0.08\n",
      "TrainLoss: 2.3004527 TrainAcc: 0.16 TestLoss: 2.3013396 TestAcc: 0.1\n",
      "TrainLoss: 2.303739 TrainAcc: 0.02 TestLoss: 2.3067274 TestAcc: 0.04\n",
      "TrainLoss: 2.3050513 TrainAcc: 0.08 TestLoss: 2.302785 TestAcc: 0.12\n",
      "TrainLoss: 2.3008828 TrainAcc: 0.06 TestLoss: 2.3061395 TestAcc: 0.08\n",
      "TrainLoss: 2.3027494 TrainAcc: 0.08 TestLoss: 2.3055143 TestAcc: 0.02\n",
      "TrainLoss: 2.3029907 TrainAcc: 0.12 TestLoss: 2.3003004 TestAcc: 0.12\n",
      "TrainLoss: 2.3038964 TrainAcc: 0.06 TestLoss: 2.3023877 TestAcc: 0.1\n",
      "TrainLoss: 2.3015127 TrainAcc: 0.1 TestLoss: 2.3037934 TestAcc: 0.1\n",
      "TrainLoss: 2.3019922 TrainAcc: 0.12 TestLoss: 2.3046362 TestAcc: 0.02\n",
      "TrainLoss: 2.3016746 TrainAcc: 0.12 TestLoss: 2.3024158 TestAcc: 0.08\n",
      "TrainLoss: 2.3047225 TrainAcc: 0.14 TestLoss: 2.3031683 TestAcc: 0.06\n",
      "TrainLoss: 2.3026738 TrainAcc: 0.08 TestLoss: 2.3047116 TestAcc: 0.06\n",
      "TrainLoss: 2.3022563 TrainAcc: 0.06 TestLoss: 2.30168 TestAcc: 0.12\n",
      "TrainLoss: 2.3023827 TrainAcc: 0.08 TestLoss: 2.3017187 TestAcc: 0.06\n",
      "TrainLoss: 2.30119 TrainAcc: 0.14 TestLoss: 2.3036928 TestAcc: 0.1\n",
      "TrainLoss: 2.3007462 TrainAcc: 0.1 TestLoss: 2.3008082 TestAcc: 0.08\n",
      "TrainLoss: 2.3052528 TrainAcc: 0.04 TestLoss: 2.3035219 TestAcc: 0.06\n",
      "TrainLoss: 2.3002212 TrainAcc: 0.2 TestLoss: 2.3039455 TestAcc: 0.14\n",
      "TrainLoss: 2.3025951 TrainAcc: 0.08 TestLoss: 2.3021514 TestAcc: 0.16\n",
      "TrainLoss: 2.300993 TrainAcc: 0.12 TestLoss: 2.2999313 TestAcc: 0.14\n",
      "TrainLoss: 2.300994 TrainAcc: 0.14 TestLoss: 2.3027606 TestAcc: 0.06\n",
      "TrainLoss: 2.3041558 TrainAcc: 0.12 TestLoss: 2.3035831 TestAcc: 0.16\n",
      "TrainLoss: 2.3016999 TrainAcc: 0.08 TestLoss: 2.3014712 TestAcc: 0.16\n",
      "TrainLoss: 2.304667 TrainAcc: 0.06 TestLoss: 2.3016312 TestAcc: 0.12\n",
      "TrainLoss: 2.3065639 TrainAcc: 0.02 TestLoss: 2.3041828 TestAcc: 0.04\n",
      "TrainLoss: 2.3012617 TrainAcc: 0.1 TestLoss: 2.3071275 TestAcc: 0.04\n",
      "TrainLoss: 2.3029592 TrainAcc: 0.06 TestLoss: 2.299592 TestAcc: 0.14\n",
      "TrainLoss: 2.3021936 TrainAcc: 0.12 TestLoss: 2.303417 TestAcc: 0.04\n",
      "TrainLoss: 2.30325 TrainAcc: 0.06 TestLoss: 2.3030186 TestAcc: 0.06\n",
      "TrainLoss: 2.304244 TrainAcc: 0.1 TestLoss: 2.3010693 TestAcc: 0.16\n",
      "TrainLoss: 2.3005204 TrainAcc: 0.14 TestLoss: 2.3048725 TestAcc: 0.12\n",
      "TrainLoss: 2.2990322 TrainAcc: 0.18 TestLoss: 2.3031528 TestAcc: 0.08\n",
      "TrainLoss: 2.3004313 TrainAcc: 0.16 TestLoss: 2.304211 TestAcc: 0.12\n",
      "TrainLoss: 2.3032188 TrainAcc: 0.1 TestLoss: 2.301149 TestAcc: 0.12\n",
      "TrainLoss: 2.3028448 TrainAcc: 0.12 TestLoss: 2.301068 TestAcc: 0.08\n",
      "TrainLoss: 2.304143 TrainAcc: 0.08 TestLoss: 2.3036897 TestAcc: 0.08\n",
      "TrainLoss: 2.3010802 TrainAcc: 0.08 TestLoss: 2.3003151 TestAcc: 0.1\n",
      "TrainLoss: 2.300712 TrainAcc: 0.1 TestLoss: 2.302272 TestAcc: 0.06\n",
      "TrainLoss: 2.3031087 TrainAcc: 0.04 TestLoss: 2.3031852 TestAcc: 0.14\n",
      "TrainLoss: 2.3010728 TrainAcc: 0.1 TestLoss: 2.3025239 TestAcc: 0.14\n",
      "TrainLoss: 2.3022928 TrainAcc: 0.12 TestLoss: 2.3024058 TestAcc: 0.16\n",
      "TrainLoss: 2.3021243 TrainAcc: 0.14 TestLoss: 2.3055694 TestAcc: 0.02\n",
      "TrainLoss: 2.3034296 TrainAcc: 0.14 TestLoss: 2.3015716 TestAcc: 0.02\n",
      "TrainLoss: 2.303437 TrainAcc: 0.14 TestLoss: 2.3003945 TestAcc: 0.04\n",
      "TrainLoss: 2.3053772 TrainAcc: 0.02 TestLoss: 2.2993696 TestAcc: 0.14\n",
      "TrainLoss: 2.3022277 TrainAcc: 0.16 TestLoss: 2.3030045 TestAcc: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3019352 TrainAcc: 0.1 TestLoss: 2.3018622 TestAcc: 0.08\n",
      "TrainLoss: 2.3002431 TrainAcc: 0.12 TestLoss: 2.3046155 TestAcc: 0.08\n",
      "TrainLoss: 2.3042293 TrainAcc: 0.06 TestLoss: 2.3037226 TestAcc: 0.14\n",
      "TrainLoss: 2.3003466 TrainAcc: 0.1 TestLoss: 2.302626 TestAcc: 0.06\n",
      "TrainLoss: 2.3028908 TrainAcc: 0.02 TestLoss: 2.3047454 TestAcc: 0.1\n",
      "TrainLoss: 2.30189 TrainAcc: 0.14 TestLoss: 2.304336 TestAcc: 0.06\n",
      "TrainLoss: 2.3022816 TrainAcc: 0.06 TestLoss: 2.3014467 TestAcc: 0.12\n",
      "TrainLoss: 2.3048751 TrainAcc: 0.08 TestLoss: 2.3053706 TestAcc: 0.06\n",
      "TrainLoss: 2.3037772 TrainAcc: 0.06 TestLoss: 2.3018594 TestAcc: 0.14\n",
      "TrainLoss: 2.3038042 TrainAcc: 0.12 TestLoss: 2.3030722 TestAcc: 0.12\n",
      "TrainLoss: 2.299624 TrainAcc: 0.14 TestLoss: 2.3058352 TestAcc: 0.08\n",
      "TrainLoss: 2.3045135 TrainAcc: 0.08 TestLoss: 2.3027124 TestAcc: 0.1\n",
      "TrainLoss: 2.30302 TrainAcc: 0.06 TestLoss: 2.3031306 TestAcc: 0.1\n",
      "TrainLoss: 2.3019352 TrainAcc: 0.14 TestLoss: 2.301077 TestAcc: 0.12\n",
      "TrainLoss: 2.3030777 TrainAcc: 0.1 TestLoss: 2.3033423 TestAcc: 0.14\n",
      "TrainLoss: 2.305702 TrainAcc: 0.06 TestLoss: 2.3031387 TestAcc: 0.14\n",
      "TrainLoss: 2.3021321 TrainAcc: 0.1 TestLoss: 2.30026 TestAcc: 0.16\n",
      "TrainLoss: 2.301934 TrainAcc: 0.1 TestLoss: 2.3025367 TestAcc: 0.08\n",
      "TrainLoss: 2.3000991 TrainAcc: 0.18 TestLoss: 2.3019345 TestAcc: 0.08\n",
      "TrainLoss: 2.303695 TrainAcc: 0.1 TestLoss: 2.305351 TestAcc: 0.06\n",
      "TrainLoss: 2.3047066 TrainAcc: 0.08 TestLoss: 2.3034446 TestAcc: 0.08\n",
      "TrainLoss: 2.2976148 TrainAcc: 0.1 TestLoss: 2.3025465 TestAcc: 0.08\n",
      "TrainLoss: 2.3029215 TrainAcc: 0.02 TestLoss: 2.3053029 TestAcc: 0.04\n",
      "TrainLoss: 2.3016403 TrainAcc: 0.16 TestLoss: 2.303295 TestAcc: 0.1\n",
      "TrainLoss: 2.3022983 TrainAcc: 0.12 TestLoss: 2.300772 TestAcc: 0.1\n",
      "TrainLoss: 2.3007462 TrainAcc: 0.1 TestLoss: 2.3045769 TestAcc: 0.12\n",
      "TrainLoss: 2.3047419 TrainAcc: 0.08 TestLoss: 2.3032503 TestAcc: 0.06\n",
      "TrainLoss: 2.3030968 TrainAcc: 0.0 TestLoss: 2.3008447 TestAcc: 0.12\n",
      "TrainLoss: 2.303457 TrainAcc: 0.12 TestLoss: 2.3022645 TestAcc: 0.08\n",
      "TrainLoss: 2.3012311 TrainAcc: 0.16 TestLoss: 2.3043885 TestAcc: 0.04\n",
      "TrainLoss: 2.3043249 TrainAcc: 0.08 TestLoss: 2.302972 TestAcc: 0.08\n",
      "TrainLoss: 2.3034844 TrainAcc: 0.04 TestLoss: 2.301094 TestAcc: 0.14\n",
      "TrainLoss: 2.3058178 TrainAcc: 0.06 TestLoss: 2.3041587 TestAcc: 0.12\n",
      "TrainLoss: 2.3035183 TrainAcc: 0.08 TestLoss: 2.3006508 TestAcc: 0.1\n",
      "TrainLoss: 2.3020725 TrainAcc: 0.06 TestLoss: 2.301599 TestAcc: 0.12\n",
      "TrainLoss: 2.2985182 TrainAcc: 0.1 TestLoss: 2.3046978 TestAcc: 0.12\n",
      "TrainLoss: 2.302526 TrainAcc: 0.16 TestLoss: 2.305245 TestAcc: 0.04\n",
      "TrainLoss: 2.3040097 TrainAcc: 0.12 TestLoss: 2.3048995 TestAcc: 0.04\n",
      "TrainLoss: 2.3019626 TrainAcc: 0.1 TestLoss: 2.3086803 TestAcc: 0.02\n",
      "TrainLoss: 2.3032386 TrainAcc: 0.12 TestLoss: 2.301847 TestAcc: 0.12\n",
      "TrainLoss: 2.3031359 TrainAcc: 0.1 TestLoss: 2.3056588 TestAcc: 0.04\n",
      "TrainLoss: 2.3030076 TrainAcc: 0.12 TestLoss: 2.3043375 TestAcc: 0.04\n",
      "TrainLoss: 2.301787 TrainAcc: 0.14 TestLoss: 2.3036723 TestAcc: 0.08\n",
      "TrainLoss: 2.3037968 TrainAcc: 0.06 TestLoss: 2.3017683 TestAcc: 0.14\n",
      "TrainLoss: 2.3035247 TrainAcc: 0.1 TestLoss: 2.3047616 TestAcc: 0.1\n",
      "TrainLoss: 2.2999003 TrainAcc: 0.16 TestLoss: 2.3027992 TestAcc: 0.06\n",
      "TrainLoss: 2.3013618 TrainAcc: 0.14 TestLoss: 2.303476 TestAcc: 0.06\n",
      "TrainLoss: 2.2996378 TrainAcc: 0.1 TestLoss: 2.3032408 TestAcc: 0.1\n",
      "TrainLoss: 2.3019323 TrainAcc: 0.1 TestLoss: 2.29974 TestAcc: 0.16\n",
      "TrainLoss: 2.3056345 TrainAcc: 0.04 TestLoss: 2.305545 TestAcc: 0.02\n",
      "TrainLoss: 2.2997568 TrainAcc: 0.2 TestLoss: 2.3015544 TestAcc: 0.12\n",
      "TrainLoss: 2.2988095 TrainAcc: 0.08 TestLoss: 2.3037636 TestAcc: 0.12\n",
      "TrainLoss: 2.301619 TrainAcc: 0.1 TestLoss: 2.2993057 TestAcc: 0.18\n",
      "TrainLoss: 2.3026447 TrainAcc: 0.08 TestLoss: 2.3040354 TestAcc: 0.08\n",
      "TrainLoss: 2.3017232 TrainAcc: 0.1 TestLoss: 2.3009057 TestAcc: 0.1\n",
      "TrainLoss: 2.301898 TrainAcc: 0.02 TestLoss: 2.300383 TestAcc: 0.16\n",
      "TrainLoss: 2.298744 TrainAcc: 0.18 TestLoss: 2.3033555 TestAcc: 0.0\n",
      "TrainLoss: 2.303517 TrainAcc: 0.1 TestLoss: 2.3001351 TestAcc: 0.1\n",
      "TrainLoss: 2.3014953 TrainAcc: 0.16 TestLoss: 2.3073733 TestAcc: 0.02\n",
      "TrainLoss: 2.2989874 TrainAcc: 0.16 TestLoss: 2.3068383 TestAcc: 0.04\n",
      "TrainLoss: 2.2992349 TrainAcc: 0.2 TestLoss: 2.3026175 TestAcc: 0.08\n",
      "TrainLoss: 2.2988393 TrainAcc: 0.14 TestLoss: 2.3034406 TestAcc: 0.06\n",
      "TrainLoss: 2.305132 TrainAcc: 0.12 TestLoss: 2.29964 TestAcc: 0.08\n",
      "TrainLoss: 2.3018866 TrainAcc: 0.1 TestLoss: 2.3023293 TestAcc: 0.1\n",
      "TrainLoss: 2.3031585 TrainAcc: 0.12 TestLoss: 2.3016658 TestAcc: 0.1\n",
      "TrainLoss: 2.3041608 TrainAcc: 0.1 TestLoss: 2.3040965 TestAcc: 0.12\n",
      "TrainLoss: 2.2971666 TrainAcc: 0.06 TestLoss: 2.3029335 TestAcc: 0.08\n",
      "TrainLoss: 2.3018882 TrainAcc: 0.16 TestLoss: 2.3015707 TestAcc: 0.08\n",
      "TrainLoss: 2.3013985 TrainAcc: 0.1 TestLoss: 2.3038423 TestAcc: 0.08\n",
      "TrainLoss: 2.2997804 TrainAcc: 0.2 TestLoss: 2.3074427 TestAcc: 0.04\n",
      "TrainLoss: 2.299719 TrainAcc: 0.22 TestLoss: 2.3033526 TestAcc: 0.08\n",
      "TrainLoss: 2.302849 TrainAcc: 0.12 TestLoss: 2.3031788 TestAcc: 0.04\n",
      "TrainLoss: 2.3008158 TrainAcc: 0.14 TestLoss: 2.3042097 TestAcc: 0.08\n",
      "TrainLoss: 2.3081274 TrainAcc: 0.04 TestLoss: 2.3019567 TestAcc: 0.14\n",
      "TrainLoss: 2.3046732 TrainAcc: 0.06 TestLoss: 2.3031275 TestAcc: 0.04\n",
      "TrainLoss: 2.3001566 TrainAcc: 0.1 TestLoss: 2.302445 TestAcc: 0.06\n",
      "TrainLoss: 2.3061259 TrainAcc: 0.08 TestLoss: 2.3019414 TestAcc: 0.12\n",
      "TrainLoss: 2.3012786 TrainAcc: 0.16 TestLoss: 2.3079143 TestAcc: 0.04\n",
      "TrainLoss: 2.3026583 TrainAcc: 0.1 TestLoss: 2.2983 TestAcc: 0.18\n",
      "TrainLoss: 2.3033216 TrainAcc: 0.02 TestLoss: 2.302081 TestAcc: 0.12\n",
      "TrainLoss: 2.3032546 TrainAcc: 0.12 TestLoss: 2.3015745 TestAcc: 0.06\n",
      "TrainLoss: 2.3014016 TrainAcc: 0.16 TestLoss: 2.3030355 TestAcc: 0.02\n",
      "TrainLoss: 2.303403 TrainAcc: 0.06 TestLoss: 2.2993624 TestAcc: 0.1\n",
      "TrainLoss: 2.3025439 TrainAcc: 0.08 TestLoss: 2.3015416 TestAcc: 0.1\n",
      "TrainLoss: 2.30587 TrainAcc: 0.04 TestLoss: 2.304438 TestAcc: 0.08\n",
      "TrainLoss: 2.3021874 TrainAcc: 0.04 TestLoss: 2.3000453 TestAcc: 0.12\n",
      "TrainLoss: 2.3003435 TrainAcc: 0.1 TestLoss: 2.3054159 TestAcc: 0.08\n",
      "TrainLoss: 2.3000495 TrainAcc: 0.06 TestLoss: 2.305544 TestAcc: 0.1\n",
      "TrainLoss: 2.3022785 TrainAcc: 0.08 TestLoss: 2.3017771 TestAcc: 0.06\n",
      "TrainLoss: 2.3022974 TrainAcc: 0.12 TestLoss: 2.3046882 TestAcc: 0.1\n",
      "TrainLoss: 2.300033 TrainAcc: 0.1 TestLoss: 2.3007207 TestAcc: 0.08\n",
      "TrainLoss: 2.3004034 TrainAcc: 0.02 TestLoss: 2.3005512 TestAcc: 0.18\n",
      "TrainLoss: 2.3055394 TrainAcc: 0.08 TestLoss: 2.3000827 TestAcc: 0.1\n",
      "TrainLoss: 2.2979448 TrainAcc: 0.16 TestLoss: 2.3036857 TestAcc: 0.08\n",
      "TrainLoss: 2.3039045 TrainAcc: 0.08 TestLoss: 2.303795 TestAcc: 0.16\n",
      "TrainLoss: 2.3057966 TrainAcc: 0.1 TestLoss: 2.3038733 TestAcc: 0.06\n",
      "TrainLoss: 2.3041697 TrainAcc: 0.04 TestLoss: 2.2991946 TestAcc: 0.14\n",
      "TrainLoss: 2.3003247 TrainAcc: 0.1 TestLoss: 2.3032126 TestAcc: 0.04\n",
      "TrainLoss: 2.302484 TrainAcc: 0.14 TestLoss: 2.3018394 TestAcc: 0.06\n",
      "TrainLoss: 2.3007421 TrainAcc: 0.12 TestLoss: 2.300194 TestAcc: 0.12\n",
      "TrainLoss: 2.305488 TrainAcc: 0.08 TestLoss: 2.301875 TestAcc: 0.08\n",
      "TrainLoss: 2.3036344 TrainAcc: 0.04 TestLoss: 2.302332 TestAcc: 0.06\n",
      "TrainLoss: 2.3002453 TrainAcc: 0.1 TestLoss: 2.3069222 TestAcc: 0.06\n",
      "TrainLoss: 2.3049111 TrainAcc: 0.08 TestLoss: 2.3062775 TestAcc: 0.04\n",
      "TrainLoss: 2.303795 TrainAcc: 0.12 TestLoss: 2.3033586 TestAcc: 0.1\n",
      "TrainLoss: 2.3048298 TrainAcc: 0.08 TestLoss: 2.3030524 TestAcc: 0.1\n",
      "TrainLoss: 2.3030987 TrainAcc: 0.12 TestLoss: 2.3036475 TestAcc: 0.08\n",
      "TrainLoss: 2.301171 TrainAcc: 0.06 TestLoss: 2.3026056 TestAcc: 0.08\n",
      "TrainLoss: 2.3037727 TrainAcc: 0.08 TestLoss: 2.302395 TestAcc: 0.1\n",
      "TrainLoss: 2.3028724 TrainAcc: 0.08 TestLoss: 2.305981 TestAcc: 0.06\n",
      "TrainLoss: 2.3053875 TrainAcc: 0.06 TestLoss: 2.3043559 TestAcc: 0.06\n",
      "TrainLoss: 2.3045592 TrainAcc: 0.04 TestLoss: 2.3057792 TestAcc: 0.06\n",
      "TrainLoss: 2.3017423 TrainAcc: 0.12 TestLoss: 2.3051264 TestAcc: 0.0\n",
      "TrainLoss: 2.303409 TrainAcc: 0.18 TestLoss: 2.3044693 TestAcc: 0.1\n",
      "TrainLoss: 2.302303 TrainAcc: 0.1 TestLoss: 2.3014886 TestAcc: 0.12\n",
      "TrainLoss: 2.3027506 TrainAcc: 0.16 TestLoss: 2.3023772 TestAcc: 0.16\n",
      "TrainLoss: 2.304467 TrainAcc: 0.1 TestLoss: 2.3028917 TestAcc: 0.08\n",
      "TrainLoss: 2.3000698 TrainAcc: 0.22 TestLoss: 2.3050218 TestAcc: 0.06\n",
      "TrainLoss: 2.301299 TrainAcc: 0.12 TestLoss: 2.3042138 TestAcc: 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3038273 TrainAcc: 0.08 TestLoss: 2.3007472 TestAcc: 0.08\n",
      "TrainLoss: 2.3040876 TrainAcc: 0.08 TestLoss: 2.301684 TestAcc: 0.1\n",
      "TrainLoss: 2.301606 TrainAcc: 0.14 TestLoss: 2.2986028 TestAcc: 0.22\n",
      "TrainLoss: 2.3016825 TrainAcc: 0.14 TestLoss: 2.302465 TestAcc: 0.06\n",
      "TrainLoss: 2.3027625 TrainAcc: 0.12 TestLoss: 2.3021514 TestAcc: 0.1\n",
      "TrainLoss: 2.3050015 TrainAcc: 0.12 TestLoss: 2.3039148 TestAcc: 0.12\n",
      "TrainLoss: 2.2995436 TrainAcc: 0.14 TestLoss: 2.3033445 TestAcc: 0.08\n",
      "TrainLoss: 2.299762 TrainAcc: 0.06 TestLoss: 2.304033 TestAcc: 0.06\n",
      "TrainLoss: 2.3033068 TrainAcc: 0.06 TestLoss: 2.304577 TestAcc: 0.08\n",
      "TrainLoss: 2.3054109 TrainAcc: 0.02 TestLoss: 2.299598 TestAcc: 0.14\n",
      "TrainLoss: 2.3014166 TrainAcc: 0.12 TestLoss: 2.3022153 TestAcc: 0.14\n",
      "TrainLoss: 2.30086 TrainAcc: 0.14 TestLoss: 2.3040705 TestAcc: 0.08\n",
      "TrainLoss: 2.3014524 TrainAcc: 0.18 TestLoss: 2.3019 TestAcc: 0.16\n",
      "TrainLoss: 2.3027627 TrainAcc: 0.1 TestLoss: 2.3000672 TestAcc: 0.18\n",
      "TrainLoss: 2.3040142 TrainAcc: 0.12 TestLoss: 2.3008928 TestAcc: 0.18\n",
      "TrainLoss: 2.3006546 TrainAcc: 0.12 TestLoss: 2.3013206 TestAcc: 0.1\n",
      "TrainLoss: 2.2995384 TrainAcc: 0.18 TestLoss: 2.3036337 TestAcc: 0.04\n",
      "TrainLoss: 2.3033926 TrainAcc: 0.12 TestLoss: 2.3042595 TestAcc: 0.08\n",
      "TrainLoss: 2.30266 TrainAcc: 0.1 TestLoss: 2.3052812 TestAcc: 0.02\n",
      "TrainLoss: 2.3013146 TrainAcc: 0.1 TestLoss: 2.30039 TestAcc: 0.12\n",
      "TrainLoss: 2.3025498 TrainAcc: 0.08 TestLoss: 2.302309 TestAcc: 0.16\n",
      "TrainLoss: 2.3014147 TrainAcc: 0.12 TestLoss: 2.3060777 TestAcc: 0.02\n",
      "TrainLoss: 2.3012505 TrainAcc: 0.12 TestLoss: 2.304277 TestAcc: 0.12\n",
      "TrainLoss: 2.3002224 TrainAcc: 0.12 TestLoss: 2.3005655 TestAcc: 0.1\n",
      "TrainLoss: 2.3030808 TrainAcc: 0.1 TestLoss: 2.3033366 TestAcc: 0.02\n",
      "TrainLoss: 2.3017566 TrainAcc: 0.14 TestLoss: 2.3029776 TestAcc: 0.04\n",
      "TrainLoss: 2.300951 TrainAcc: 0.04 TestLoss: 2.3028333 TestAcc: 0.1\n",
      "TrainLoss: 2.3041532 TrainAcc: 0.08 TestLoss: 2.3054504 TestAcc: 0.1\n",
      "TrainLoss: 2.298768 TrainAcc: 0.24 TestLoss: 2.2999253 TestAcc: 0.14\n",
      "TrainLoss: 2.302857 TrainAcc: 0.08 TestLoss: 2.3021684 TestAcc: 0.06\n",
      "TrainLoss: 2.2990992 TrainAcc: 0.08 TestLoss: 2.3035107 TestAcc: 0.06\n",
      "TrainLoss: 2.3031242 TrainAcc: 0.1 TestLoss: 2.3012028 TestAcc: 0.1\n",
      "TrainLoss: 2.3023262 TrainAcc: 0.14 TestLoss: 2.306927 TestAcc: 0.1\n",
      "TrainLoss: 2.3023002 TrainAcc: 0.08 TestLoss: 2.3046777 TestAcc: 0.12\n",
      "TrainLoss: 2.3035488 TrainAcc: 0.06 TestLoss: 2.3020062 TestAcc: 0.04\n",
      "TrainLoss: 2.3064306 TrainAcc: 0.06 TestLoss: 2.3057761 TestAcc: 0.06\n",
      "TrainLoss: 2.3025827 TrainAcc: 0.02 TestLoss: 2.3016882 TestAcc: 0.1\n",
      "TrainLoss: 2.303697 TrainAcc: 0.14 TestLoss: 2.304855 TestAcc: 0.1\n",
      "TrainLoss: 2.3062546 TrainAcc: 0.08 TestLoss: 2.3025098 TestAcc: 0.1\n",
      "TrainLoss: 2.3039455 TrainAcc: 0.1 TestLoss: 2.3033118 TestAcc: 0.06\n",
      "TrainLoss: 2.305555 TrainAcc: 0.06 TestLoss: 2.3003433 TestAcc: 0.12\n",
      "TrainLoss: 2.3009079 TrainAcc: 0.18 TestLoss: 2.3037863 TestAcc: 0.2\n",
      "TrainLoss: 2.3011932 TrainAcc: 0.16 TestLoss: 2.299049 TestAcc: 0.16\n",
      "TrainLoss: 2.3048954 TrainAcc: 0.14 TestLoss: 2.3034894 TestAcc: 0.14\n",
      "TrainLoss: 2.304732 TrainAcc: 0.1 TestLoss: 2.3007128 TestAcc: 0.12\n",
      "TrainLoss: 2.3042634 TrainAcc: 0.08 TestLoss: 2.3036344 TestAcc: 0.06\n",
      "TrainLoss: 2.3011808 TrainAcc: 0.2 TestLoss: 2.3040285 TestAcc: 0.06\n",
      "TrainLoss: 2.2992704 TrainAcc: 0.12 TestLoss: 2.3041537 TestAcc: 0.16\n",
      "TrainLoss: 2.3002777 TrainAcc: 0.14 TestLoss: 2.3021631 TestAcc: 0.12\n",
      "TrainLoss: 2.3071532 TrainAcc: 0.06 TestLoss: 2.3025894 TestAcc: 0.16\n",
      "TrainLoss: 2.304016 TrainAcc: 0.08 TestLoss: 2.3022215 TestAcc: 0.14\n",
      "TrainLoss: 2.3034523 TrainAcc: 0.12 TestLoss: 2.3031003 TestAcc: 0.08\n",
      "TrainLoss: 2.3007214 TrainAcc: 0.08 TestLoss: 2.3011966 TestAcc: 0.14\n",
      "TrainLoss: 2.2991123 TrainAcc: 0.1 TestLoss: 2.305683 TestAcc: 0.06\n",
      "TrainLoss: 2.29974 TrainAcc: 0.06 TestLoss: 2.2993298 TestAcc: 0.18\n",
      "TrainLoss: 2.3072238 TrainAcc: 0.06 TestLoss: 2.3008559 TestAcc: 0.18\n",
      "TrainLoss: 2.3028023 TrainAcc: 0.1 TestLoss: 2.3056977 TestAcc: 0.02\n",
      "TrainLoss: 2.3016872 TrainAcc: 0.16 TestLoss: 2.2993484 TestAcc: 0.12\n",
      "TrainLoss: 2.3008537 TrainAcc: 0.12 TestLoss: 2.3045542 TestAcc: 0.1\n",
      "TrainLoss: 2.3004372 TrainAcc: 0.1 TestLoss: 2.3028412 TestAcc: 0.1\n",
      "TrainLoss: 2.3043647 TrainAcc: 0.08 TestLoss: 2.3038638 TestAcc: 0.1\n",
      "TrainLoss: 2.305252 TrainAcc: 0.08 TestLoss: 2.3058622 TestAcc: 0.08\n",
      "TrainLoss: 2.301872 TrainAcc: 0.12 TestLoss: 2.302115 TestAcc: 0.12\n",
      "TrainLoss: 2.30359 TrainAcc: 0.08 TestLoss: 2.3023586 TestAcc: 0.06\n",
      "TrainLoss: 2.3033173 TrainAcc: 0.12 TestLoss: 2.3016553 TestAcc: 0.16\n",
      "TrainLoss: 2.3000028 TrainAcc: 0.14 TestLoss: 2.301758 TestAcc: 0.14\n",
      "TrainLoss: 2.3035858 TrainAcc: 0.08 TestLoss: 2.3018239 TestAcc: 0.12\n",
      "TrainLoss: 2.301518 TrainAcc: 0.08 TestLoss: 2.3042207 TestAcc: 0.08\n",
      "TrainLoss: 2.3003097 TrainAcc: 0.08 TestLoss: 2.3028255 TestAcc: 0.04\n",
      "TrainLoss: 2.3026981 TrainAcc: 0.18 TestLoss: 2.3030398 TestAcc: 0.02\n",
      "TrainLoss: 2.3019295 TrainAcc: 0.04 TestLoss: 2.303025 TestAcc: 0.02\n",
      "TrainLoss: 2.3001342 TrainAcc: 0.2 TestLoss: 2.3011055 TestAcc: 0.06\n",
      "TrainLoss: 2.3011453 TrainAcc: 0.08 TestLoss: 2.302268 TestAcc: 0.12\n",
      "TrainLoss: 2.302721 TrainAcc: 0.1 TestLoss: 2.3036683 TestAcc: 0.1\n",
      "TrainLoss: 2.3049455 TrainAcc: 0.1 TestLoss: 2.3023617 TestAcc: 0.1\n",
      "TrainLoss: 2.304331 TrainAcc: 0.1 TestLoss: 2.305028 TestAcc: 0.02\n",
      "TrainLoss: 2.3013766 TrainAcc: 0.06 TestLoss: 2.3048763 TestAcc: 0.06\n",
      "TrainLoss: 2.3034468 TrainAcc: 0.12 TestLoss: 2.3025205 TestAcc: 0.1\n",
      "TrainLoss: 2.3003378 TrainAcc: 0.14 TestLoss: 2.305238 TestAcc: 0.06\n",
      "TrainLoss: 2.3049831 TrainAcc: 0.08 TestLoss: 2.304275 TestAcc: 0.06\n",
      "TrainLoss: 2.3030317 TrainAcc: 0.06 TestLoss: 2.3026664 TestAcc: 0.06\n",
      "TrainLoss: 2.3036356 TrainAcc: 0.08 TestLoss: 2.302678 TestAcc: 0.1\n",
      "TrainLoss: 2.302498 TrainAcc: 0.2 TestLoss: 2.3049302 TestAcc: 0.08\n",
      "TrainLoss: 2.302515 TrainAcc: 0.08 TestLoss: 2.3052137 TestAcc: 0.1\n",
      "TrainLoss: 2.3018067 TrainAcc: 0.08 TestLoss: 2.300188 TestAcc: 0.06\n",
      "TrainLoss: 2.3021052 TrainAcc: 0.16 TestLoss: 2.3000824 TestAcc: 0.14\n",
      "TrainLoss: 2.3035142 TrainAcc: 0.04 TestLoss: 2.3046923 TestAcc: 0.12\n",
      "TrainLoss: 2.3044238 TrainAcc: 0.14 TestLoss: 2.302453 TestAcc: 0.06\n",
      "TrainLoss: 2.3045619 TrainAcc: 0.06 TestLoss: 2.302721 TestAcc: 0.08\n",
      "TrainLoss: 2.301112 TrainAcc: 0.08 TestLoss: 2.3003113 TestAcc: 0.14\n",
      "TrainLoss: 2.3018699 TrainAcc: 0.1 TestLoss: 2.3054867 TestAcc: 0.02\n",
      "TrainLoss: 2.2985854 TrainAcc: 0.14 TestLoss: 2.30055 TestAcc: 0.16\n",
      "TrainLoss: 2.3015604 TrainAcc: 0.2 TestLoss: 2.3002996 TestAcc: 0.1\n",
      "TrainLoss: 2.3021095 TrainAcc: 0.1 TestLoss: 2.3022158 TestAcc: 0.08\n",
      "TrainLoss: 2.3007421 TrainAcc: 0.1 TestLoss: 2.3017762 TestAcc: 0.1\n",
      "TrainLoss: 2.300264 TrainAcc: 0.14 TestLoss: 2.2988749 TestAcc: 0.16\n",
      "TrainLoss: 2.300464 TrainAcc: 0.14 TestLoss: 2.2996607 TestAcc: 0.14\n",
      "TrainLoss: 2.303781 TrainAcc: 0.08 TestLoss: 2.3041756 TestAcc: 0.06\n",
      "TrainLoss: 2.3048735 TrainAcc: 0.14 TestLoss: 2.301907 TestAcc: 0.18\n",
      "TrainLoss: 2.3036292 TrainAcc: 0.1 TestLoss: 2.302659 TestAcc: 0.16\n",
      "TrainLoss: 2.3038988 TrainAcc: 0.08 TestLoss: 2.3004305 TestAcc: 0.16\n",
      "TrainLoss: 2.3040519 TrainAcc: 0.1 TestLoss: 2.3051064 TestAcc: 0.08\n",
      "TrainLoss: 2.302744 TrainAcc: 0.12 TestLoss: 2.3057113 TestAcc: 0.0\n",
      "TrainLoss: 2.3035347 TrainAcc: 0.12 TestLoss: 2.303743 TestAcc: 0.12\n",
      "TrainLoss: 2.304355 TrainAcc: 0.06 TestLoss: 2.303879 TestAcc: 0.14\n",
      "TrainLoss: 2.3016365 TrainAcc: 0.14 TestLoss: 2.3035994 TestAcc: 0.04\n",
      "TrainLoss: 2.3059618 TrainAcc: 0.02 TestLoss: 2.3014135 TestAcc: 0.08\n",
      "TrainLoss: 2.3028207 TrainAcc: 0.02 TestLoss: 2.3009853 TestAcc: 0.1\n",
      "TrainLoss: 2.300759 TrainAcc: 0.14 TestLoss: 2.3045413 TestAcc: 0.1\n",
      "TrainLoss: 2.3042367 TrainAcc: 0.12 TestLoss: 2.3028562 TestAcc: 0.06\n",
      "TrainLoss: 2.3048778 TrainAcc: 0.06 TestLoss: 2.303868 TestAcc: 0.08\n",
      "TrainLoss: 2.3033063 TrainAcc: 0.06 TestLoss: 2.3007424 TestAcc: 0.18\n",
      "TrainLoss: 2.304387 TrainAcc: 0.08 TestLoss: 2.3018236 TestAcc: 0.08\n",
      "TrainLoss: 2.3021567 TrainAcc: 0.1 TestLoss: 2.298908 TestAcc: 0.2\n",
      "TrainLoss: 2.301635 TrainAcc: 0.1 TestLoss: 2.303278 TestAcc: 0.1\n",
      "TrainLoss: 2.3023484 TrainAcc: 0.06 TestLoss: 2.3044488 TestAcc: 0.08\n",
      "TrainLoss: 2.303237 TrainAcc: 0.04 TestLoss: 2.3020103 TestAcc: 0.08\n",
      "TrainLoss: 2.302593 TrainAcc: 0.1 TestLoss: 2.300956 TestAcc: 0.16\n",
      "TrainLoss: 2.3018284 TrainAcc: 0.12 TestLoss: 2.3029861 TestAcc: 0.1\n",
      "TrainLoss: 2.3015387 TrainAcc: 0.16 TestLoss: 2.305357 TestAcc: 0.06\n",
      "TrainLoss: 2.3031857 TrainAcc: 0.04 TestLoss: 2.3029354 TestAcc: 0.04\n",
      "TrainLoss: 2.3016267 TrainAcc: 0.08 TestLoss: 2.3023036 TestAcc: 0.14\n",
      "TrainLoss: 2.3025036 TrainAcc: 0.12 TestLoss: 2.3043358 TestAcc: 0.12\n",
      "TrainLoss: 2.3036892 TrainAcc: 0.1 TestLoss: 2.3046336 TestAcc: 0.12\n",
      "TrainLoss: 2.3042734 TrainAcc: 0.1 TestLoss: 2.3036692 TestAcc: 0.08\n",
      "TrainLoss: 2.3042943 TrainAcc: 0.08 TestLoss: 2.302852 TestAcc: 0.08\n",
      "TrainLoss: 2.3024487 TrainAcc: 0.08 TestLoss: 2.3034735 TestAcc: 0.04\n",
      "TrainLoss: 2.3061821 TrainAcc: 0.04 TestLoss: 2.3046653 TestAcc: 0.02\n",
      "TrainLoss: 2.300994 TrainAcc: 0.12 TestLoss: 2.30232 TestAcc: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3005335 TrainAcc: 0.12 TestLoss: 2.300865 TestAcc: 0.14\n",
      "TrainLoss: 2.301165 TrainAcc: 0.14 TestLoss: 2.302654 TestAcc: 0.12\n",
      "TrainLoss: 2.3029149 TrainAcc: 0.12 TestLoss: 2.2995436 TestAcc: 0.16\n",
      "TrainLoss: 2.301058 TrainAcc: 0.1 TestLoss: 2.3034256 TestAcc: 0.1\n",
      "TrainLoss: 2.3031657 TrainAcc: 0.08 TestLoss: 2.3020086 TestAcc: 0.08\n",
      "TrainLoss: 2.3014288 TrainAcc: 0.1 TestLoss: 2.3022616 TestAcc: 0.06\n",
      "TrainLoss: 2.3036125 TrainAcc: 0.1 TestLoss: 2.302203 TestAcc: 0.1\n",
      "TrainLoss: 2.302754 TrainAcc: 0.16 TestLoss: 2.3029442 TestAcc: 0.14\n",
      "TrainLoss: 2.3017786 TrainAcc: 0.16 TestLoss: 2.3016374 TestAcc: 0.12\n",
      "TrainLoss: 2.3046012 TrainAcc: 0.08 TestLoss: 2.3046098 TestAcc: 0.02\n",
      "TrainLoss: 2.3025215 TrainAcc: 0.08 TestLoss: 2.3023756 TestAcc: 0.14\n",
      "TrainLoss: 2.3018682 TrainAcc: 0.12 TestLoss: 2.3016005 TestAcc: 0.14\n",
      "TrainLoss: 2.3049579 TrainAcc: 0.08 TestLoss: 2.3008537 TestAcc: 0.2\n",
      "TrainLoss: 2.3025444 TrainAcc: 0.12 TestLoss: 2.3014283 TestAcc: 0.08\n",
      "TrainLoss: 2.3031285 TrainAcc: 0.12 TestLoss: 2.3004599 TestAcc: 0.08\n",
      "TrainLoss: 2.3017385 TrainAcc: 0.16 TestLoss: 2.303165 TestAcc: 0.1\n",
      "TrainLoss: 2.302421 TrainAcc: 0.14 TestLoss: 2.30343 TestAcc: 0.06\n",
      "TrainLoss: 2.3036425 TrainAcc: 0.1 TestLoss: 2.303195 TestAcc: 0.06\n",
      "TrainLoss: 2.3003066 TrainAcc: 0.16 TestLoss: 2.3043857 TestAcc: 0.08\n",
      "TrainLoss: 2.3039062 TrainAcc: 0.06 TestLoss: 2.301959 TestAcc: 0.14\n",
      "TrainLoss: 2.3045187 TrainAcc: 0.06 TestLoss: 2.3026617 TestAcc: 0.08\n",
      "TrainLoss: 2.3034837 TrainAcc: 0.1 TestLoss: 2.3045204 TestAcc: 0.12\n",
      "TrainLoss: 2.3006425 TrainAcc: 0.08 TestLoss: 2.302871 TestAcc: 0.14\n",
      "TrainLoss: 2.3031778 TrainAcc: 0.14 TestLoss: 2.3030317 TestAcc: 0.1\n",
      "TrainLoss: 2.3022432 TrainAcc: 0.08 TestLoss: 2.3018126 TestAcc: 0.12\n",
      "TrainLoss: 2.3046854 TrainAcc: 0.04 TestLoss: 2.303776 TestAcc: 0.02\n",
      "TrainLoss: 2.3004565 TrainAcc: 0.08 TestLoss: 2.3031464 TestAcc: 0.08\n",
      "TrainLoss: 2.3005543 TrainAcc: 0.08 TestLoss: 2.29991 TestAcc: 0.16\n",
      "TrainLoss: 2.303908 TrainAcc: 0.02 TestLoss: 2.30125 TestAcc: 0.14\n",
      "TrainLoss: 2.3064377 TrainAcc: 0.02 TestLoss: 2.3010116 TestAcc: 0.1\n",
      "TrainLoss: 2.3022203 TrainAcc: 0.08 TestLoss: 2.302966 TestAcc: 0.04\n",
      "TrainLoss: 2.298915 TrainAcc: 0.16 TestLoss: 2.2992969 TestAcc: 0.24\n",
      "TrainLoss: 2.2990997 TrainAcc: 0.18 TestLoss: 2.3045034 TestAcc: 0.06\n",
      "TrainLoss: 2.3025792 TrainAcc: 0.1 TestLoss: 2.2995095 TestAcc: 0.16\n",
      "TrainLoss: 2.3024979 TrainAcc: 0.16 TestLoss: 2.304546 TestAcc: 0.04\n",
      "TrainLoss: 2.3047972 TrainAcc: 0.04 TestLoss: 2.30627 TestAcc: 0.02\n",
      "TrainLoss: 2.3018968 TrainAcc: 0.08 TestLoss: 2.302688 TestAcc: 0.08\n",
      "TrainLoss: 2.300534 TrainAcc: 0.1 TestLoss: 2.3027637 TestAcc: 0.1\n",
      "TrainLoss: 2.3039854 TrainAcc: 0.14 TestLoss: 2.3042898 TestAcc: 0.1\n",
      "TrainLoss: 2.304108 TrainAcc: 0.06 TestLoss: 2.3028 TestAcc: 0.08\n",
      "TrainLoss: 2.3031597 TrainAcc: 0.08 TestLoss: 2.301 TestAcc: 0.1\n",
      "TrainLoss: 2.2999337 TrainAcc: 0.18 TestLoss: 2.3038895 TestAcc: 0.04\n",
      "TrainLoss: 2.3032372 TrainAcc: 0.1 TestLoss: 2.301581 TestAcc: 0.14\n",
      "TrainLoss: 2.302529 TrainAcc: 0.12 TestLoss: 2.3014553 TestAcc: 0.18\n",
      "TrainLoss: 2.301309 TrainAcc: 0.06 TestLoss: 2.2998443 TestAcc: 0.04\n",
      "TrainLoss: 2.3016214 TrainAcc: 0.18 TestLoss: 2.3042338 TestAcc: 0.08\n",
      "TrainLoss: 2.3035223 TrainAcc: 0.1 TestLoss: 2.3043537 TestAcc: 0.06\n",
      "TrainLoss: 2.3023663 TrainAcc: 0.14 TestLoss: 2.30106 TestAcc: 0.16\n",
      "TrainLoss: 2.302029 TrainAcc: 0.1 TestLoss: 2.3013794 TestAcc: 0.16\n",
      "TrainLoss: 2.302006 TrainAcc: 0.08 TestLoss: 2.3030572 TestAcc: 0.12\n",
      "TrainLoss: 2.303257 TrainAcc: 0.08 TestLoss: 2.3051383 TestAcc: 0.1\n",
      "TrainLoss: 2.3034475 TrainAcc: 0.14 TestLoss: 2.3032732 TestAcc: 0.1\n",
      "TrainLoss: 2.2998142 TrainAcc: 0.16 TestLoss: 2.301235 TestAcc: 0.14\n",
      "TrainLoss: 2.3006659 TrainAcc: 0.12 TestLoss: 2.3044758 TestAcc: 0.12\n",
      "TrainLoss: 2.3006313 TrainAcc: 0.14 TestLoss: 2.3027809 TestAcc: 0.06\n",
      "TrainLoss: 2.298855 TrainAcc: 0.22 TestLoss: 2.3027248 TestAcc: 0.04\n",
      "TrainLoss: 2.3008556 TrainAcc: 0.1 TestLoss: 2.3013506 TestAcc: 0.14\n",
      "TrainLoss: 2.3023252 TrainAcc: 0.08 TestLoss: 2.3012893 TestAcc: 0.14\n",
      "TrainLoss: 2.3044245 TrainAcc: 0.08 TestLoss: 2.3018296 TestAcc: 0.08\n",
      "TrainLoss: 2.301311 TrainAcc: 0.14 TestLoss: 2.3027143 TestAcc: 0.12\n",
      "TrainLoss: 2.3067217 TrainAcc: 0.04 TestLoss: 2.3008242 TestAcc: 0.12\n",
      "TrainLoss: 2.30404 TrainAcc: 0.06 TestLoss: 2.2998624 TestAcc: 0.2\n",
      "TrainLoss: 2.3047144 TrainAcc: 0.06 TestLoss: 2.3002822 TestAcc: 0.14\n",
      "TrainLoss: 2.2991946 TrainAcc: 0.14 TestLoss: 2.302612 TestAcc: 0.04\n",
      "TrainLoss: 2.3038104 TrainAcc: 0.04 TestLoss: 2.3030937 TestAcc: 0.06\n",
      "TrainLoss: 2.3011274 TrainAcc: 0.16 TestLoss: 2.300927 TestAcc: 0.08\n",
      "TrainLoss: 2.3018985 TrainAcc: 0.16 TestLoss: 2.3035812 TestAcc: 0.1\n",
      "TrainLoss: 2.3017738 TrainAcc: 0.08 TestLoss: 2.305854 TestAcc: 0.1\n",
      "TrainLoss: 2.3023322 TrainAcc: 0.1 TestLoss: 2.305179 TestAcc: 0.06\n",
      "TrainLoss: 2.3025675 TrainAcc: 0.1 TestLoss: 2.3015401 TestAcc: 0.1\n",
      "TrainLoss: 2.3008568 TrainAcc: 0.14 TestLoss: 2.2995775 TestAcc: 0.16\n",
      "TrainLoss: 2.3025844 TrainAcc: 0.1 TestLoss: 2.2997258 TestAcc: 0.14\n",
      "TrainLoss: 2.302321 TrainAcc: 0.1 TestLoss: 2.3025875 TestAcc: 0.08\n",
      "TrainLoss: 2.3007362 TrainAcc: 0.18 TestLoss: 2.3048546 TestAcc: 0.06\n",
      "TrainLoss: 2.3055222 TrainAcc: 0.02 TestLoss: 2.3053901 TestAcc: 0.04\n",
      "TrainLoss: 2.3031824 TrainAcc: 0.12 TestLoss: 2.3038545 TestAcc: 0.08\n",
      "TrainLoss: 2.3037853 TrainAcc: 0.1 TestLoss: 2.3014414 TestAcc: 0.14\n",
      "TrainLoss: 2.3007154 TrainAcc: 0.16 TestLoss: 2.3016467 TestAcc: 0.1\n",
      "TrainLoss: 2.3018105 TrainAcc: 0.08 TestLoss: 2.3036997 TestAcc: 0.06\n",
      "TrainLoss: 2.3011076 TrainAcc: 0.12 TestLoss: 2.2987764 TestAcc: 0.16\n",
      "TrainLoss: 2.300667 TrainAcc: 0.14 TestLoss: 2.3016007 TestAcc: 0.14\n",
      "TrainLoss: 2.300878 TrainAcc: 0.12 TestLoss: 2.3033645 TestAcc: 0.08\n",
      "TrainLoss: 2.302683 TrainAcc: 0.06 TestLoss: 2.3030612 TestAcc: 0.14\n",
      "TrainLoss: 2.3046927 TrainAcc: 0.04 TestLoss: 2.3013785 TestAcc: 0.08\n",
      "TrainLoss: 2.2995784 TrainAcc: 0.22 TestLoss: 2.2999477 TestAcc: 0.18\n",
      "TrainLoss: 2.3050272 TrainAcc: 0.08 TestLoss: 2.3027434 TestAcc: 0.14\n",
      "TrainLoss: 2.3008206 TrainAcc: 0.14 TestLoss: 2.3046958 TestAcc: 0.1\n",
      "TrainLoss: 2.3008785 TrainAcc: 0.16 TestLoss: 2.3044329 TestAcc: 0.1\n",
      "TrainLoss: 2.298152 TrainAcc: 0.2 TestLoss: 2.3038747 TestAcc: 0.06\n",
      "TrainLoss: 2.2982438 TrainAcc: 0.18 TestLoss: 2.305777 TestAcc: 0.04\n",
      "TrainLoss: 2.3040507 TrainAcc: 0.08 TestLoss: 2.3037484 TestAcc: 0.1\n",
      "TrainLoss: 2.300762 TrainAcc: 0.18 TestLoss: 2.3034732 TestAcc: 0.14\n",
      "TrainLoss: 2.3031592 TrainAcc: 0.12 TestLoss: 2.3047268 TestAcc: 0.04\n",
      "TrainLoss: 2.3007205 TrainAcc: 0.14 TestLoss: 2.2997935 TestAcc: 0.16\n",
      "TrainLoss: 2.3023965 TrainAcc: 0.08 TestLoss: 2.3045902 TestAcc: 0.08\n",
      "TrainLoss: 2.3023407 TrainAcc: 0.12 TestLoss: 2.3015928 TestAcc: 0.14\n",
      "TrainLoss: 2.302832 TrainAcc: 0.1 TestLoss: 2.303731 TestAcc: 0.06\n",
      "TrainLoss: 2.3040638 TrainAcc: 0.1 TestLoss: 2.3025215 TestAcc: 0.1\n",
      "TrainLoss: 2.3020766 TrainAcc: 0.08 TestLoss: 2.3024876 TestAcc: 0.04\n",
      "TrainLoss: 2.3020856 TrainAcc: 0.12 TestLoss: 2.3021739 TestAcc: 0.1\n",
      "TrainLoss: 2.3008392 TrainAcc: 0.18 TestLoss: 2.302904 TestAcc: 0.1\n",
      "TrainLoss: 2.2999346 TrainAcc: 0.16 TestLoss: 2.3043816 TestAcc: 0.08\n",
      "TrainLoss: 2.3016288 TrainAcc: 0.16 TestLoss: 2.3044555 TestAcc: 0.06\n",
      "TrainLoss: 2.299478 TrainAcc: 0.16 TestLoss: 2.3031547 TestAcc: 0.08\n",
      "TrainLoss: 2.3035083 TrainAcc: 0.12 TestLoss: 2.3052201 TestAcc: 0.06\n",
      "TrainLoss: 2.3030825 TrainAcc: 0.06 TestLoss: 2.302456 TestAcc: 0.06\n",
      "TrainLoss: 2.305774 TrainAcc: 0.06 TestLoss: 2.3023157 TestAcc: 0.06\n",
      "TrainLoss: 2.2994802 TrainAcc: 0.18 TestLoss: 2.3056686 TestAcc: 0.08\n",
      "TrainLoss: 2.3015432 TrainAcc: 0.12 TestLoss: 2.3005428 TestAcc: 0.14\n",
      "TrainLoss: 2.302774 TrainAcc: 0.08 TestLoss: 2.3030462 TestAcc: 0.12\n",
      "TrainLoss: 2.3021038 TrainAcc: 0.1 TestLoss: 2.3027074 TestAcc: 0.06\n",
      "TrainLoss: 2.3029869 TrainAcc: 0.08 TestLoss: 2.3020492 TestAcc: 0.12\n",
      "TrainLoss: 2.3018289 TrainAcc: 0.16 TestLoss: 2.3026953 TestAcc: 0.1\n",
      "TrainLoss: 2.3028684 TrainAcc: 0.1 TestLoss: 2.3025863 TestAcc: 0.1\n",
      "TrainLoss: 2.303193 TrainAcc: 0.12 TestLoss: 2.3021946 TestAcc: 0.12\n",
      "TrainLoss: 2.3010137 TrainAcc: 0.08 TestLoss: 2.302376 TestAcc: 0.12\n",
      "TrainLoss: 2.305502 TrainAcc: 0.02 TestLoss: 2.3029983 TestAcc: 0.08\n",
      "TrainLoss: 2.3056524 TrainAcc: 0.04 TestLoss: 2.298108 TestAcc: 0.2\n",
      "TrainLoss: 2.3036964 TrainAcc: 0.06 TestLoss: 2.3024638 TestAcc: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3024638 TrainAcc: 0.1 TestLoss: 2.303424 TestAcc: 0.08\n",
      "TrainLoss: 2.3032396 TrainAcc: 0.06 TestLoss: 2.3031602 TestAcc: 0.06\n",
      "TrainLoss: 2.302945 TrainAcc: 0.08 TestLoss: 2.3035812 TestAcc: 0.12\n",
      "TrainLoss: 2.3002005 TrainAcc: 0.12 TestLoss: 2.3045702 TestAcc: 0.1\n",
      "TrainLoss: 2.302115 TrainAcc: 0.14 TestLoss: 2.3034947 TestAcc: 0.06\n",
      "TrainLoss: 2.304444 TrainAcc: 0.1 TestLoss: 2.3036833 TestAcc: 0.1\n",
      "TrainLoss: 2.3048124 TrainAcc: 0.06 TestLoss: 2.3044097 TestAcc: 0.04\n",
      "TrainLoss: 2.3000998 TrainAcc: 0.16 TestLoss: 2.3022764 TestAcc: 0.12\n",
      "TrainLoss: 2.3037608 TrainAcc: 0.1 TestLoss: 2.302121 TestAcc: 0.12\n",
      "TrainLoss: 2.3036945 TrainAcc: 0.04 TestLoss: 2.304628 TestAcc: 0.1\n",
      "TrainLoss: 2.3008327 TrainAcc: 0.12 TestLoss: 2.302937 TestAcc: 0.06\n",
      "TrainLoss: 2.3063853 TrainAcc: 0.04 TestLoss: 2.3035324 TestAcc: 0.1\n",
      "TrainLoss: 2.3023005 TrainAcc: 0.08 TestLoss: 2.302157 TestAcc: 0.1\n",
      "TrainLoss: 2.3041081 TrainAcc: 0.04 TestLoss: 2.3035054 TestAcc: 0.02\n",
      "TrainLoss: 2.3016572 TrainAcc: 0.1 TestLoss: 2.3043325 TestAcc: 0.08\n",
      "TrainLoss: 2.3008592 TrainAcc: 0.12 TestLoss: 2.3016684 TestAcc: 0.14\n",
      "TrainLoss: 2.3018353 TrainAcc: 0.1 TestLoss: 2.3041642 TestAcc: 0.08\n",
      "TrainLoss: 2.3017533 TrainAcc: 0.14 TestLoss: 2.3030791 TestAcc: 0.08\n",
      "TrainLoss: 2.30459 TrainAcc: 0.04 TestLoss: 2.302945 TestAcc: 0.1\n",
      "TrainLoss: 2.3004904 TrainAcc: 0.2 TestLoss: 2.3008738 TestAcc: 0.16\n",
      "TrainLoss: 2.303492 TrainAcc: 0.06 TestLoss: 2.3038862 TestAcc: 0.1\n",
      "TrainLoss: 2.3014588 TrainAcc: 0.12 TestLoss: 2.3023233 TestAcc: 0.12\n",
      "TrainLoss: 2.3042917 TrainAcc: 0.08 TestLoss: 2.305738 TestAcc: 0.04\n",
      "TrainLoss: 2.30234 TrainAcc: 0.12 TestLoss: 2.2997015 TestAcc: 0.18\n",
      "TrainLoss: 2.3023205 TrainAcc: 0.1 TestLoss: 2.3003101 TestAcc: 0.12\n",
      "TrainLoss: 2.3016615 TrainAcc: 0.1 TestLoss: 2.297747 TestAcc: 0.16\n",
      "TrainLoss: 2.3035326 TrainAcc: 0.1 TestLoss: 2.3036869 TestAcc: 0.06\n",
      "TrainLoss: 2.3018012 TrainAcc: 0.18 TestLoss: 2.304892 TestAcc: 0.06\n",
      "TrainLoss: 2.3050134 TrainAcc: 0.06 TestLoss: 2.3003728 TestAcc: 0.14\n",
      "TrainLoss: 2.3034947 TrainAcc: 0.1 TestLoss: 2.3044333 TestAcc: 0.08\n",
      "TrainLoss: 2.3052547 TrainAcc: 0.04 TestLoss: 2.305035 TestAcc: 0.04\n",
      "TrainLoss: 2.3034024 TrainAcc: 0.12 TestLoss: 2.3045526 TestAcc: 0.02\n",
      "TrainLoss: 2.3008344 TrainAcc: 0.14 TestLoss: 2.3028176 TestAcc: 0.1\n",
      "TrainLoss: 2.3042753 TrainAcc: 0.02 TestLoss: 2.3065305 TestAcc: 0.02\n",
      "TrainLoss: 2.3012192 TrainAcc: 0.14 TestLoss: 2.3019407 TestAcc: 0.12\n",
      "TrainLoss: 2.3013384 TrainAcc: 0.12 TestLoss: 2.3014574 TestAcc: 0.12\n",
      "TrainLoss: 2.3001165 TrainAcc: 0.14 TestLoss: 2.3027158 TestAcc: 0.12\n",
      "TrainLoss: 2.3037307 TrainAcc: 0.0 TestLoss: 2.3038101 TestAcc: 0.08\n",
      "TrainLoss: 2.3023324 TrainAcc: 0.12 TestLoss: 2.305508 TestAcc: 0.08\n",
      "TrainLoss: 2.3047068 TrainAcc: 0.04 TestLoss: 2.3016508 TestAcc: 0.1\n",
      "TrainLoss: 2.303937 TrainAcc: 0.06 TestLoss: 2.3025274 TestAcc: 0.1\n",
      "TrainLoss: 2.3056352 TrainAcc: 0.04 TestLoss: 2.301939 TestAcc: 0.12\n",
      "TrainLoss: 2.300571 TrainAcc: 0.14 TestLoss: 2.3019924 TestAcc: 0.14\n",
      "TrainLoss: 2.3022869 TrainAcc: 0.12 TestLoss: 2.3025663 TestAcc: 0.12\n",
      "TrainLoss: 2.303364 TrainAcc: 0.08 TestLoss: 2.3012393 TestAcc: 0.08\n",
      "TrainLoss: 2.3024652 TrainAcc: 0.12 TestLoss: 2.3049548 TestAcc: 0.08\n",
      "TrainLoss: 2.302855 TrainAcc: 0.1 TestLoss: 2.3007488 TestAcc: 0.18\n",
      "TrainLoss: 2.3017726 TrainAcc: 0.14 TestLoss: 2.3020556 TestAcc: 0.1\n",
      "TrainLoss: 2.303592 TrainAcc: 0.06 TestLoss: 2.302303 TestAcc: 0.1\n",
      "TrainLoss: 2.3023121 TrainAcc: 0.08 TestLoss: 2.3010473 TestAcc: 0.12\n",
      "TrainLoss: 2.297594 TrainAcc: 0.2 TestLoss: 2.3023005 TestAcc: 0.12\n",
      "TrainLoss: 2.303859 TrainAcc: 0.06 TestLoss: 2.30264 TestAcc: 0.06\n",
      "TrainLoss: 2.3069994 TrainAcc: 0.02 TestLoss: 2.3062713 TestAcc: 0.02\n",
      "TrainLoss: 2.2992513 TrainAcc: 0.16 TestLoss: 2.302952 TestAcc: 0.12\n",
      "TrainLoss: 2.303451 TrainAcc: 0.1 TestLoss: 2.307106 TestAcc: 0.06\n",
      "TrainLoss: 2.304133 TrainAcc: 0.06 TestLoss: 2.302242 TestAcc: 0.14\n",
      "TrainLoss: 2.3022044 TrainAcc: 0.1 TestLoss: 2.3023133 TestAcc: 0.08\n",
      "TrainLoss: 2.3004167 TrainAcc: 0.14 TestLoss: 2.3030992 TestAcc: 0.06\n",
      "TrainLoss: 2.3056958 TrainAcc: 0.04 TestLoss: 2.3060331 TestAcc: 0.02\n",
      "TrainLoss: 2.3008685 TrainAcc: 0.14 TestLoss: 2.301959 TestAcc: 0.06\n",
      "TrainLoss: 2.3029218 TrainAcc: 0.12 TestLoss: 2.3032842 TestAcc: 0.1\n",
      "TrainLoss: 2.303559 TrainAcc: 0.06 TestLoss: 2.3028078 TestAcc: 0.12\n",
      "TrainLoss: 2.3031332 TrainAcc: 0.06 TestLoss: 2.3022308 TestAcc: 0.1\n",
      "TrainLoss: 2.3009627 TrainAcc: 0.14 TestLoss: 2.302658 TestAcc: 0.14\n",
      "TrainLoss: 2.3023624 TrainAcc: 0.14 TestLoss: 2.301711 TestAcc: 0.12\n",
      "TrainLoss: 2.3020077 TrainAcc: 0.1 TestLoss: 2.2980573 TestAcc: 0.18\n",
      "TrainLoss: 2.3034751 TrainAcc: 0.06 TestLoss: 2.3019176 TestAcc: 0.14\n",
      "TrainLoss: 2.3022544 TrainAcc: 0.1 TestLoss: 2.303451 TestAcc: 0.08\n",
      "TrainLoss: 2.3033738 TrainAcc: 0.08 TestLoss: 2.2987971 TestAcc: 0.22\n",
      "TrainLoss: 2.304195 TrainAcc: 0.12 TestLoss: 2.301858 TestAcc: 0.14\n",
      "TrainLoss: 2.3022895 TrainAcc: 0.14 TestLoss: 2.300002 TestAcc: 0.12\n",
      "TrainLoss: 2.3019373 TrainAcc: 0.14 TestLoss: 2.3022087 TestAcc: 0.1\n",
      "TrainLoss: 2.3035285 TrainAcc: 0.06 TestLoss: 2.2987702 TestAcc: 0.16\n",
      "TrainLoss: 2.3038778 TrainAcc: 0.08 TestLoss: 2.3007429 TestAcc: 0.18\n",
      "TrainLoss: 2.3035505 TrainAcc: 0.02 TestLoss: 2.3026297 TestAcc: 0.08\n",
      "TrainLoss: 2.3034012 TrainAcc: 0.06 TestLoss: 2.3021526 TestAcc: 0.12\n",
      "TrainLoss: 2.3027701 TrainAcc: 0.1 TestLoss: 2.3045154 TestAcc: 0.08\n",
      "TrainLoss: 2.302477 TrainAcc: 0.04 TestLoss: 2.3025827 TestAcc: 0.08\n",
      "TrainLoss: 2.3019986 TrainAcc: 0.08 TestLoss: 2.302619 TestAcc: 0.12\n",
      "TrainLoss: 2.3010993 TrainAcc: 0.16 TestLoss: 2.3023095 TestAcc: 0.14\n",
      "TrainLoss: 2.3027275 TrainAcc: 0.06 TestLoss: 2.3018687 TestAcc: 0.1\n",
      "TrainLoss: 2.3011706 TrainAcc: 0.16 TestLoss: 2.3040526 TestAcc: 0.02\n",
      "TrainLoss: 2.302339 TrainAcc: 0.04 TestLoss: 2.3046114 TestAcc: 0.08\n",
      "TrainLoss: 2.3042161 TrainAcc: 0.04 TestLoss: 2.3033154 TestAcc: 0.08\n",
      "TrainLoss: 2.300347 TrainAcc: 0.14 TestLoss: 2.3001032 TestAcc: 0.12\n",
      "TrainLoss: 2.3004055 TrainAcc: 0.2 TestLoss: 2.3038516 TestAcc: 0.08\n",
      "TrainLoss: 2.2999778 TrainAcc: 0.26 TestLoss: 2.305346 TestAcc: 0.06\n",
      "TrainLoss: 2.3002799 TrainAcc: 0.18 TestLoss: 2.30231 TestAcc: 0.1\n",
      "TrainLoss: 2.298528 TrainAcc: 0.14 TestLoss: 2.3038874 TestAcc: 0.04\n",
      "TrainLoss: 2.3016238 TrainAcc: 0.1 TestLoss: 2.3047237 TestAcc: 0.02\n",
      "TrainLoss: 2.3040779 TrainAcc: 0.1 TestLoss: 2.3042731 TestAcc: 0.06\n",
      "TrainLoss: 2.301922 TrainAcc: 0.14 TestLoss: 2.3011498 TestAcc: 0.12\n",
      "TrainLoss: 2.300784 TrainAcc: 0.12 TestLoss: 2.3026817 TestAcc: 0.1\n",
      "TrainLoss: 2.3042889 TrainAcc: 0.06 TestLoss: 2.300759 TestAcc: 0.14\n",
      "TrainLoss: 2.303743 TrainAcc: 0.1 TestLoss: 2.302315 TestAcc: 0.08\n",
      "TrainLoss: 2.3022215 TrainAcc: 0.14 TestLoss: 2.3020873 TestAcc: 0.08\n",
      "TrainLoss: 2.301889 TrainAcc: 0.12 TestLoss: 2.3018856 TestAcc: 0.12\n",
      "TrainLoss: 2.3050516 TrainAcc: 0.06 TestLoss: 2.3025658 TestAcc: 0.12\n",
      "TrainLoss: 2.3002481 TrainAcc: 0.14 TestLoss: 2.302712 TestAcc: 0.16\n",
      "TrainLoss: 2.3027525 TrainAcc: 0.08 TestLoss: 2.2995949 TestAcc: 0.2\n",
      "TrainLoss: 2.3018115 TrainAcc: 0.14 TestLoss: 2.301373 TestAcc: 0.12\n",
      "TrainLoss: 2.3035889 TrainAcc: 0.1 TestLoss: 2.3024201 TestAcc: 0.06\n",
      "TrainLoss: 2.3035204 TrainAcc: 0.06 TestLoss: 2.3043044 TestAcc: 0.08\n",
      "TrainLoss: 2.3008902 TrainAcc: 0.2 TestLoss: 2.301807 TestAcc: 0.1\n",
      "TrainLoss: 2.3012693 TrainAcc: 0.14 TestLoss: 2.3015068 TestAcc: 0.2\n",
      "TrainLoss: 2.3031533 TrainAcc: 0.06 TestLoss: 2.3028862 TestAcc: 0.08\n",
      "TrainLoss: 2.3036294 TrainAcc: 0.1 TestLoss: 2.3016264 TestAcc: 0.16\n",
      "TrainLoss: 2.301253 TrainAcc: 0.16 TestLoss: 2.3061187 TestAcc: 0.06\n",
      "TrainLoss: 2.3008795 TrainAcc: 0.16 TestLoss: 2.303184 TestAcc: 0.06\n",
      "TrainLoss: 2.3025649 TrainAcc: 0.1 TestLoss: 2.3059902 TestAcc: 0.06\n",
      "TrainLoss: 2.3016834 TrainAcc: 0.08 TestLoss: 2.3019197 TestAcc: 0.16\n",
      "TrainLoss: 2.3030775 TrainAcc: 0.08 TestLoss: 2.3048372 TestAcc: 0.04\n",
      "TrainLoss: 2.3034458 TrainAcc: 0.1 TestLoss: 2.3032649 TestAcc: 0.1\n",
      "TrainLoss: 2.3002162 TrainAcc: 0.12 TestLoss: 2.3053215 TestAcc: 0.04\n",
      "TrainLoss: 2.3013985 TrainAcc: 0.12 TestLoss: 2.3037245 TestAcc: 0.1\n",
      "TrainLoss: 2.30231 TrainAcc: 0.06 TestLoss: 2.30362 TestAcc: 0.08\n",
      "TrainLoss: 2.3040614 TrainAcc: 0.06 TestLoss: 2.3023267 TestAcc: 0.06\n",
      "TrainLoss: 2.3024108 TrainAcc: 0.08 TestLoss: 2.3031573 TestAcc: 0.06\n",
      "TrainLoss: 2.303116 TrainAcc: 0.06 TestLoss: 2.3032818 TestAcc: 0.1\n",
      "TrainLoss: 2.3007832 TrainAcc: 0.14 TestLoss: 2.303927 TestAcc: 0.06\n",
      "TrainLoss: 2.3034732 TrainAcc: 0.06 TestLoss: 2.2983675 TestAcc: 0.2\n",
      "TrainLoss: 2.3006322 TrainAcc: 0.12 TestLoss: 2.3057177 TestAcc: 0.02\n",
      "TrainLoss: 2.302204 TrainAcc: 0.12 TestLoss: 2.302364 TestAcc: 0.08\n",
      "TrainLoss: 2.3018258 TrainAcc: 0.1 TestLoss: 2.3040178 TestAcc: 0.04\n",
      "TrainLoss: 2.303243 TrainAcc: 0.08 TestLoss: 2.302552 TestAcc: 0.1\n",
      "TrainLoss: 2.3011155 TrainAcc: 0.12 TestLoss: 2.3043418 TestAcc: 0.04\n",
      "TrainLoss: 2.3024635 TrainAcc: 0.08 TestLoss: 2.3033335 TestAcc: 0.08\n",
      "TrainLoss: 2.3013906 TrainAcc: 0.14 TestLoss: 2.3026104 TestAcc: 0.1\n",
      "TrainLoss: 2.3039064 TrainAcc: 0.12 TestLoss: 2.3008313 TestAcc: 0.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3025732 TrainAcc: 0.12 TestLoss: 2.305032 TestAcc: 0.04\n",
      "TrainLoss: 2.3044598 TrainAcc: 0.08 TestLoss: 2.3050613 TestAcc: 0.06\n",
      "TrainLoss: 2.3018885 TrainAcc: 0.08 TestLoss: 2.3014734 TestAcc: 0.1\n",
      "TrainLoss: 2.303343 TrainAcc: 0.1 TestLoss: 2.3048313 TestAcc: 0.08\n",
      "TrainLoss: 2.299552 TrainAcc: 0.16 TestLoss: 2.305404 TestAcc: 0.06\n",
      "TrainLoss: 2.3038526 TrainAcc: 0.08 TestLoss: 2.2983625 TestAcc: 0.2\n",
      "TrainLoss: 2.30223 TrainAcc: 0.1 TestLoss: 2.301507 TestAcc: 0.08\n",
      "TrainLoss: 2.3049195 TrainAcc: 0.08 TestLoss: 2.3006897 TestAcc: 0.1\n",
      "TrainLoss: 2.3009734 TrainAcc: 0.08 TestLoss: 2.302408 TestAcc: 0.06\n",
      "TrainLoss: 2.3029578 TrainAcc: 0.1 TestLoss: 2.3040614 TestAcc: 0.06\n",
      "TrainLoss: 2.3018374 TrainAcc: 0.14 TestLoss: 2.300452 TestAcc: 0.14\n",
      "TrainLoss: 2.3039813 TrainAcc: 0.04 TestLoss: 2.3014116 TestAcc: 0.1\n",
      "TrainLoss: 2.3019097 TrainAcc: 0.1 TestLoss: 2.3010612 TestAcc: 0.12\n",
      "TrainLoss: 2.3025181 TrainAcc: 0.08 TestLoss: 2.3021793 TestAcc: 0.12\n",
      "TrainLoss: 2.304102 TrainAcc: 0.04 TestLoss: 2.3035946 TestAcc: 0.08\n",
      "TrainLoss: 2.3039808 TrainAcc: 0.06 TestLoss: 2.302945 TestAcc: 0.1\n",
      "TrainLoss: 2.3017223 TrainAcc: 0.06 TestLoss: 2.3045034 TestAcc: 0.04\n",
      "TrainLoss: 2.2993963 TrainAcc: 0.16 TestLoss: 2.3053894 TestAcc: 0.06\n",
      "TrainLoss: 2.3004022 TrainAcc: 0.06 TestLoss: 2.301728 TestAcc: 0.12\n",
      "TrainLoss: 2.304564 TrainAcc: 0.1 TestLoss: 2.3030577 TestAcc: 0.14\n",
      "TrainLoss: 2.303089 TrainAcc: 0.08 TestLoss: 2.3006907 TestAcc: 0.12\n",
      "TrainLoss: 2.3006938 TrainAcc: 0.18 TestLoss: 2.303701 TestAcc: 0.06\n",
      "TrainLoss: 2.3037772 TrainAcc: 0.04 TestLoss: 2.3022718 TestAcc: 0.08\n",
      "TrainLoss: 2.3013897 TrainAcc: 0.1 TestLoss: 2.3007343 TestAcc: 0.12\n",
      "TrainLoss: 2.3003216 TrainAcc: 0.14 TestLoss: 2.3039181 TestAcc: 0.04\n",
      "TrainLoss: 2.3029466 TrainAcc: 0.1 TestLoss: 2.3043127 TestAcc: 0.06\n",
      "TrainLoss: 2.304886 TrainAcc: 0.08 TestLoss: 2.3018055 TestAcc: 0.1\n",
      "TrainLoss: 2.304007 TrainAcc: 0.06 TestLoss: 2.302888 TestAcc: 0.14\n",
      "TrainLoss: 2.302212 TrainAcc: 0.12 TestLoss: 2.3047743 TestAcc: 0.06\n",
      "TrainLoss: 2.3029866 TrainAcc: 0.08 TestLoss: 2.3042557 TestAcc: 0.08\n",
      "TrainLoss: 2.301455 TrainAcc: 0.1 TestLoss: 2.3021827 TestAcc: 0.16\n",
      "TrainLoss: 2.302404 TrainAcc: 0.12 TestLoss: 2.304608 TestAcc: 0.04\n",
      "TrainLoss: 2.300282 TrainAcc: 0.14 TestLoss: 2.3031187 TestAcc: 0.12\n",
      "TrainLoss: 2.30102 TrainAcc: 0.12 TestLoss: 2.3040533 TestAcc: 0.14\n",
      "TrainLoss: 2.2997706 TrainAcc: 0.12 TestLoss: 2.2999346 TestAcc: 0.18\n",
      "TrainLoss: 2.3040967 TrainAcc: 0.08 TestLoss: 2.3012273 TestAcc: 0.08\n",
      "TrainLoss: 2.3011148 TrainAcc: 0.18 TestLoss: 2.3000205 TestAcc: 0.22\n",
      "TrainLoss: 2.3024192 TrainAcc: 0.16 TestLoss: 2.3026567 TestAcc: 0.1\n",
      "TrainLoss: 2.3018658 TrainAcc: 0.16 TestLoss: 2.3020325 TestAcc: 0.12\n",
      "TrainLoss: 2.2994967 TrainAcc: 0.14 TestLoss: 2.303094 TestAcc: 0.08\n",
      "TrainLoss: 2.3012857 TrainAcc: 0.16 TestLoss: 2.3044462 TestAcc: 0.1\n",
      "TrainLoss: 2.3037603 TrainAcc: 0.06 TestLoss: 2.3034692 TestAcc: 0.12\n",
      "TrainLoss: 2.3016548 TrainAcc: 0.1 TestLoss: 2.3031542 TestAcc: 0.06\n",
      "TrainLoss: 2.3010736 TrainAcc: 0.16 TestLoss: 2.3042283 TestAcc: 0.1\n",
      "TrainLoss: 2.2994556 TrainAcc: 0.22 TestLoss: 2.2986643 TestAcc: 0.14\n",
      "TrainLoss: 2.3031447 TrainAcc: 0.04 TestLoss: 2.302078 TestAcc: 0.16\n",
      "TrainLoss: 2.3057182 TrainAcc: 0.06 TestLoss: 2.3040936 TestAcc: 0.08\n",
      "TrainLoss: 2.302579 TrainAcc: 0.12 TestLoss: 2.3040552 TestAcc: 0.04\n",
      "TrainLoss: 2.3003702 TrainAcc: 0.18 TestLoss: 2.3025188 TestAcc: 0.1\n",
      "TrainLoss: 2.2996922 TrainAcc: 0.16 TestLoss: 2.3007822 TestAcc: 0.14\n",
      "TrainLoss: 2.3032835 TrainAcc: 0.06 TestLoss: 2.2983427 TestAcc: 0.2\n",
      "TrainLoss: 2.3019989 TrainAcc: 0.08 TestLoss: 2.305211 TestAcc: 0.02\n",
      "TrainLoss: 2.304795 TrainAcc: 0.06 TestLoss: 2.3064442 TestAcc: 0.02\n",
      "TrainLoss: 2.3036256 TrainAcc: 0.1 TestLoss: 2.3034458 TestAcc: 0.08\n",
      "TrainLoss: 2.3023858 TrainAcc: 0.1 TestLoss: 2.3046136 TestAcc: 0.08\n",
      "TrainLoss: 2.3041034 TrainAcc: 0.1 TestLoss: 2.3043206 TestAcc: 0.06\n",
      "TrainLoss: 2.3062778 TrainAcc: 0.04 TestLoss: 2.3035166 TestAcc: 0.08\n",
      "TrainLoss: 2.3036222 TrainAcc: 0.04 TestLoss: 2.3025331 TestAcc: 0.08\n",
      "TrainLoss: 2.3027232 TrainAcc: 0.08 TestLoss: 2.3006763 TestAcc: 0.16\n",
      "TrainLoss: 2.3026328 TrainAcc: 0.12 TestLoss: 2.3033152 TestAcc: 0.08\n",
      "TrainLoss: 2.301539 TrainAcc: 0.12 TestLoss: 2.301414 TestAcc: 0.08\n",
      "TrainLoss: 2.3029346 TrainAcc: 0.12 TestLoss: 2.303773 TestAcc: 0.08\n",
      "TrainLoss: 2.3037698 TrainAcc: 0.04 TestLoss: 2.3057232 TestAcc: 0.02\n",
      "TrainLoss: 2.3048887 TrainAcc: 0.04 TestLoss: 2.301134 TestAcc: 0.16\n",
      "TrainLoss: 2.3036554 TrainAcc: 0.06 TestLoss: 2.301406 TestAcc: 0.16\n",
      "TrainLoss: 2.3009338 TrainAcc: 0.12 TestLoss: 2.3012562 TestAcc: 0.14\n",
      "TrainLoss: 2.3033445 TrainAcc: 0.06 TestLoss: 2.3015957 TestAcc: 0.14\n",
      "TrainLoss: 2.3029091 TrainAcc: 0.12 TestLoss: 2.3022735 TestAcc: 0.08\n",
      "TrainLoss: 2.3028116 TrainAcc: 0.1 TestLoss: 2.300641 TestAcc: 0.14\n",
      "TrainLoss: 2.30334 TrainAcc: 0.1 TestLoss: 2.2983823 TestAcc: 0.18\n",
      "TrainLoss: 2.302903 TrainAcc: 0.12 TestLoss: 2.3032565 TestAcc: 0.06\n",
      "TrainLoss: 2.3041582 TrainAcc: 0.08 TestLoss: 2.3044047 TestAcc: 0.06\n",
      "TrainLoss: 2.300047 TrainAcc: 0.16 TestLoss: 2.3057137 TestAcc: 0.06\n",
      "TrainLoss: 2.3026457 TrainAcc: 0.1 TestLoss: 2.3028457 TestAcc: 0.06\n",
      "TrainLoss: 2.3061357 TrainAcc: 0.06 TestLoss: 2.3019204 TestAcc: 0.14\n",
      "TrainLoss: 2.3034809 TrainAcc: 0.04 TestLoss: 2.3046265 TestAcc: 0.06\n",
      "TrainLoss: 2.3030708 TrainAcc: 0.14 TestLoss: 2.3021297 TestAcc: 0.1\n",
      "TrainLoss: 2.299943 TrainAcc: 0.12 TestLoss: 2.302728 TestAcc: 0.04\n",
      "TrainLoss: 2.302874 TrainAcc: 0.06 TestLoss: 2.303878 TestAcc: 0.08\n",
      "TrainLoss: 2.3030195 TrainAcc: 0.08 TestLoss: 2.3028526 TestAcc: 0.08\n",
      "TrainLoss: 2.3037553 TrainAcc: 0.06 TestLoss: 2.3026779 TestAcc: 0.12\n",
      "TrainLoss: 2.3012161 TrainAcc: 0.18 TestLoss: 2.3041112 TestAcc: 0.02\n",
      "TrainLoss: 2.3021214 TrainAcc: 0.06 TestLoss: 2.3016114 TestAcc: 0.04\n",
      "TrainLoss: 2.3014135 TrainAcc: 0.12 TestLoss: 2.3040078 TestAcc: 0.12\n",
      "TrainLoss: 2.3031654 TrainAcc: 0.1 TestLoss: 2.3015716 TestAcc: 0.12\n",
      "TrainLoss: 2.3026505 TrainAcc: 0.1 TestLoss: 2.3041172 TestAcc: 0.02\n",
      "TrainLoss: 2.3032024 TrainAcc: 0.08 TestLoss: 2.303915 TestAcc: 0.06\n",
      "TrainLoss: 2.3024824 TrainAcc: 0.1 TestLoss: 2.3034632 TestAcc: 0.08\n",
      "TrainLoss: 2.3037114 TrainAcc: 0.06 TestLoss: 2.3022604 TestAcc: 0.1\n",
      "TrainLoss: 2.3018134 TrainAcc: 0.12 TestLoss: 2.3026192 TestAcc: 0.08\n",
      "TrainLoss: 2.3008804 TrainAcc: 0.1 TestLoss: 2.303355 TestAcc: 0.1\n",
      "TrainLoss: 2.3015075 TrainAcc: 0.14 TestLoss: 2.3047361 TestAcc: 0.04\n",
      "TrainLoss: 2.3006124 TrainAcc: 0.12 TestLoss: 2.303334 TestAcc: 0.06\n",
      "TrainLoss: 2.3038578 TrainAcc: 0.06 TestLoss: 2.3039997 TestAcc: 0.06\n",
      "TrainLoss: 2.3034961 TrainAcc: 0.14 TestLoss: 2.301335 TestAcc: 0.16\n",
      "TrainLoss: 2.3025236 TrainAcc: 0.1 TestLoss: 2.302127 TestAcc: 0.1\n",
      "TrainLoss: 2.302089 TrainAcc: 0.14 TestLoss: 2.3058639 TestAcc: 0.02\n",
      "TrainLoss: 2.302191 TrainAcc: 0.1 TestLoss: 2.300696 TestAcc: 0.16\n",
      "TrainLoss: 2.302301 TrainAcc: 0.12 TestLoss: 2.3022122 TestAcc: 0.08\n",
      "TrainLoss: 2.2995477 TrainAcc: 0.16 TestLoss: 2.3027306 TestAcc: 0.08\n",
      "TrainLoss: 2.2996392 TrainAcc: 0.14 TestLoss: 2.3057535 TestAcc: 0.02\n",
      "TrainLoss: 2.3039973 TrainAcc: 0.06 TestLoss: 2.3043296 TestAcc: 0.1\n",
      "TrainLoss: 2.301691 TrainAcc: 0.1 TestLoss: 2.3019896 TestAcc: 0.16\n",
      "TrainLoss: 2.3027315 TrainAcc: 0.02 TestLoss: 2.3036466 TestAcc: 0.06\n",
      "TrainLoss: 2.303253 TrainAcc: 0.08 TestLoss: 2.300781 TestAcc: 0.12\n",
      "TrainLoss: 2.2998247 TrainAcc: 0.14 TestLoss: 2.3047662 TestAcc: 0.04\n",
      "TrainLoss: 2.3017983 TrainAcc: 0.04 TestLoss: 2.3025866 TestAcc: 0.08\n",
      "TrainLoss: 2.301551 TrainAcc: 0.06 TestLoss: 2.3021379 TestAcc: 0.1\n",
      "TrainLoss: 2.3042336 TrainAcc: 0.06 TestLoss: 2.30116 TestAcc: 0.08\n",
      "TrainLoss: 2.3036406 TrainAcc: 0.08 TestLoss: 2.3023167 TestAcc: 0.1\n",
      "TrainLoss: 2.3032386 TrainAcc: 0.04 TestLoss: 2.302638 TestAcc: 0.06\n",
      "TrainLoss: 2.304411 TrainAcc: 0.06 TestLoss: 2.3047302 TestAcc: 0.06\n",
      "TrainLoss: 2.30332 TrainAcc: 0.1 TestLoss: 2.303093 TestAcc: 0.08\n",
      "TrainLoss: 2.3006148 TrainAcc: 0.18 TestLoss: 2.3023348 TestAcc: 0.16\n",
      "TrainLoss: 2.3018205 TrainAcc: 0.08 TestLoss: 2.3020234 TestAcc: 0.14\n",
      "TrainLoss: 2.301583 TrainAcc: 0.1 TestLoss: 2.3021145 TestAcc: 0.1\n",
      "TrainLoss: 2.3006332 TrainAcc: 0.14 TestLoss: 2.3044124 TestAcc: 0.1\n",
      "TrainLoss: 2.3026872 TrainAcc: 0.14 TestLoss: 2.3011043 TestAcc: 0.16\n",
      "TrainLoss: 2.3022227 TrainAcc: 0.1 TestLoss: 2.3038414 TestAcc: 0.08\n",
      "TrainLoss: 2.3047652 TrainAcc: 0.06 TestLoss: 2.3026512 TestAcc: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3033116 TrainAcc: 0.1 TestLoss: 2.3027604 TestAcc: 0.12\n",
      "TrainLoss: 2.3015966 TrainAcc: 0.14 TestLoss: 2.3011703 TestAcc: 0.14\n",
      "TrainLoss: 2.2997177 TrainAcc: 0.18 TestLoss: 2.298987 TestAcc: 0.14\n",
      "TrainLoss: 2.3034906 TrainAcc: 0.08 TestLoss: 2.301183 TestAcc: 0.1\n",
      "TrainLoss: 2.3023713 TrainAcc: 0.1 TestLoss: 2.3021727 TestAcc: 0.1\n",
      "TrainLoss: 2.301507 TrainAcc: 0.1 TestLoss: 2.304729 TestAcc: 0.08\n",
      "TrainLoss: 2.3032928 TrainAcc: 0.06 TestLoss: 2.305436 TestAcc: 0.04\n",
      "TrainLoss: 2.3021286 TrainAcc: 0.14 TestLoss: 2.3015113 TestAcc: 0.08\n",
      "TrainLoss: 2.3012106 TrainAcc: 0.04 TestLoss: 2.3033946 TestAcc: 0.08\n",
      "TrainLoss: 2.3069456 TrainAcc: 0.06 TestLoss: 2.3035843 TestAcc: 0.08\n",
      "TrainLoss: 2.3042521 TrainAcc: 0.06 TestLoss: 2.3038397 TestAcc: 0.12\n",
      "TrainLoss: 2.303671 TrainAcc: 0.08 TestLoss: 2.3028286 TestAcc: 0.06\n",
      "TrainLoss: 2.3042138 TrainAcc: 0.1 TestLoss: 2.302657 TestAcc: 0.08\n",
      "TrainLoss: 2.3016481 TrainAcc: 0.16 TestLoss: 2.3031464 TestAcc: 0.06\n",
      "TrainLoss: 2.3033807 TrainAcc: 0.1 TestLoss: 2.3050382 TestAcc: 0.04\n",
      "TrainLoss: 2.3039443 TrainAcc: 0.08 TestLoss: 2.300579 TestAcc: 0.14\n",
      "TrainLoss: 2.302819 TrainAcc: 0.08 TestLoss: 2.300806 TestAcc: 0.1\n",
      "TrainLoss: 2.301295 TrainAcc: 0.16 TestLoss: 2.3028345 TestAcc: 0.08\n",
      "TrainLoss: 2.3021405 TrainAcc: 0.1 TestLoss: 2.3052323 TestAcc: 0.02\n",
      "TrainLoss: 2.3023431 TrainAcc: 0.16 TestLoss: 2.3015723 TestAcc: 0.12\n",
      "TrainLoss: 2.300929 TrainAcc: 0.1 TestLoss: 2.302411 TestAcc: 0.14\n",
      "TrainLoss: 2.3033571 TrainAcc: 0.06 TestLoss: 2.3033826 TestAcc: 0.1\n",
      "TrainLoss: 2.3028731 TrainAcc: 0.08 TestLoss: 2.3025756 TestAcc: 0.1\n",
      "TrainLoss: 2.3024223 TrainAcc: 0.14 TestLoss: 2.3015792 TestAcc: 0.1\n",
      "TrainLoss: 2.3006284 TrainAcc: 0.08 TestLoss: 2.3025897 TestAcc: 0.1\n",
      "TrainLoss: 2.303883 TrainAcc: 0.12 TestLoss: 2.303389 TestAcc: 0.04\n",
      "TrainLoss: 2.3030932 TrainAcc: 0.02 TestLoss: 2.3060687 TestAcc: 0.02\n",
      "TrainLoss: 2.2986884 TrainAcc: 0.16 TestLoss: 2.303962 TestAcc: 0.06\n",
      "TrainLoss: 2.3021493 TrainAcc: 0.1 TestLoss: 2.3029084 TestAcc: 0.1\n",
      "TrainLoss: 2.301493 TrainAcc: 0.1 TestLoss: 2.3024833 TestAcc: 0.16\n",
      "TrainLoss: 2.303954 TrainAcc: 0.06 TestLoss: 2.303663 TestAcc: 0.06\n",
      "TrainLoss: 2.3015435 TrainAcc: 0.14 TestLoss: 2.302728 TestAcc: 0.12\n",
      "TrainLoss: 2.3045356 TrainAcc: 0.06 TestLoss: 2.3001573 TestAcc: 0.12\n",
      "TrainLoss: 2.3025517 TrainAcc: 0.1 TestLoss: 2.3064992 TestAcc: 0.04\n",
      "TrainLoss: 2.300525 TrainAcc: 0.1 TestLoss: 2.3044806 TestAcc: 0.04\n",
      "TrainLoss: 2.3032475 TrainAcc: 0.06 TestLoss: 2.300798 TestAcc: 0.16\n",
      "TrainLoss: 2.302737 TrainAcc: 0.08 TestLoss: 2.302645 TestAcc: 0.08\n",
      "TrainLoss: 2.3031597 TrainAcc: 0.08 TestLoss: 2.302223 TestAcc: 0.04\n",
      "TrainLoss: 2.302266 TrainAcc: 0.12 TestLoss: 2.3068497 TestAcc: 0.02\n",
      "TrainLoss: 2.300303 TrainAcc: 0.18 TestLoss: 2.3021874 TestAcc: 0.06\n",
      "TrainLoss: 2.3004487 TrainAcc: 0.14 TestLoss: 2.3027332 TestAcc: 0.08\n",
      "TrainLoss: 2.301945 TrainAcc: 0.06 TestLoss: 2.3015273 TestAcc: 0.16\n",
      "TrainLoss: 2.3013756 TrainAcc: 0.1 TestLoss: 2.3038752 TestAcc: 0.12\n",
      "TrainLoss: 2.3006155 TrainAcc: 0.12 TestLoss: 2.303711 TestAcc: 0.04\n",
      "TrainLoss: 2.3003817 TrainAcc: 0.12 TestLoss: 2.3002234 TestAcc: 0.16\n",
      "TrainLoss: 2.3014417 TrainAcc: 0.1 TestLoss: 2.301915 TestAcc: 0.08\n",
      "TrainLoss: 2.3002431 TrainAcc: 0.14 TestLoss: 2.3054295 TestAcc: 0.04\n",
      "TrainLoss: 2.3052108 TrainAcc: 0.06 TestLoss: 2.3044553 TestAcc: 0.08\n",
      "TrainLoss: 2.3009357 TrainAcc: 0.18 TestLoss: 2.303966 TestAcc: 0.04\n",
      "TrainLoss: 2.3004913 TrainAcc: 0.08 TestLoss: 2.3022387 TestAcc: 0.1\n",
      "TrainLoss: 2.3038404 TrainAcc: 0.04 TestLoss: 2.3048437 TestAcc: 0.1\n",
      "TrainLoss: 2.3009756 TrainAcc: 0.14 TestLoss: 2.3041363 TestAcc: 0.1\n",
      "TrainLoss: 2.301635 TrainAcc: 0.08 TestLoss: 2.3070655 TestAcc: 0.02\n",
      "TrainLoss: 2.302847 TrainAcc: 0.04 TestLoss: 2.3010972 TestAcc: 0.12\n",
      "TrainLoss: 2.2997084 TrainAcc: 0.14 TestLoss: 2.3015053 TestAcc: 0.12\n",
      "TrainLoss: 2.3041482 TrainAcc: 0.06 TestLoss: 2.3023875 TestAcc: 0.06\n",
      "TrainLoss: 2.3024971 TrainAcc: 0.06 TestLoss: 2.3019412 TestAcc: 0.04\n",
      "TrainLoss: 2.3020651 TrainAcc: 0.12 TestLoss: 2.3029351 TestAcc: 0.08\n",
      "TrainLoss: 2.3015432 TrainAcc: 0.12 TestLoss: 2.3010507 TestAcc: 0.08\n",
      "TrainLoss: 2.3014863 TrainAcc: 0.08 TestLoss: 2.3002877 TestAcc: 0.08\n",
      "TrainLoss: 2.3011854 TrainAcc: 0.08 TestLoss: 2.3009546 TestAcc: 0.14\n",
      "TrainLoss: 2.3003206 TrainAcc: 0.12 TestLoss: 2.3030088 TestAcc: 0.1\n",
      "TrainLoss: 2.3041093 TrainAcc: 0.08 TestLoss: 2.3031533 TestAcc: 0.12\n",
      "TrainLoss: 2.3015306 TrainAcc: 0.1 TestLoss: 2.30384 TestAcc: 0.06\n",
      "TrainLoss: 2.302226 TrainAcc: 0.1 TestLoss: 2.3058457 TestAcc: 0.08\n",
      "TrainLoss: 2.3013942 TrainAcc: 0.14 TestLoss: 2.303207 TestAcc: 0.06\n",
      "TrainLoss: 2.3028712 TrainAcc: 0.06 TestLoss: 2.3031642 TestAcc: 0.02\n",
      "TrainLoss: 2.3022747 TrainAcc: 0.12 TestLoss: 2.3065739 TestAcc: 0.02\n",
      "TrainLoss: 2.3015783 TrainAcc: 0.12 TestLoss: 2.3051276 TestAcc: 0.04\n",
      "TrainLoss: 2.3040304 TrainAcc: 0.08 TestLoss: 2.3044927 TestAcc: 0.12\n",
      "TrainLoss: 2.3012981 TrainAcc: 0.14 TestLoss: 2.305677 TestAcc: 0.08\n",
      "TrainLoss: 2.3013995 TrainAcc: 0.16 TestLoss: 2.3013978 TestAcc: 0.08\n",
      "TrainLoss: 2.3035188 TrainAcc: 0.06 TestLoss: 2.303935 TestAcc: 0.06\n",
      "TrainLoss: 2.301632 TrainAcc: 0.08 TestLoss: 2.3067517 TestAcc: 0.06\n",
      "TrainLoss: 2.3046913 TrainAcc: 0.06 TestLoss: 2.305623 TestAcc: 0.12\n",
      "TrainLoss: 2.3020546 TrainAcc: 0.12 TestLoss: 2.3023639 TestAcc: 0.1\n",
      "TrainLoss: 2.2992063 TrainAcc: 0.08 TestLoss: 2.3034775 TestAcc: 0.1\n",
      "TrainLoss: 2.3030694 TrainAcc: 0.06 TestLoss: 2.30013 TestAcc: 0.18\n",
      "TrainLoss: 2.301004 TrainAcc: 0.14 TestLoss: 2.3041759 TestAcc: 0.04\n",
      "TrainLoss: 2.3054 TrainAcc: 0.06 TestLoss: 2.3012872 TestAcc: 0.08\n",
      "TrainLoss: 2.297238 TrainAcc: 0.2 TestLoss: 2.3026984 TestAcc: 0.12\n",
      "TrainLoss: 2.3016868 TrainAcc: 0.12 TestLoss: 2.3035555 TestAcc: 0.04\n",
      "TrainLoss: 2.304636 TrainAcc: 0.08 TestLoss: 2.3014452 TestAcc: 0.06\n",
      "TrainLoss: 2.2990742 TrainAcc: 0.12 TestLoss: 2.3038049 TestAcc: 0.06\n",
      "TrainLoss: 2.3036072 TrainAcc: 0.14 TestLoss: 2.3029253 TestAcc: 0.06\n",
      "TrainLoss: 2.3034432 TrainAcc: 0.08 TestLoss: 2.3054314 TestAcc: 0.06\n",
      "TrainLoss: 2.3038402 TrainAcc: 0.08 TestLoss: 2.305234 TestAcc: 0.1\n",
      "TrainLoss: 2.3015082 TrainAcc: 0.08 TestLoss: 2.3013234 TestAcc: 0.18\n",
      "TrainLoss: 2.299131 TrainAcc: 0.1 TestLoss: 2.3014514 TestAcc: 0.08\n",
      "TrainLoss: 2.3026857 TrainAcc: 0.08 TestLoss: 2.301617 TestAcc: 0.14\n",
      "TrainLoss: 2.299483 TrainAcc: 0.2 TestLoss: 2.3031137 TestAcc: 0.1\n",
      "TrainLoss: 2.3022811 TrainAcc: 0.08 TestLoss: 2.3012245 TestAcc: 0.06\n",
      "TrainLoss: 2.3031673 TrainAcc: 0.16 TestLoss: 2.304266 TestAcc: 0.1\n",
      "TrainLoss: 2.2998395 TrainAcc: 0.16 TestLoss: 2.3023288 TestAcc: 0.1\n",
      "TrainLoss: 2.3032136 TrainAcc: 0.1 TestLoss: 2.3021514 TestAcc: 0.14\n",
      "TrainLoss: 2.3064048 TrainAcc: 0.16 TestLoss: 2.30306 TestAcc: 0.1\n",
      "TrainLoss: 2.3020692 TrainAcc: 0.16 TestLoss: 2.3037913 TestAcc: 0.14\n",
      "TrainLoss: 2.302011 TrainAcc: 0.1 TestLoss: 2.3025646 TestAcc: 0.12\n",
      "TrainLoss: 2.3020415 TrainAcc: 0.08 TestLoss: 2.3054738 TestAcc: 0.1\n",
      "TrainLoss: 2.302372 TrainAcc: 0.14 TestLoss: 2.299261 TestAcc: 0.14\n",
      "TrainLoss: 2.3025825 TrainAcc: 0.14 TestLoss: 2.3024902 TestAcc: 0.1\n",
      "TrainLoss: 2.2993474 TrainAcc: 0.12 TestLoss: 2.3018117 TestAcc: 0.12\n",
      "TrainLoss: 2.300043 TrainAcc: 0.18 TestLoss: 2.3017244 TestAcc: 0.18\n",
      "TrainLoss: 2.3044083 TrainAcc: 0.08 TestLoss: 2.3023918 TestAcc: 0.14\n",
      "TrainLoss: 2.3025565 TrainAcc: 0.12 TestLoss: 2.3042343 TestAcc: 0.08\n",
      "TrainLoss: 2.3027542 TrainAcc: 0.06 TestLoss: 2.3030758 TestAcc: 0.16\n",
      "TrainLoss: 2.3014367 TrainAcc: 0.1 TestLoss: 2.3065786 TestAcc: 0.08\n",
      "TrainLoss: 2.302052 TrainAcc: 0.12 TestLoss: 2.3034217 TestAcc: 0.02\n",
      "TrainLoss: 2.3011765 TrainAcc: 0.12 TestLoss: 2.302744 TestAcc: 0.1\n",
      "TrainLoss: 2.3040156 TrainAcc: 0.12 TestLoss: 2.3011272 TestAcc: 0.14\n",
      "TrainLoss: 2.304192 TrainAcc: 0.06 TestLoss: 2.3026106 TestAcc: 0.14\n",
      "TrainLoss: 2.300899 TrainAcc: 0.12 TestLoss: 2.3056767 TestAcc: 0.06\n",
      "TrainLoss: 2.3022954 TrainAcc: 0.1 TestLoss: 2.298709 TestAcc: 0.14\n",
      "TrainLoss: 2.3040829 TrainAcc: 0.12 TestLoss: 2.3040245 TestAcc: 0.1\n",
      "TrainLoss: 2.302856 TrainAcc: 0.14 TestLoss: 2.3020852 TestAcc: 0.08\n",
      "TrainLoss: 2.3008702 TrainAcc: 0.1 TestLoss: 2.304731 TestAcc: 0.04\n",
      "TrainLoss: 2.3033721 TrainAcc: 0.08 TestLoss: 2.2997437 TestAcc: 0.14\n",
      "TrainLoss: 2.2990792 TrainAcc: 0.1 TestLoss: 2.3048036 TestAcc: 0.08\n",
      "TrainLoss: 2.3036985 TrainAcc: 0.1 TestLoss: 2.3015993 TestAcc: 0.06\n",
      "TrainLoss: 2.3042803 TrainAcc: 0.08 TestLoss: 2.300725 TestAcc: 0.08\n",
      "TrainLoss: 2.3011885 TrainAcc: 0.14 TestLoss: 2.3039858 TestAcc: 0.04\n",
      "TrainLoss: 2.3004637 TrainAcc: 0.14 TestLoss: 2.3024626 TestAcc: 0.02\n",
      "TrainLoss: 2.3006756 TrainAcc: 0.16 TestLoss: 2.298418 TestAcc: 0.14\n",
      "TrainLoss: 2.3024516 TrainAcc: 0.16 TestLoss: 2.30413 TestAcc: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.304303 TrainAcc: 0.08 TestLoss: 2.301332 TestAcc: 0.18\n",
      "TrainLoss: 2.3013074 TrainAcc: 0.16 TestLoss: 2.302722 TestAcc: 0.12\n",
      "TrainLoss: 2.3038225 TrainAcc: 0.1 TestLoss: 2.3049629 TestAcc: 0.06\n",
      "TrainLoss: 2.3024669 TrainAcc: 0.1 TestLoss: 2.3011017 TestAcc: 0.06\n",
      "TrainLoss: 2.303963 TrainAcc: 0.16 TestLoss: 2.302149 TestAcc: 0.1\n",
      "TrainLoss: 2.2995539 TrainAcc: 0.14 TestLoss: 2.3042178 TestAcc: 0.06\n",
      "TrainLoss: 2.304335 TrainAcc: 0.06 TestLoss: 2.3005128 TestAcc: 0.16\n",
      "TrainLoss: 2.2959816 TrainAcc: 0.26 TestLoss: 2.304073 TestAcc: 0.08\n",
      "TrainLoss: 2.2953653 TrainAcc: 0.2 TestLoss: 2.3018882 TestAcc: 0.1\n",
      "TrainLoss: 2.301112 TrainAcc: 0.14 TestLoss: 2.302732 TestAcc: 0.1\n",
      "TrainLoss: 2.3040373 TrainAcc: 0.08 TestLoss: 2.3005497 TestAcc: 0.1\n",
      "TrainLoss: 2.3026175 TrainAcc: 0.1 TestLoss: 2.303595 TestAcc: 0.08\n",
      "TrainLoss: 2.3040817 TrainAcc: 0.06 TestLoss: 2.3053603 TestAcc: 0.08\n",
      "TrainLoss: 2.3018897 TrainAcc: 0.14 TestLoss: 2.3008912 TestAcc: 0.12\n",
      "TrainLoss: 2.304385 TrainAcc: 0.06 TestLoss: 2.3045013 TestAcc: 0.06\n",
      "TrainLoss: 2.3019257 TrainAcc: 0.12 TestLoss: 2.303047 TestAcc: 0.12\n",
      "TrainLoss: 2.3000307 TrainAcc: 0.1 TestLoss: 2.3054988 TestAcc: 0.06\n",
      "TrainLoss: 2.305762 TrainAcc: 0.08 TestLoss: 2.3050334 TestAcc: 0.08\n",
      "TrainLoss: 2.3029418 TrainAcc: 0.12 TestLoss: 2.3048296 TestAcc: 0.06\n",
      "TrainLoss: 2.3005126 TrainAcc: 0.16 TestLoss: 2.3022287 TestAcc: 0.12\n",
      "TrainLoss: 2.303082 TrainAcc: 0.1 TestLoss: 2.3017297 TestAcc: 0.14\n",
      "TrainLoss: 2.3034878 TrainAcc: 0.14 TestLoss: 2.303748 TestAcc: 0.08\n",
      "TrainLoss: 2.3046806 TrainAcc: 0.08 TestLoss: 2.304803 TestAcc: 0.08\n",
      "TrainLoss: 2.3077903 TrainAcc: 0.04 TestLoss: 2.2987654 TestAcc: 0.18\n",
      "TrainLoss: 2.302195 TrainAcc: 0.08 TestLoss: 2.306123 TestAcc: 0.04\n",
      "TrainLoss: 2.3040507 TrainAcc: 0.12 TestLoss: 2.3013506 TestAcc: 0.16\n",
      "TrainLoss: 2.3041854 TrainAcc: 0.06 TestLoss: 2.3063672 TestAcc: 0.04\n",
      "TrainLoss: 2.303361 TrainAcc: 0.12 TestLoss: 2.3047678 TestAcc: 0.04\n",
      "TrainLoss: 2.3001122 TrainAcc: 0.14 TestLoss: 2.301078 TestAcc: 0.08\n",
      "TrainLoss: 2.3060577 TrainAcc: 0.02 TestLoss: 2.3054311 TestAcc: 0.02\n",
      "TrainLoss: 2.2992916 TrainAcc: 0.16 TestLoss: 2.3068814 TestAcc: 0.06\n",
      "TrainLoss: 2.301508 TrainAcc: 0.1 TestLoss: 2.303682 TestAcc: 0.08\n",
      "TrainLoss: 2.3007116 TrainAcc: 0.1 TestLoss: 2.302362 TestAcc: 0.16\n",
      "TrainLoss: 2.3043203 TrainAcc: 0.12 TestLoss: 2.3031738 TestAcc: 0.1\n",
      "TrainLoss: 2.3030045 TrainAcc: 0.06 TestLoss: 2.3000872 TestAcc: 0.18\n",
      "TrainLoss: 2.3031645 TrainAcc: 0.06 TestLoss: 2.3012207 TestAcc: 0.12\n",
      "TrainLoss: 2.3008235 TrainAcc: 0.14 TestLoss: 2.3036878 TestAcc: 0.04\n",
      "TrainLoss: 2.303017 TrainAcc: 0.06 TestLoss: 2.3025506 TestAcc: 0.1\n",
      "TrainLoss: 2.3024156 TrainAcc: 0.08 TestLoss: 2.3016152 TestAcc: 0.16\n",
      "TrainLoss: 2.3050835 TrainAcc: 0.04 TestLoss: 2.3019667 TestAcc: 0.08\n",
      "TrainLoss: 2.3026907 TrainAcc: 0.08 TestLoss: 2.3000863 TestAcc: 0.08\n",
      "TrainLoss: 2.3019161 TrainAcc: 0.14 TestLoss: 2.3029416 TestAcc: 0.06\n",
      "TrainLoss: 2.2999794 TrainAcc: 0.14 TestLoss: 2.3043666 TestAcc: 0.04\n",
      "TrainLoss: 2.3019269 TrainAcc: 0.08 TestLoss: 2.300078 TestAcc: 0.16\n",
      "TrainLoss: 2.2996483 TrainAcc: 0.16 TestLoss: 2.301678 TestAcc: 0.12\n",
      "TrainLoss: 2.3036978 TrainAcc: 0.1 TestLoss: 2.3017478 TestAcc: 0.14\n",
      "TrainLoss: 2.3027976 TrainAcc: 0.1 TestLoss: 2.301986 TestAcc: 0.1\n",
      "TrainLoss: 2.3060832 TrainAcc: 0.04 TestLoss: 2.3031476 TestAcc: 0.08\n",
      "TrainLoss: 2.3029726 TrainAcc: 0.06 TestLoss: 2.3006477 TestAcc: 0.12\n",
      "TrainLoss: 2.3057885 TrainAcc: 0.06 TestLoss: 2.3065443 TestAcc: 0.04\n",
      "TrainLoss: 2.3025587 TrainAcc: 0.06 TestLoss: 2.3042738 TestAcc: 0.08\n",
      "TrainLoss: 2.3050551 TrainAcc: 0.08 TestLoss: 2.3032358 TestAcc: 0.06\n",
      "TrainLoss: 2.3034735 TrainAcc: 0.06 TestLoss: 2.3047116 TestAcc: 0.04\n",
      "TrainLoss: 2.300404 TrainAcc: 0.2 TestLoss: 2.3054063 TestAcc: 0.04\n",
      "TrainLoss: 2.3048036 TrainAcc: 0.06 TestLoss: 2.3030663 TestAcc: 0.08\n",
      "TrainLoss: 2.2999117 TrainAcc: 0.18 TestLoss: 2.3015378 TestAcc: 0.14\n",
      "TrainLoss: 2.3009524 TrainAcc: 0.14 TestLoss: 2.3045065 TestAcc: 0.04\n",
      "TrainLoss: 2.3032372 TrainAcc: 0.06 TestLoss: 2.3058121 TestAcc: 0.12\n",
      "TrainLoss: 2.3035922 TrainAcc: 0.12 TestLoss: 2.3011312 TestAcc: 0.12\n",
      "TrainLoss: 2.3035586 TrainAcc: 0.06 TestLoss: 2.30327 TestAcc: 0.1\n",
      "TrainLoss: 2.3032694 TrainAcc: 0.12 TestLoss: 2.3034022 TestAcc: 0.04\n",
      "TrainLoss: 2.3016784 TrainAcc: 0.12 TestLoss: 2.3008883 TestAcc: 0.1\n",
      "TrainLoss: 2.3008213 TrainAcc: 0.14 TestLoss: 2.301279 TestAcc: 0.12\n",
      "TrainLoss: 2.3022919 TrainAcc: 0.06 TestLoss: 2.3032055 TestAcc: 0.1\n",
      "TrainLoss: 2.3019483 TrainAcc: 0.08 TestLoss: 2.3037183 TestAcc: 0.08\n",
      "TrainLoss: 2.3012764 TrainAcc: 0.16 TestLoss: 2.3025706 TestAcc: 0.1\n",
      "TrainLoss: 2.3024695 TrainAcc: 0.16 TestLoss: 2.3029864 TestAcc: 0.12\n",
      "TrainLoss: 2.302939 TrainAcc: 0.08 TestLoss: 2.3041348 TestAcc: 0.08\n",
      "TrainLoss: 2.3020928 TrainAcc: 0.06 TestLoss: 2.303568 TestAcc: 0.04\n",
      "TrainLoss: 2.3039353 TrainAcc: 0.06 TestLoss: 2.305412 TestAcc: 0.08\n",
      "TrainLoss: 2.3032393 TrainAcc: 0.1 TestLoss: 2.3016558 TestAcc: 0.14\n",
      "TrainLoss: 2.3019245 TrainAcc: 0.14 TestLoss: 2.3033025 TestAcc: 0.04\n",
      "TrainLoss: 2.3042047 TrainAcc: 0.12 TestLoss: 2.3077805 TestAcc: 0.0\n",
      "TrainLoss: 2.3065622 TrainAcc: 0.0 TestLoss: 2.3037179 TestAcc: 0.08\n",
      "TrainLoss: 2.2992377 TrainAcc: 0.16 TestLoss: 2.3029394 TestAcc: 0.1\n",
      "TrainLoss: 2.301667 TrainAcc: 0.12 TestLoss: 2.3041759 TestAcc: 0.04\n",
      "TrainLoss: 2.3018465 TrainAcc: 0.1 TestLoss: 2.3046064 TestAcc: 0.06\n",
      "TrainLoss: 2.3065467 TrainAcc: 0.02 TestLoss: 2.305375 TestAcc: 0.04\n",
      "TrainLoss: 2.3027196 TrainAcc: 0.08 TestLoss: 2.3002634 TestAcc: 0.16\n",
      "TrainLoss: 2.3037295 TrainAcc: 0.1 TestLoss: 2.3019004 TestAcc: 0.14\n",
      "TrainLoss: 2.301139 TrainAcc: 0.12 TestLoss: 2.300127 TestAcc: 0.18\n",
      "TrainLoss: 2.3015528 TrainAcc: 0.18 TestLoss: 2.3055806 TestAcc: 0.04\n",
      "TrainLoss: 2.303683 TrainAcc: 0.12 TestLoss: 2.304031 TestAcc: 0.08\n",
      "TrainLoss: 2.3052068 TrainAcc: 0.02 TestLoss: 2.3032334 TestAcc: 0.12\n",
      "TrainLoss: 2.301861 TrainAcc: 0.12 TestLoss: 2.3010886 TestAcc: 0.1\n",
      "TrainLoss: 2.3016071 TrainAcc: 0.12 TestLoss: 2.303378 TestAcc: 0.04\n",
      "TrainLoss: 2.301752 TrainAcc: 0.12 TestLoss: 2.3058095 TestAcc: 0.04\n",
      "TrainLoss: 2.3055313 TrainAcc: 0.04 TestLoss: 2.3037138 TestAcc: 0.12\n",
      "TrainLoss: 2.303637 TrainAcc: 0.08 TestLoss: 2.3022847 TestAcc: 0.1\n",
      "TrainLoss: 2.300836 TrainAcc: 0.08 TestLoss: 2.3023345 TestAcc: 0.16\n",
      "TrainLoss: 2.2998374 TrainAcc: 0.12 TestLoss: 2.3013825 TestAcc: 0.14\n",
      "TrainLoss: 2.3027754 TrainAcc: 0.12 TestLoss: 2.3017182 TestAcc: 0.14\n",
      "TrainLoss: 2.3020873 TrainAcc: 0.08 TestLoss: 2.3013074 TestAcc: 0.16\n",
      "TrainLoss: 2.3043282 TrainAcc: 0.06 TestLoss: 2.3042037 TestAcc: 0.1\n",
      "TrainLoss: 2.2973275 TrainAcc: 0.22 TestLoss: 2.303595 TestAcc: 0.1\n",
      "TrainLoss: 2.3017573 TrainAcc: 0.06 TestLoss: 2.3043542 TestAcc: 0.02\n",
      "TrainLoss: 2.3010898 TrainAcc: 0.16 TestLoss: 2.3037348 TestAcc: 0.04\n",
      "TrainLoss: 2.30203 TrainAcc: 0.08 TestLoss: 2.3006713 TestAcc: 0.16\n",
      "TrainLoss: 2.3033528 TrainAcc: 0.1 TestLoss: 2.3044767 TestAcc: 0.08\n",
      "TrainLoss: 2.3026266 TrainAcc: 0.12 TestLoss: 2.3027806 TestAcc: 0.16\n",
      "TrainLoss: 2.3031237 TrainAcc: 0.1 TestLoss: 2.3000627 TestAcc: 0.18\n",
      "TrainLoss: 2.3036864 TrainAcc: 0.06 TestLoss: 2.3053362 TestAcc: 0.02\n",
      "TrainLoss: 2.3026104 TrainAcc: 0.1 TestLoss: 2.3035243 TestAcc: 0.06\n",
      "TrainLoss: 2.302176 TrainAcc: 0.12 TestLoss: 2.3013608 TestAcc: 0.12\n",
      "TrainLoss: 2.3028066 TrainAcc: 0.06 TestLoss: 2.305593 TestAcc: 0.04\n",
      "TrainLoss: 2.3044982 TrainAcc: 0.04 TestLoss: 2.297204 TestAcc: 0.2\n",
      "TrainLoss: 2.301023 TrainAcc: 0.14 TestLoss: 2.3022668 TestAcc: 0.1\n",
      "TrainLoss: 2.3020823 TrainAcc: 0.06 TestLoss: 2.3057735 TestAcc: 0.08\n",
      "TrainLoss: 2.3025236 TrainAcc: 0.04 TestLoss: 2.2984002 TestAcc: 0.2\n",
      "TrainLoss: 2.3012323 TrainAcc: 0.12 TestLoss: 2.302114 TestAcc: 0.04\n",
      "TrainLoss: 2.2991984 TrainAcc: 0.14 TestLoss: 2.3021493 TestAcc: 0.12\n",
      "TrainLoss: 2.3013864 TrainAcc: 0.12 TestLoss: 2.3032014 TestAcc: 0.08\n",
      "TrainLoss: 2.3009295 TrainAcc: 0.12 TestLoss: 2.303122 TestAcc: 0.1\n",
      "TrainLoss: 2.3037934 TrainAcc: 0.06 TestLoss: 2.304157 TestAcc: 0.08\n",
      "TrainLoss: 2.300134 TrainAcc: 0.04 TestLoss: 2.300927 TestAcc: 0.16\n",
      "TrainLoss: 2.3047798 TrainAcc: 0.1 TestLoss: 2.3020535 TestAcc: 0.08\n",
      "TrainLoss: 2.3022282 TrainAcc: 0.16 TestLoss: 2.302256 TestAcc: 0.1\n",
      "TrainLoss: 2.301049 TrainAcc: 0.1 TestLoss: 2.308889 TestAcc: 0.02\n",
      "TrainLoss: 2.3024595 TrainAcc: 0.04 TestLoss: 2.3020108 TestAcc: 0.08\n",
      "TrainLoss: 2.2981954 TrainAcc: 0.22 TestLoss: 2.2999878 TestAcc: 0.22\n",
      "TrainLoss: 2.3025086 TrainAcc: 0.08 TestLoss: 2.303595 TestAcc: 0.06\n",
      "TrainLoss: 2.301314 TrainAcc: 0.14 TestLoss: 2.3001804 TestAcc: 0.1\n",
      "TrainLoss: 2.301261 TrainAcc: 0.1 TestLoss: 2.300187 TestAcc: 0.1\n",
      "TrainLoss: 2.3059177 TrainAcc: 0.08 TestLoss: 2.3027866 TestAcc: 0.14\n",
      "TrainLoss: 2.3018422 TrainAcc: 0.12 TestLoss: 2.2994366 TestAcc: 0.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3037477 TrainAcc: 0.02 TestLoss: 2.3050344 TestAcc: 0.06\n",
      "TrainLoss: 2.3031049 TrainAcc: 0.04 TestLoss: 2.3037362 TestAcc: 0.06\n",
      "TrainLoss: 2.3028324 TrainAcc: 0.1 TestLoss: 2.3038862 TestAcc: 0.12\n",
      "TrainLoss: 2.3023715 TrainAcc: 0.12 TestLoss: 2.3033094 TestAcc: 0.08\n",
      "TrainLoss: 2.3036523 TrainAcc: 0.12 TestLoss: 2.3015559 TestAcc: 0.16\n",
      "TrainLoss: 2.3048925 TrainAcc: 0.02 TestLoss: 2.3000555 TestAcc: 0.1\n",
      "TrainLoss: 2.3054411 TrainAcc: 0.08 TestLoss: 2.3021493 TestAcc: 0.08\n",
      "TrainLoss: 2.3003564 TrainAcc: 0.14 TestLoss: 2.3042002 TestAcc: 0.08\n",
      "TrainLoss: 2.3028088 TrainAcc: 0.08 TestLoss: 2.3034399 TestAcc: 0.08\n",
      "TrainLoss: 2.3050208 TrainAcc: 0.04 TestLoss: 2.3042786 TestAcc: 0.06\n",
      "TrainLoss: 2.3012817 TrainAcc: 0.04 TestLoss: 2.3022006 TestAcc: 0.14\n",
      "TrainLoss: 2.3025343 TrainAcc: 0.08 TestLoss: 2.3034992 TestAcc: 0.08\n",
      "TrainLoss: 2.301013 TrainAcc: 0.1 TestLoss: 2.3043642 TestAcc: 0.08\n",
      "TrainLoss: 2.3022108 TrainAcc: 0.12 TestLoss: 2.304734 TestAcc: 0.1\n",
      "TrainLoss: 2.3037393 TrainAcc: 0.08 TestLoss: 2.2991173 TestAcc: 0.12\n",
      "TrainLoss: 2.3033767 TrainAcc: 0.06 TestLoss: 2.304156 TestAcc: 0.02\n",
      "TrainLoss: 2.299337 TrainAcc: 0.14 TestLoss: 2.3043368 TestAcc: 0.04\n",
      "TrainLoss: 2.303539 TrainAcc: 0.1 TestLoss: 2.2993226 TestAcc: 0.12\n",
      "TrainLoss: 2.3035562 TrainAcc: 0.08 TestLoss: 2.3027186 TestAcc: 0.12\n",
      "TrainLoss: 2.2996 TrainAcc: 0.14 TestLoss: 2.3042746 TestAcc: 0.04\n",
      "TrainLoss: 2.3027778 TrainAcc: 0.06 TestLoss: 2.3023725 TestAcc: 0.12\n",
      "TrainLoss: 2.2985628 TrainAcc: 0.18 TestLoss: 2.3047335 TestAcc: 0.02\n",
      "TrainLoss: 2.3007448 TrainAcc: 0.16 TestLoss: 2.306537 TestAcc: 0.0\n",
      "TrainLoss: 2.3041968 TrainAcc: 0.04 TestLoss: 2.3045614 TestAcc: 0.04\n",
      "TrainLoss: 2.2997918 TrainAcc: 0.18 TestLoss: 2.3039222 TestAcc: 0.08\n",
      "TrainLoss: 2.3026047 TrainAcc: 0.06 TestLoss: 2.2999127 TestAcc: 0.2\n",
      "TrainLoss: 2.3033516 TrainAcc: 0.06 TestLoss: 2.3006692 TestAcc: 0.12\n",
      "TrainLoss: 2.3021898 TrainAcc: 0.12 TestLoss: 2.302072 TestAcc: 0.14\n",
      "TrainLoss: 2.304775 TrainAcc: 0.06 TestLoss: 2.3027089 TestAcc: 0.14\n",
      "TrainLoss: 2.3015304 TrainAcc: 0.02 TestLoss: 2.2991934 TestAcc: 0.24\n",
      "TrainLoss: 2.2982323 TrainAcc: 0.12 TestLoss: 2.3007882 TestAcc: 0.06\n",
      "TrainLoss: 2.302682 TrainAcc: 0.1 TestLoss: 2.3015432 TestAcc: 0.08\n",
      "TrainLoss: 2.302164 TrainAcc: 0.1 TestLoss: 2.2999358 TestAcc: 0.1\n",
      "TrainLoss: 2.3041675 TrainAcc: 0.06 TestLoss: 2.3048275 TestAcc: 0.04\n",
      "TrainLoss: 2.3037448 TrainAcc: 0.08 TestLoss: 2.304778 TestAcc: 0.14\n",
      "TrainLoss: 2.3012288 TrainAcc: 0.12 TestLoss: 2.3021672 TestAcc: 0.12\n",
      "TrainLoss: 2.298041 TrainAcc: 0.12 TestLoss: 2.30293 TestAcc: 0.16\n",
      "TrainLoss: 2.3003342 TrainAcc: 0.08 TestLoss: 2.304113 TestAcc: 0.08\n",
      "TrainLoss: 2.2999635 TrainAcc: 0.18 TestLoss: 2.3033993 TestAcc: 0.04\n",
      "TrainLoss: 2.3057418 TrainAcc: 0.1 TestLoss: 2.3008375 TestAcc: 0.12\n",
      "TrainLoss: 2.3019583 TrainAcc: 0.22 TestLoss: 2.2995842 TestAcc: 0.1\n",
      "TrainLoss: 2.3012252 TrainAcc: 0.06 TestLoss: 2.3007112 TestAcc: 0.1\n",
      "TrainLoss: 2.301574 TrainAcc: 0.18 TestLoss: 2.303687 TestAcc: 0.12\n",
      "TrainLoss: 2.303618 TrainAcc: 0.02 TestLoss: 2.3005874 TestAcc: 0.16\n",
      "TrainLoss: 2.3008606 TrainAcc: 0.08 TestLoss: 2.302443 TestAcc: 0.14\n",
      "TrainLoss: 2.3002877 TrainAcc: 0.1 TestLoss: 2.301938 TestAcc: 0.12\n",
      "TrainLoss: 2.3031776 TrainAcc: 0.1 TestLoss: 2.3049815 TestAcc: 0.04\n",
      "TrainLoss: 2.2999067 TrainAcc: 0.14 TestLoss: 2.3031783 TestAcc: 0.06\n",
      "TrainLoss: 2.3012826 TrainAcc: 0.12 TestLoss: 2.3027487 TestAcc: 0.1\n",
      "TrainLoss: 2.3004286 TrainAcc: 0.18 TestLoss: 2.3054018 TestAcc: 0.1\n",
      "TrainLoss: 2.3025527 TrainAcc: 0.08 TestLoss: 2.30138 TestAcc: 0.2\n",
      "TrainLoss: 2.3031375 TrainAcc: 0.06 TestLoss: 2.3050466 TestAcc: 0.04\n",
      "TrainLoss: 2.303291 TrainAcc: 0.12 TestLoss: 2.3009481 TestAcc: 0.1\n",
      "TrainLoss: 2.2995763 TrainAcc: 0.1 TestLoss: 2.3026218 TestAcc: 0.1\n",
      "TrainLoss: 2.2991917 TrainAcc: 0.06 TestLoss: 2.3013747 TestAcc: 0.1\n",
      "TrainLoss: 2.3019886 TrainAcc: 0.02 TestLoss: 2.3024955 TestAcc: 0.14\n",
      "TrainLoss: 2.3032877 TrainAcc: 0.06 TestLoss: 2.3064513 TestAcc: 0.02\n",
      "TrainLoss: 2.3000214 TrainAcc: 0.12 TestLoss: 2.3021615 TestAcc: 0.06\n",
      "TrainLoss: 2.3025749 TrainAcc: 0.06 TestLoss: 2.3039503 TestAcc: 0.08\n",
      "TrainLoss: 2.301364 TrainAcc: 0.1 TestLoss: 2.3004904 TestAcc: 0.08\n",
      "TrainLoss: 2.3025577 TrainAcc: 0.08 TestLoss: 2.3042784 TestAcc: 0.08\n",
      "TrainLoss: 2.3013341 TrainAcc: 0.12 TestLoss: 2.3010528 TestAcc: 0.1\n",
      "TrainLoss: 2.3032742 TrainAcc: 0.06 TestLoss: 2.298683 TestAcc: 0.24\n",
      "TrainLoss: 2.303613 TrainAcc: 0.1 TestLoss: 2.3072016 TestAcc: 0.02\n",
      "TrainLoss: 2.302864 TrainAcc: 0.08 TestLoss: 2.3006427 TestAcc: 0.12\n",
      "TrainLoss: 2.3063989 TrainAcc: 0.06 TestLoss: 2.3023143 TestAcc: 0.06\n",
      "TrainLoss: 2.3008032 TrainAcc: 0.18 TestLoss: 2.3034372 TestAcc: 0.16\n",
      "TrainLoss: 2.296843 TrainAcc: 0.2 TestLoss: 2.304235 TestAcc: 0.04\n",
      "TrainLoss: 2.3023038 TrainAcc: 0.08 TestLoss: 2.303734 TestAcc: 0.08\n",
      "TrainLoss: 2.3000574 TrainAcc: 0.1 TestLoss: 2.3020658 TestAcc: 0.08\n",
      "TrainLoss: 2.2991586 TrainAcc: 0.14 TestLoss: 2.3023396 TestAcc: 0.1\n",
      "TrainLoss: 2.2995355 TrainAcc: 0.14 TestLoss: 2.3014574 TestAcc: 0.16\n",
      "TrainLoss: 2.3018813 TrainAcc: 0.12 TestLoss: 2.30467 TestAcc: 0.1\n",
      "TrainLoss: 2.3042388 TrainAcc: 0.06 TestLoss: 2.305166 TestAcc: 0.06\n",
      "TrainLoss: 2.300423 TrainAcc: 0.14 TestLoss: 2.3028104 TestAcc: 0.1\n",
      "TrainLoss: 2.3028982 TrainAcc: 0.1 TestLoss: 2.3037379 TestAcc: 0.1\n",
      "TrainLoss: 2.3053813 TrainAcc: 0.04 TestLoss: 2.3020234 TestAcc: 0.06\n",
      "TrainLoss: 2.3046916 TrainAcc: 0.14 TestLoss: 2.3036566 TestAcc: 0.06\n",
      "TrainLoss: 2.3029356 TrainAcc: 0.12 TestLoss: 2.3013587 TestAcc: 0.08\n",
      "TrainLoss: 2.3034332 TrainAcc: 0.08 TestLoss: 2.3022513 TestAcc: 0.16\n",
      "TrainLoss: 2.303989 TrainAcc: 0.1 TestLoss: 2.3055131 TestAcc: 0.06\n",
      "TrainLoss: 2.3055444 TrainAcc: 0.04 TestLoss: 2.304152 TestAcc: 0.08\n",
      "TrainLoss: 2.308438 TrainAcc: 0.02 TestLoss: 2.3032036 TestAcc: 0.12\n",
      "TrainLoss: 2.2986565 TrainAcc: 0.2 TestLoss: 2.3004377 TestAcc: 0.14\n",
      "TrainLoss: 2.3024137 TrainAcc: 0.1 TestLoss: 2.3032556 TestAcc: 0.02\n",
      "TrainLoss: 2.3041117 TrainAcc: 0.08 TestLoss: 2.298614 TestAcc: 0.14\n",
      "TrainLoss: 2.303313 TrainAcc: 0.08 TestLoss: 2.3055906 TestAcc: 0.06\n",
      "TrainLoss: 2.30423 TrainAcc: 0.12 TestLoss: 2.304946 TestAcc: 0.16\n",
      "TrainLoss: 2.3032827 TrainAcc: 0.16 TestLoss: 2.3008082 TestAcc: 0.1\n",
      "TrainLoss: 2.302117 TrainAcc: 0.16 TestLoss: 2.3008306 TestAcc: 0.12\n",
      "TrainLoss: 2.3021512 TrainAcc: 0.06 TestLoss: 2.3072824 TestAcc: 0.06\n",
      "TrainLoss: 2.3035998 TrainAcc: 0.14 TestLoss: 2.3033211 TestAcc: 0.16\n",
      "TrainLoss: 2.3015902 TrainAcc: 0.06 TestLoss: 2.3001661 TestAcc: 0.16\n",
      "TrainLoss: 2.3014557 TrainAcc: 0.1 TestLoss: 2.3031762 TestAcc: 0.1\n",
      "TrainLoss: 2.2981124 TrainAcc: 0.14 TestLoss: 2.3010647 TestAcc: 0.08\n",
      "TrainLoss: 2.3038104 TrainAcc: 0.08 TestLoss: 2.3064275 TestAcc: 0.08\n",
      "TrainLoss: 2.303703 TrainAcc: 0.1 TestLoss: 2.3001986 TestAcc: 0.12\n",
      "TrainLoss: 2.3040295 TrainAcc: 0.1 TestLoss: 2.304185 TestAcc: 0.08\n",
      "TrainLoss: 2.3035672 TrainAcc: 0.12 TestLoss: 2.2996786 TestAcc: 0.16\n",
      "TrainLoss: 2.3054554 TrainAcc: 0.08 TestLoss: 2.302125 TestAcc: 0.08\n",
      "TrainLoss: 2.304031 TrainAcc: 0.1 TestLoss: 2.3025854 TestAcc: 0.06\n",
      "TrainLoss: 2.2991314 TrainAcc: 0.12 TestLoss: 2.3041668 TestAcc: 0.04\n",
      "TrainLoss: 2.30219 TrainAcc: 0.08 TestLoss: 2.3035467 TestAcc: 0.16\n",
      "TrainLoss: 2.3051794 TrainAcc: 0.04 TestLoss: 2.3044164 TestAcc: 0.08\n",
      "TrainLoss: 2.3029788 TrainAcc: 0.12 TestLoss: 2.3029914 TestAcc: 0.08\n",
      "TrainLoss: 2.3005304 TrainAcc: 0.16 TestLoss: 2.3016775 TestAcc: 0.18\n",
      "TrainLoss: 2.3005102 TrainAcc: 0.1 TestLoss: 2.3014846 TestAcc: 0.14\n",
      "TrainLoss: 2.3016748 TrainAcc: 0.14 TestLoss: 2.296759 TestAcc: 0.2\n",
      "TrainLoss: 2.3005586 TrainAcc: 0.2 TestLoss: 2.3028502 TestAcc: 0.08\n",
      "TrainLoss: 2.301768 TrainAcc: 0.18 TestLoss: 2.3022041 TestAcc: 0.06\n",
      "TrainLoss: 2.3046095 TrainAcc: 0.06 TestLoss: 2.3016942 TestAcc: 0.1\n",
      "TrainLoss: 2.3020341 TrainAcc: 0.06 TestLoss: 2.3027453 TestAcc: 0.06\n",
      "TrainLoss: 2.3049593 TrainAcc: 0.1 TestLoss: 2.3007085 TestAcc: 0.08\n",
      "TrainLoss: 2.307792 TrainAcc: 0.02 TestLoss: 2.3057044 TestAcc: 0.08\n",
      "TrainLoss: 2.302901 TrainAcc: 0.1 TestLoss: 2.3003533 TestAcc: 0.14\n",
      "TrainLoss: 2.3022027 TrainAcc: 0.12 TestLoss: 2.3016746 TestAcc: 0.14\n",
      "TrainLoss: 2.3006663 TrainAcc: 0.1 TestLoss: 2.3057475 TestAcc: 0.06\n",
      "TrainLoss: 2.2991588 TrainAcc: 0.14 TestLoss: 2.3001597 TestAcc: 0.1\n",
      "TrainLoss: 2.3019886 TrainAcc: 0.12 TestLoss: 2.3042498 TestAcc: 0.08\n",
      "TrainLoss: 2.3018434 TrainAcc: 0.18 TestLoss: 2.3044035 TestAcc: 0.08\n",
      "TrainLoss: 2.301067 TrainAcc: 0.08 TestLoss: 2.302594 TestAcc: 0.14\n",
      "TrainLoss: 2.304719 TrainAcc: 0.06 TestLoss: 2.3018072 TestAcc: 0.14\n",
      "TrainLoss: 2.301875 TrainAcc: 0.08 TestLoss: 2.302857 TestAcc: 0.08\n",
      "TrainLoss: 2.3017893 TrainAcc: 0.1 TestLoss: 2.3017263 TestAcc: 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss: 2.3057055 TrainAcc: 0.06 TestLoss: 2.3077796 TestAcc: 0.08\n",
      "TrainLoss: 2.303618 TrainAcc: 0.08 TestLoss: 2.3045163 TestAcc: 0.1\n",
      "TrainLoss: 2.302879 TrainAcc: 0.1 TestLoss: 2.3007166 TestAcc: 0.08\n",
      "TrainLoss: 2.3019202 TrainAcc: 0.12 TestLoss: 2.3027692 TestAcc: 0.12\n",
      "TrainLoss: 2.30247 TrainAcc: 0.08 TestLoss: 2.3016043 TestAcc: 0.06\n",
      "TrainLoss: 2.3008282 TrainAcc: 0.1 TestLoss: 2.3036795 TestAcc: 0.08\n",
      "TrainLoss: 2.3007903 TrainAcc: 0.12 TestLoss: 2.3020775 TestAcc: 0.08\n",
      "TrainLoss: 2.3044245 TrainAcc: 0.08 TestLoss: 2.3017116 TestAcc: 0.08\n",
      "TrainLoss: 2.302137 TrainAcc: 0.08 TestLoss: 2.3009028 TestAcc: 0.12\n",
      "TrainLoss: 2.301969 TrainAcc: 0.18 TestLoss: 2.3001575 TestAcc: 0.14\n",
      "TrainLoss: 2.304138 TrainAcc: 0.08 TestLoss: 2.3007555 TestAcc: 0.16\n",
      "TrainLoss: 2.2993894 TrainAcc: 0.16 TestLoss: 2.3026452 TestAcc: 0.1\n",
      "TrainLoss: 2.3046784 TrainAcc: 0.02 TestLoss: 2.3044634 TestAcc: 0.12\n",
      "TrainLoss: 2.3041222 TrainAcc: 0.06 TestLoss: 2.2981663 TestAcc: 0.1\n",
      "TrainLoss: 2.3005106 TrainAcc: 0.12 TestLoss: 2.3003194 TestAcc: 0.1\n",
      "TrainLoss: 2.300589 TrainAcc: 0.16 TestLoss: 2.2988687 TestAcc: 0.14\n",
      "TrainLoss: 2.2998204 TrainAcc: 0.1 TestLoss: 2.3055358 TestAcc: 0.04\n",
      "TrainLoss: 2.3064392 TrainAcc: 0.06 TestLoss: 2.3043873 TestAcc: 0.04\n",
      "TrainLoss: 2.3030474 TrainAcc: 0.1 TestLoss: 2.3040957 TestAcc: 0.16\n",
      "TrainLoss: 2.3030198 TrainAcc: 0.16 TestLoss: 2.3035815 TestAcc: 0.1\n",
      "TrainLoss: 2.301586 TrainAcc: 0.12 TestLoss: 2.3018827 TestAcc: 0.18\n",
      "TrainLoss: 2.3053205 TrainAcc: 0.08 TestLoss: 2.303757 TestAcc: 0.1\n",
      "TrainLoss: 2.3075347 TrainAcc: 0.06 TestLoss: 2.304732 TestAcc: 0.08\n",
      "TrainLoss: 2.3003082 TrainAcc: 0.1 TestLoss: 2.3029346 TestAcc: 0.06\n",
      "TrainLoss: 2.3041701 TrainAcc: 0.04 TestLoss: 2.3036168 TestAcc: 0.06\n",
      "TrainLoss: 2.303216 TrainAcc: 0.1 TestLoss: 2.3030198 TestAcc: 0.08\n",
      "TrainLoss: 2.3006408 TrainAcc: 0.14 TestLoss: 2.3041558 TestAcc: 0.04\n",
      "TrainLoss: 2.3022773 TrainAcc: 0.06 TestLoss: 2.3022695 TestAcc: 0.12\n",
      "TrainLoss: 2.302745 TrainAcc: 0.12 TestLoss: 2.3063908 TestAcc: 0.06\n",
      "TrainLoss: 2.3053558 TrainAcc: 0.06 TestLoss: 2.3011634 TestAcc: 0.14\n",
      "TrainLoss: 2.3013215 TrainAcc: 0.04 TestLoss: 2.30365 TestAcc: 0.08\n",
      "TrainLoss: 2.3011732 TrainAcc: 0.12 TestLoss: 2.3008943 TestAcc: 0.16\n",
      "TrainLoss: 2.3012087 TrainAcc: 0.14 TestLoss: 2.2987897 TestAcc: 0.1\n",
      "TrainLoss: 2.3016737 TrainAcc: 0.16 TestLoss: 2.3029778 TestAcc: 0.14\n",
      "TrainLoss: 2.3040216 TrainAcc: 0.12 TestLoss: 2.2982428 TestAcc: 0.22\n",
      "TrainLoss: 2.3025138 TrainAcc: 0.12 TestLoss: 2.302726 TestAcc: 0.14\n",
      "TrainLoss: 2.3001428 TrainAcc: 0.08 TestLoss: 2.3040228 TestAcc: 0.06\n",
      "TrainLoss: 2.3024049 TrainAcc: 0.14 TestLoss: 2.3032532 TestAcc: 0.06\n",
      "TrainLoss: 2.3049276 TrainAcc: 0.1 TestLoss: 2.303201 TestAcc: 0.14\n",
      "TrainLoss: 2.3035493 TrainAcc: 0.04 TestLoss: 2.3042655 TestAcc: 0.16\n",
      "TrainLoss: 2.3035486 TrainAcc: 0.04 TestLoss: 2.302153 TestAcc: 0.1\n",
      "TrainLoss: 2.3059015 TrainAcc: 0.1 TestLoss: 2.3027647 TestAcc: 0.06\n",
      "TrainLoss: 2.3009577 TrainAcc: 0.16 TestLoss: 2.3004522 TestAcc: 0.1\n",
      "TrainLoss: 2.3003125 TrainAcc: 0.16 TestLoss: 2.304516 TestAcc: 0.04\n",
      "TrainLoss: 2.2992482 TrainAcc: 0.14 TestLoss: 2.3038003 TestAcc: 0.18\n",
      "TrainLoss: 2.3018205 TrainAcc: 0.12 TestLoss: 2.3015711 TestAcc: 0.2\n",
      "TrainLoss: 2.3031979 TrainAcc: 0.12 TestLoss: 2.30212 TestAcc: 0.14\n",
      "TrainLoss: 2.305602 TrainAcc: 0.08 TestLoss: 2.3029444 TestAcc: 0.06\n",
      "TrainLoss: 2.299885 TrainAcc: 0.1 TestLoss: 2.304435 TestAcc: 0.08\n",
      "TrainLoss: 2.3015592 TrainAcc: 0.12 TestLoss: 2.3078659 TestAcc: 0.02\n",
      "TrainLoss: 2.302414 TrainAcc: 0.08 TestLoss: 2.3038595 TestAcc: 0.14\n",
      "TrainLoss: 2.3061001 TrainAcc: 0.08 TestLoss: 2.3017662 TestAcc: 0.12\n",
      "TrainLoss: 2.3012981 TrainAcc: 0.14 TestLoss: 2.304876 TestAcc: 0.02\n",
      "TrainLoss: 2.3035774 TrainAcc: 0.08 TestLoss: 2.304669 TestAcc: 0.0\n",
      "TrainLoss: 2.3023145 TrainAcc: 0.1 TestLoss: 2.3013577 TestAcc: 0.06\n",
      "TrainLoss: 2.2991397 TrainAcc: 0.16 TestLoss: 2.3030503 TestAcc: 0.08\n",
      "TrainLoss: 2.3046005 TrainAcc: 0.08 TestLoss: 2.3014982 TestAcc: 0.2\n",
      "TrainLoss: 2.3037405 TrainAcc: 0.1 TestLoss: 2.3059614 TestAcc: 0.02\n",
      "TrainLoss: 2.3026557 TrainAcc: 0.04 TestLoss: 2.3042688 TestAcc: 0.08\n",
      "TrainLoss: 2.300819 TrainAcc: 0.16 TestLoss: 2.3052125 TestAcc: 0.04\n",
      "TrainLoss: 2.300707 TrainAcc: 0.1 TestLoss: 2.3055184 TestAcc: 0.12\n",
      "TrainLoss: 2.3048818 TrainAcc: 0.08 TestLoss: 2.3066032 TestAcc: 0.08\n",
      "TrainLoss: 2.3005667 TrainAcc: 0.14 TestLoss: 2.304708 TestAcc: 0.02\n",
      "TrainLoss: 2.303147 TrainAcc: 0.04 TestLoss: 2.3024938 TestAcc: 0.12\n",
      "TrainLoss: 2.3050568 TrainAcc: 0.12 TestLoss: 2.3044384 TestAcc: 0.08\n",
      "TrainLoss: 2.3053305 TrainAcc: 0.08 TestLoss: 2.3026593 TestAcc: 0.12\n",
      "TrainLoss: 2.303794 TrainAcc: 0.06 TestLoss: 2.3002863 TestAcc: 0.08\n",
      "TrainLoss: 2.3034365 TrainAcc: 0.08 TestLoss: 2.301368 TestAcc: 0.08\n",
      "TrainLoss: 2.302662 TrainAcc: 0.1 TestLoss: 2.304229 TestAcc: 0.1\n",
      "TrainLoss: 2.3042681 TrainAcc: 0.04 TestLoss: 2.301947 TestAcc: 0.06\n",
      "TrainLoss: 2.3004448 TrainAcc: 0.12 TestLoss: 2.300493 TestAcc: 0.14\n",
      "TrainLoss: 2.2987175 TrainAcc: 0.12 TestLoss: 2.3033385 TestAcc: 0.1\n",
      "TrainLoss: 2.3005319 TrainAcc: 0.16 TestLoss: 2.301912 TestAcc: 0.14\n",
      "TrainLoss: 2.3012176 TrainAcc: 0.12 TestLoss: 2.2988157 TestAcc: 0.16\n",
      "TrainLoss: 2.303032 TrainAcc: 0.14 TestLoss: 2.304254 TestAcc: 0.1\n",
      "TrainLoss: 2.3017652 TrainAcc: 0.12 TestLoss: 2.3029258 TestAcc: 0.14\n",
      "TrainLoss: 2.3043587 TrainAcc: 0.08 TestLoss: 2.3028913 TestAcc: 0.06\n",
      "TrainLoss: 2.3018637 TrainAcc: 0.14 TestLoss: 2.303175 TestAcc: 0.1\n",
      "TrainLoss: 2.3026536 TrainAcc: 0.06 TestLoss: 2.3025188 TestAcc: 0.16\n",
      "TrainLoss: 2.3005562 TrainAcc: 0.08 TestLoss: 2.3026862 TestAcc: 0.04\n",
      "TrainLoss: 2.3034573 TrainAcc: 0.08 TestLoss: 2.3006318 TestAcc: 0.14\n",
      "TrainLoss: 2.2994068 TrainAcc: 0.12 TestLoss: 2.303478 TestAcc: 0.06\n",
      "TrainLoss: 2.3049188 TrainAcc: 0.06 TestLoss: 2.3025692 TestAcc: 0.12\n",
      "TrainLoss: 2.3050358 TrainAcc: 0.12 TestLoss: 2.2999487 TestAcc: 0.08\n",
      "TrainLoss: 2.3036654 TrainAcc: 0.06 TestLoss: 2.3039904 TestAcc: 0.1\n",
      "TrainLoss: 2.303917 TrainAcc: 0.12 TestLoss: 2.3033495 TestAcc: 0.1\n",
      "TrainLoss: 2.3004808 TrainAcc: 0.18 TestLoss: 2.3050249 TestAcc: 0.06\n",
      "TrainLoss: 2.3050027 TrainAcc: 0.08 TestLoss: 2.3010287 TestAcc: 0.12\n",
      "TrainLoss: 2.3009353 TrainAcc: 0.16 TestLoss: 2.3033826 TestAcc: 0.08\n",
      "TrainLoss: 2.3018124 TrainAcc: 0.14 TestLoss: 2.3022404 TestAcc: 0.1\n",
      "TrainLoss: 2.3027523 TrainAcc: 0.08 TestLoss: 2.3023832 TestAcc: 0.12\n",
      "TrainLoss: 2.2990172 TrainAcc: 0.2 TestLoss: 2.3034735 TestAcc: 0.08\n",
      "TrainLoss: 2.303176 TrainAcc: 0.08 TestLoss: 2.3057077 TestAcc: 0.06\n",
      "TrainLoss: 2.3030117 TrainAcc: 0.08 TestLoss: 2.3050523 TestAcc: 0.08\n",
      "TrainLoss: 2.301572 TrainAcc: 0.1 TestLoss: 2.3020277 TestAcc: 0.04\n",
      "TrainLoss: 2.3027427 TrainAcc: 0.06 TestLoss: 2.300941 TestAcc: 0.12\n",
      "TrainLoss: 2.3024905 TrainAcc: 0.12 TestLoss: 2.3040335 TestAcc: 0.1\n",
      "TrainLoss: 2.3046708 TrainAcc: 0.04 TestLoss: 2.3029952 TestAcc: 0.12\n",
      "TrainLoss: 2.3031847 TrainAcc: 0.12 TestLoss: 2.3031805 TestAcc: 0.08\n",
      "TrainLoss: 2.3010988 TrainAcc: 0.14 TestLoss: 2.3014612 TestAcc: 0.08\n",
      "TrainLoss: 2.3018367 TrainAcc: 0.14 TestLoss: 2.3061552 TestAcc: 0.04\n",
      "TrainLoss: 2.3025727 TrainAcc: 0.12 TestLoss: 2.3036594 TestAcc: 0.06\n",
      "TrainLoss: 2.304396 TrainAcc: 0.12 TestLoss: 2.3005056 TestAcc: 0.2\n",
      "TrainLoss: 2.3032365 TrainAcc: 0.12 TestLoss: 2.303781 TestAcc: 0.1\n",
      "TrainLoss: 2.303731 TrainAcc: 0.08 TestLoss: 2.3003461 TestAcc: 0.2\n",
      "TrainLoss: 2.3016338 TrainAcc: 0.18 TestLoss: 2.3025675 TestAcc: 0.04\n",
      "TrainLoss: 2.3000255 TrainAcc: 0.18 TestLoss: 2.30506 TestAcc: 0.02\n",
      "TrainLoss: 2.3020234 TrainAcc: 0.14 TestLoss: 2.2979584 TestAcc: 0.08\n",
      "TrainLoss: 2.3010283 TrainAcc: 0.12 TestLoss: 2.3011308 TestAcc: 0.12\n",
      "TrainLoss: 2.3062065 TrainAcc: 0.06 TestLoss: 2.3039987 TestAcc: 0.12\n",
      "TrainLoss: 2.3019378 TrainAcc: 0.12 TestLoss: 2.3012867 TestAcc: 0.16\n",
      "TrainLoss: 2.3032084 TrainAcc: 0.04 TestLoss: 2.3028104 TestAcc: 0.08\n",
      "TrainLoss: 2.2994251 TrainAcc: 0.16 TestLoss: 2.30293 TestAcc: 0.08\n",
      "TrainLoss: 2.3024359 TrainAcc: 0.14 TestLoss: 2.3033912 TestAcc: 0.06\n",
      "TrainLoss: 2.301586 TrainAcc: 0.2 TestLoss: 2.304136 TestAcc: 0.1\n",
      "TrainLoss: 2.3020422 TrainAcc: 0.12 TestLoss: 2.3022687 TestAcc: 0.1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2000):\n",
    "    random_indices=np.random.choice(X_train.shape[0],50)\n",
    "    #print(random_indices)\n",
    "    x_train_data=X_train[random_indices]\n",
    "    y_train_data=y_train[random_indices]\n",
    "    test_random_indices=np.random.choice(X_test.shape[0],50)\n",
    "    x_test_data=X_test[test_random_indices]\n",
    "    y_test_data=y_test[test_random_indices]\n",
    "    session.run(train,feed_dict={X:x_train_data,y:y_train_data})\n",
    "    print(\"TrainLoss:\",session.run(loss,feed_dict={X:x_train_data,y:y_train_data}),\\\n",
    "         \"TrainAcc:\",session.run(accuracy,feed_dict={X:x_train_data,y:y_train_data}),\\\n",
    "         \"TestLoss:\",session.run(loss,feed_dict={X:x_test_data,y:y_test_data}),\\\n",
    "         \"TestAcc:\",session.run(accuracy,feed_dict={X:x_test_data,y:y_test_data}))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Because Gradients are not calculated as frequently as using Keras, accuracy is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INN_R7_Project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
